{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import semiolog as slg"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Downloading and preparing dataset text/corpus to /Users/Gianni/.cache/huggingface/datasets/text/corpus-401908cfa88089c5/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 3/3 [00:00<00:00, 4432.16it/s]\n",
                        "100%|██████████| 3/3 [00:00<00:00, 455.24it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset text downloaded and prepared to /Users/Gianni/.cache/huggingface/datasets/text/corpus-401908cfa88089c5/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 3/3 [00:00<00:00, 158.48it/s]\n",
                        "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
                        "\n",
                        "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at models/en_bnc_old_segments/paradigms/tf_model.h5.\n",
                        "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
                    ]
                }
            ],
            "source": [
                "semiotic = slg.Cenematic(\"en_bnc_old_segments\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Downloading and preparing dataset text/original to /Users/Gianni/.cache/huggingface/datasets/text/original-96a4700e95cca36e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1/1 [00:00<00:00, 2011.66it/s]\n",
                        "100%|██████████| 1/1 [00:00<00:00, 123.09it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset text downloaded and prepared to /Users/Gianni/.cache/huggingface/datasets/text/original-96a4700e95cca36e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1/1 [00:00<00:00, 101.12it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "SLG: Dataset loaded from the following files: bnc_test.txt.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Corpus built\n",
                        "Corpus saved to models/en_bnc_test/corpus\n",
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "\u001b[A\u001b[A\n",
                        "  0%|          | 0/1125 [00:00<?, ?ex/s]\n",
                        "\n",
                        "\u001b[A\u001b[A\n",
                        " 22%|██▏       | 243/1125 [00:00<00:00, 2409.58ex/s]\n",
                        "\n",
                        "\u001b[A\u001b[A\n",
                        "100%|██████████| 1125/1125 [00:00<00:00, 3154.88ex/s]\n",
                        "\n",
                        "\u001b[A\n",
                        "\n",
                        " 64%|██████▎   | 717/1125 [00:00<00:00, 1811.12ex/s]\n",
                        "\u001b[A\n",
                        "\n",
                        " 84%|████████▍ | 949/1125 [00:00<00:00, 1975.97ex/s]\n",
                        "100%|██████████| 1125/1125 [00:00<00:00, 1873.88ex/s]\n",
                        "100%|██████████| 1125/1125 [00:00<00:00, 1996.88ex/s]\n",
                        "100%|██████████| 1125/1125 [00:00<00:00, 1795.52ex/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "... computed in 0.815479040145874 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.00030612945556640625 secs.\n",
                        "\n",
                        "Alphabet Size: 56\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 939\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5e771b483a474f95b52580182dcc53ed",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/939 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as the 'labels' key of the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Compute freq...\n",
                        "... computed in 0.05032515525817871 secs.\n",
                        "\n",
                        "Vocabulary built\n",
                        "\n",
                        "Vocabulary saved to models/en_bnc_test/vocabulary\n",
                        "\n",
                        "Syntagmatic and Paradigmatic updated with the new vocabulary\n",
                        "\n",
                        "SLG: Compiling model\n",
                        "SLG: Tokenizing dataset...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 9/9 [00:08<00:00,  1.11ba/s]\n",
                        "100%|██████████| 1/1 [00:00<00:00,  2.39ba/s]\n",
                        "100%|██████████| 1/1 [00:00<00:00,  2.48ba/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SLG: Building train set\n",
                        "SLG: Building validation set\n",
                        "SLG: Starting training...\n",
                        "\n",
                        "Epoch 1/2\n",
                        "281/281 [==============================] - 1031s 4s/step - loss: 6.4022 - val_loss: 5.9969\n",
                        "Epoch 2/2\n",
                        "281/281 [==============================] - 1049s 4s/step - loss: 5.8227 - val_loss: 5.5268\n",
                        "SLG: Training finished\n",
                        "\n",
                        "SLG: Model saved.\n",
                        "SLG: Training history saved.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'\\nSLG: Model built!\\n'"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "semiotic.corpus.build(\n",
                "    save = True,\n",
                "    )\n",
                "\n",
                "semiotic.vocab.build(\n",
                "    save = True,\n",
                "    parallel = True,\n",
                "    )\n",
                "\n",
                "semiotic.paradigmatic.build()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "135348\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6b34b1954f694f558fc386bb19ac756f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "<function semiolog.syntagmatic.tree.Tree.plot.<locals>.inter()>"
                        ]
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from random import randint\n",
                "n = randint(0,semiotic.corpus.train.num_rows)\n",
                "# n = 51900\n",
                "sent = semiotic.corpus.train[n][\"text\"]\n",
                "sent\n",
                "\n",
                "sent_seq = semiotic(sent)\n",
                "print(n)\n",
                "sent_seq.tree.plot()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['more', 'athome', 'and', 'on', 'together', 'in', 'increasingly', 'away', 'so', 'very', 'up', 'alittle', 'busy', 'awayfrom', 'from', 'back', 'too', 'alone', 'still', 'apart', 'out', 'seriously', 'moreandmore', 'around', 'un', 'completely', 'alive', 'with', 'quite', 'somewhere', 'heavily', 'well', 'forever', 'deeply', 'but', 'totally', 'abit', 'onthe', 'again', 'after', 'here', 'by', 'as', 'fully', 'down', 'going', 'kept', 'slowly', 'caught', 'quiet', 'today', 'friendly', 'happy', 'while', 'light', 'even', 'short', 'almost', 'off', 'better', 'less', 'left', 'right', 'lying', 'feeling', 'ahead', 'forward', 'some', 'badly', 'closeto', 'further', 'hard', 'withher', 'now', 'awake', 'particularly', 'growing', 'found', 'fresh', 'over', 'thoroughly', 'steadily', 'without', 'inhospital', 'tired', 'quickly', 'dead', 'fairly', 'before', 'warm', 'evenmore', 'talking', 'athim', 'hesaid', 'at', 'firmly', 'gently', 'already', 'easily', 'free', 'inthe', 'or', 'tight', 'outof', 'atall', 'inlondon', 'everywhere', 'there', 'especially', 'soon', 'physically', 'deep', 'onsunday', 'being', 'constantly', 'really', 'active', 'about', 'through', 'rapidly', 'extremely', 'comfortably', 'intouch', 'fast', 'far', 's', 'slightly', 'for', 'inthehouse', 'happily', 'atlast', 'inplace', 'most', 'young', 'fighting', 'themselves', 'born', 'literally', 'tall', 'when', 'immediately', 'held', 'frozen', 'withhim', 'the', 'cold', 'gradually', 'suddenly', 'much', 'tough', 'altogether', 'regularly', 'awayfromthe', 'mostly', 'home', 'withthe', 'all', 'near', 'himself', 'entirely', 'long', 'both', 'rough', 'deliberately', 'clean', 'inforce', 'serious', 'late', 'atrisk', 'toomuch', 'verymuch', 'p', 'morethan', 'slow', 'sexually', 'forawhile', 'high', 'made', 'shortof', 'involvedin', 'mainly', 'buried', 'quietly', 'working', 'thisyear', 'yet', 'spent', 'open', 'close', 'hot', 'somehow', 'rather', 'ever', 'effectively', 'worse', 'somuch', 'rising', 'er', 'lost', 'just', 'atwork', 'water', 'en', 'farfrom', 'however', 'round', 'fishing', 'riding', 'ill', 'angry', 'ayear', 'bitterly', 'themost', 'built', 'somewhat', 'anyway', 'generally', 'flying', 'intheface', 'wild', 'asleep', 'cool', 'good', 'pretty', 'mad', 'behind', 'intheair', 'falling', 'public', 'moving', 'alert', 'ar', 'onceagain', 'withyou', 'utterly', 'mentally', 'smiling', 'dis', 'ed', 'travelling', 'headded', 'aday', 'outside', 'old', 'greatly', 'naturally', 'also', 'silent', 'directly', 'anywhere', 'set', 'often', 'killed', 'lightly', 'during', 'apparently', 'rich', 'wide', 'blind', 'straight', 'eating', 'ir', 'involved', 'seenas', 'outthere', 'abroad', 'united', 'of', 'thenight', 'those', 'relatively', 'ind', 'informed', 'seen', 'withme', 'todate', 'overnight', 'planted', 'whenhewas', 'highly', 'inlove', 'low', 'd', 'absolutely', 'allnight', 'allday', 'throughthe', 'cut', 'full', 'muchmore', 'terribly', 'running', 'injured', 'dry', 'not', 'balanced', 'withthem', 'under', 'successfully', 'ward', 'onthefloor', 'desperately', 'hungry', 'onher', 'shesaid', 'atnight', 'later', 'you', 'atthemoment', 'calm', 'her', 'withhis', 'quick', 'inprison', 'ontheground', 'ormore', 'obviously', 'removed', 'looking', 'andthen', 'walking', 'confident', 'ham', 'closely', 'aswellas', 'sometimes', 'then', 'hand', 'onhis', 'ful', 'strong', 'trouble', 'said', 'head', 'naked', 'heavy', 'politically', 'flat', 'closer', 'ling', 'into', 'outofthe', 'declared', 'heart', 'once', 'herself', 'always', 'suffering', 'usedto', 'playing', 'equally', 'ingand', 'between', 'describedas', 'blood', 'elsewhere', 'properly', 'strongly', 'atthesametime', 'getting', 'beautiful', 'veryvery', 'bitter', 'beaten', 'alongtime', 'inthecity', 'theday', 'only', 'lives', 'th', 'streets', 'because', 'released', 'atpresent', 'upwith', 'shut', 'theriskof', 'moreorless', 'awareof', 'stronger', 'him', 'living', 'people', 'feeding', 'side', 'driving', 'air', 'ontelevision', 'self', 'ing', 'someone', 'treated', 'everyday', 'a', 'introduced', 'atthe', 'formanyyears', 'doing', 'independent', 'simply', 'along', 'something', 'admitted', 'indeed', 'though', 'atleast', 'edand', 'interestedin', 'time', 'accusedof', 'them', 'sitting', 'staying', 'it', 'partially', 'unhappy', 'reasonably', 'thatway', 'atfirst', 'below', 'ontheway', 'virtually', 'ontheroad', 'although', 'perfectly', 'devotedto', 'similarly', 'infact', 'butstill', 'oncemore', 'comfortable', 'fat', 'intothe', 'forced', 'racing', 'som', 'distant', 'used', 'drinking', 'themore', 'able', 'ashore', 'environmentally', 'sweet', 'awarethat', 'breaking', 'thrown', 'waiting', 'life', 'fit', 'largely', 'strictly', 'torn', 'likethis', 'lonely', 'others', 'thebest', 'located', 'sometime', 'theworld', 'us', 'until', 'severely', 'afterwards', 'ofcourse', 'incourt', 'to', 'concerned', 'severe', 'clearly', 'early', 'vulnerable', 'first', 'placed', 'despite', 'inthis', 'having', 'filledwith', 'put', 'tomorrow', 'thesedays', 'hesays', 'upon', 'aggressive', 'weight', 'exposed', 'ent', 'finally', 'inside', 'whilst', 'aneye', 'stuck', 'sh', 'tonight', 'live', 'inbritain', 'run', 'is', 'ath', 'dueto', 'occasionally', 'attimes', 'instantly', 'shesays', 'forwards', 'situated', 'longer', 'higher', 'economically', 'truly', 'potentially', 'ly', 'wards', 'content', 'consciously', 'aswell', 'his', 'rid', 'trying', 'ontop', 'ats', 'tightly', 'committedto', 'feet', 'nomore', 'thinking', 'started', 'inthecountry', 'ers', 'inthedark', 'ontheir', 'fullof', 'cross', 'surprisingly', 'large', 'bringing', 'chargedwith', 'inscotland', 'red', 'intheregion', 'sick', 'bust', 'club', 'goingon', 'thechild', 'bad', 'among', 'becoming', 'personally', 'sufficiently', 'frequently', 'continually', 'atthem', 'wet', 'constant', 'ousand', 'onthem', 'am', 'aweek', 'justas', 'withit', 'enjoyed', 'upto', 'tofeel', 'charged', 'andthe', 'enough', 'threatening', 'beyond', 'merely', 'new', 'fromthe', 'sharply', 'best', 'longterm', 'clear', 'homes', 'facing', 'recently', 'present', 'worried', 'considering', 'ins', 'bored', 'pleased', 'based', 'forthose', 'past', 'available', 'fulltime', 'shorter', 'nodoubt', 'twoyears', 'watching', 'staged', 'writing', 'nearby', 'lastnight', 'furious', 'ous', 'edby', 'ather', 'andso', 'actively', 'staring', 'bright', 'carefully', 'otherwise', 'operating', 'organised', 'thecountry', 'coming', 'erm', 'south', 'wounded', 'yourself', 'ped', 'easy', 'if', 'worn', 'per', 'alongwith', 'spending', 'm', 'remaining', 'hands', 'sofar', 'against', 'endless', 'isolated', 'standing', 'towards', 'ts', 'behindhim', 'soft', 'hishead', 'atatime', 'ons', 'since', 'successful', 'stop', 'actually', 'solong', 'exactly', 'anymore', 'breathing', 'inthenorth', 'tobe', 'buthewas', 'ground', 'improving', 'allthetime', 'fall', 'effective', 'tostay', 'init', 'butnow', 'hidden', 'older', 'behindthe', 'beginning', 'ready', 'infrance', 'justified', 'counter', 'nearly', 'bed', 'feel', 'freely', 'possibly', 'alongway', 'war', 'thick', 'grow', 'lessly', 'stiff', 'barely', 'innewyork', 'an', 'aking', 'ared', 'hunting', 'london', 'incredibly', 'named', 'established', 'drunk', 'expressed', 'independently', 'faster', 'half', 'upthere', 'alot', 'east', 'thisweek', 'opened', 'everyyear', 'captured', 'satisfied', 'afirm', 'ab', 'ands', 'shallow', 'this', 'onthis', 'responsiblefor', 'ast', 'worth', 'like']"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sent_seq.chain.tokens[6].paradigm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'input_ids': [10930, 10681, 3687, 7, 10443, 4389, 7, 6944, 6208, 18108, 8, 8217, 1704, 78, 139, 1285, 11393, 13, 496, 4389, 45, 101, 216, 917, 895, 203, 4433, 12, 582], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
                        ]
                    },
                    "execution_count": 42,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "tok_output = semiotic.syntagmatic.bert_tokenizer(semiotic.corpus.train[n][\"text\"])\n",
                "tok_output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'whoever hadtaken thedecision to commit theparty to armed insur rection in 1930 whatwas now tobe called inquestion was whether theparty had any right atall tomake such decisions for itself'"
                        ]
                    },
                    "execution_count": 45,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "semiotic.syntagmatic.bert_tokenizer.decode(tok_output[\"input_ids\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as the 'labels' key of the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SLG: Compiling model\n",
                        "SLG: Tokenizing dataset...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 37/10594 [00:53<4:13:23,  1.44s/ba]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-3-178ba1324395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msemiotic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparadigmatic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m~/semiolog/semiolog/paradigmatic/__init__.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, dataset, save)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SLG: Tokenizing dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         tokenized_datasets = dataset.map(\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mtokenize_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mbatched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_tokenize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batched\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mcache_file_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         return DatasetDict(\n\u001b[0;32m--> 486\u001b[0;31m             {\n\u001b[0m\u001b[1;32m    487\u001b[0m                 k: dataset.map(\n\u001b[1;32m    488\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    485\u001b[0m         return DatasetDict(\n\u001b[1;32m    486\u001b[0m             {\n\u001b[0;32m--> 487\u001b[0;31m                 k: dataset.map(\n\u001b[0m\u001b[1;32m    488\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2017\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2018\u001b[0;31m             return self._map_single(\n\u001b[0m\u001b[1;32m   2019\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m         }\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2387\u001b[0m                         )  # Something simpler?\n\u001b[1;32m   2388\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[1;32m   2390\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2275\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2276\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2277\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2278\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2279\u001b[0m                 \u001b[0;31m# Check if the function returns updated examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1976\u001b[0m                 )\n\u001b[1;32m   1977\u001b[0m                 \u001b[0;31m# Use the LazyDict internally, while mapping the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1978\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecorated_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1979\u001b[0m                 \u001b[0;31m# Return a standard dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1980\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/semiolog/semiolog/paradigmatic/__init__.py\u001b[0m in \u001b[0;36mtokenize_function\u001b[0;34m(syntagmas)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyntagmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyntagmas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_special_tokens_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SLG: Tokenizing dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2411\u001b[0m                 )\n\u001b[1;32m   2412\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2413\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2414\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2415\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2596\u001b[0m         )\n\u001b[1;32m   2597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2598\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2599\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "semiotic.paradigmatic.build()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "cccfb5a2fd0f3598afe9b50a9347d4d83dedd025ace78c5a47e49a8025e0aa16"
        },
        "kernelspec": {
            "display_name": "Python 3.9.4 64-bit ('3.9.4')",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
