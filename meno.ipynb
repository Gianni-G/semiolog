{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from os import listdir\n",
                "import datasets\n",
                "from nltk.tokenize import sent_tokenize\n",
                "\n",
                "import semiolog as slg"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SLG: Checking config correctness...\n",
                        "SLG: Config correct!\n",
                        "SLG Warning: models/abacus/corpus/train.txt does not exist.\n",
                        "Corpus will not be loaded from file.\n",
                        "\n",
                        "SLG Warning: models/abacus/vocabulary/merges.txt does not exist.\n",
                        "Vocabulary will not be loaded from file.\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "semiotic = slg.Cenematic(\"abacus\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "path_orig = semiotic.corpus.path / \"original\"\n",
                "fns = [fn[:-4] for fn in listdir(path_orig) if fn.endswith(\".txt\")]\n",
                "text = []\n",
                "source = []\n",
                "for fn in fns:\n",
                "    treatise_raw = slg.util.txt2list(fn,path_orig)\n",
                "    treatise_sents = []\n",
                "    for paragraph in treatise_raw:\n",
                "        treatise_sents += sent_tokenize(paragraph)\n",
                "    text += treatise_sents\n",
                "    source += [fn]*len(treatise_sents)\n",
                "\n",
                "dataset = datasets.Dataset.from_dict({\"text\":text, \"source\":source})\n",
                "dataset.save_to_disk(semiotic.corpus.path/\"dataset\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 2.118035078048706 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.00043272972106933594 secs.\n",
                        "\n",
                        "Alphabet Size: 56\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 1939\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "951ca7fd5f5a441aaaacaffdb1f412c5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/1939 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Compute freq...\n",
                        "... computed in 0.05408000946044922 secs.\n",
                        "\n",
                        "Vocabulary built\n",
                        "\n",
                        "Syntagmatic and Paradigmatic updated with the new vocabulary\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "semiotic.vocab.build(\n",
                "    save = False,\n",
                "    vocab_size=2000,\n",
                "    parallel = True,\n",
                "    save_step=20\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['i',\n",
                            " 'have',\n",
                            " 'm',\n",
                            " 'ad',\n",
                            " 'em',\n",
                            " 'y',\n",
                            " 'pl',\n",
                            " 'an',\n",
                            " 'sand',\n",
                            " 'im',\n",
                            " 'ust',\n",
                            " 'st',\n",
                            " 'ic',\n",
                            " 'k',\n",
                            " 'tothe',\n",
                            " 'm']"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sent = \"I have made my plans and I must stick to them\"\n",
                "sent_seq = semiotic(sent)\n",
                "sent_seq.chain.labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['ihave', 'made', 'my', 'plan', 'sand', 'i', 'must', 'st', 'ick', 'to', 'them']"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sent_seq = semiotic(sent)\n",
                "sent_seq.chain.labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
        },
        "kernelspec": {
            "display_name": "Python 3.9.5 64-bit",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
