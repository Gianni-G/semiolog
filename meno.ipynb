{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "import semiolog as slg"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SLG Warning: models/abacus/corpus/train.txt does not exist.\n",
                        "Corpus will not be loaded from file.\n",
                        "\n",
                        "SLG Warning: models/abacus/vocabulary/merges.txt does not exist.\n",
                        "Vocabulary will not be loaded from file.\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "semiotic = slg.Cenematic(\"abacus\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Resolving data files: 100%|██████████| 26/26 [00:00<00:00, 122118.59it/s]\n",
                        "Using custom data configuration original-ff77e2635de9851d\n",
                        "Reusing dataset text (/Users/Gianni/.cache/huggingface/datasets/text/original-ff77e2635de9851d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
                        "100%|██████████| 1/1 [00:00<00:00, 280.80it/s]\n",
                        "Loading cached processed dataset at /Users/Gianni/.cache/huggingface/datasets/text/original-ff77e2635de9851d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-bed11e4206b2d975.arrow\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "SLG: Dataset loaded from the following files: [\"Toti Rigatelli_L_1984_Gori_Praticha d'alcibra_S.txt\", \"Franci_R_2015_Un trattato d'abaco pisano della fine del XIII secolo (MS)_Working copy.txt\", 'Simi_A_1992_Anonimo fiorentino_Regole di geometria e della cosa_S.txt', 'Arrighi_G_1967{c}_Trascelta di Giovanni di Bartolo_S.txt', 'Arrighi_G_1999_Anonimo_Algorismus_S.txt', 'Arrighi_G_1964_(ps)-Paolo_S.txt', \"Arrighi_G_1989{a}_Livero de l'abbecho_S.txt\", \"Arrighi_G_1973_Scuola lucchese--LIbro d'abaco_S.txt\", \"Franci_R_1983{a}_Gilio_Questioni d'algebra_S.txt\", \"Jens Hoyrup - Jacopo da Firenze's Tractatus Algorismi and Early Italian Abbacus Culture (Science Networks. Historical Studies) (2007).txt\", 'Arrighi_G_1967{a}_Mazzingi-fioretti_S.txt', 'Simi_A_1994_Alcibra amuchabile_S.txt', \"Arrighi_G_1974_P-M-Calandri[Benedetto]_Tractato-d'abbacho_S.txt\", \"Franci_R_2015_Un trattato d'abaco pisano della fine del XIII secolo_S_OK.txt\", 'Arrighi_G_1987{f}_Gherardi+Liber habaci_S.txt', 'Franci_R_2001_Dardi-Aliabraa argibra_S.txt', 'Arrighi_G_1969_Filippo Calandri_Trattato di Aritmetica_S.txt', 'Gregori_S_&Grugnetti_1998__Anonimo_Libro di conti e mercatanzie_S.txt', \"Arrighi_G_1970_Piero-Trattato d'abbaco_S.txt\", 'Arrighi_G_1964{a}_Guglielmo_De arithmetica compendiose tractata_S.txt', \"Pieraccini_L_1983_Maestro Biagio_Chasi exemplari alla regola dell'argibra_S.txt\", \"Franci_R_1988{c}_&Pancanti_Trattato d'algibra_S.txt\", \"Arrighi_G_1966{b}_Paolo dell'Abbaco_Regoluzze_S.txt\", 'Gino Arrighi - Maestro Umbro (sec. XIII), LIVERO DE L’ABBECHO (1987).txt', 'Simi_A_1995_Jacopo-Riccardiana 2236_S.txt', \"Arrighi_G_1987_Giovanni de' Danti_Tractato de l'algorisimo_S.txt\", 'Paolo Gherardi, Gino Arrighi - OPERA MATEMATICA - Libro ài ragioni - Liber habaci-Maria Pacini Pazzi (1987).txt'].\n",
                        "\n",
                        "Spliting original text into sentences.\n",
                        "Corpus built\n"
                    ]
                }
            ],
            "source": [
                "semiotic.corpus.build(\n",
                "    save = False,\n",
                "    split_sent= True,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "DatasetDict({\n",
                            "    train: Dataset({\n",
                            "        features: ['text'],\n",
                            "        num_rows: 63660\n",
                            "    })\n",
                            "    dev: Dataset({\n",
                            "        features: ['text'],\n",
                            "        num_rows: 3537\n",
                            "    })\n",
                            "    test: Dataset({\n",
                            "        features: ['text'],\n",
                            "        num_rows: 3537\n",
                            "    })\n",
                            "})"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "semiotic.corpus.dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "path = str(semiotic.paths.corpus/\"original\"/\"test.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Using custom data configuration default-53c8efaf25db273b\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Downloading and preparing dataset csv/default to /Users/Gianni/.cache/huggingface/datasets/csv/default-53c8efaf25db273b/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1/1 [00:00<00:00, 3472.11it/s]\n",
                        "100%|██████████| 1/1 [00:00<00:00, 343.96it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset csv downloaded and prepared to /Users/Gianni/.cache/huggingface/datasets/csv/default-53c8efaf25db273b/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1/1 [00:00<00:00, 253.11it/s]\n"
                    ]
                }
            ],
            "source": [
                "dataset = load_dataset('csv', data_files=path, delimiter=',', column_names=['text', 'work'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'text': ['text',\n",
                            "  'MS. 2236 della Biblioteca Riccardiana di Firenze',\n",
                            "  's. XIV (1307, d) Montpellier',\n",
                            "  'lr - 48v JACOPO DA FIRENZE. TRACTATUSALGORISMI.',\n",
                            "  '(Ir) Incipit tractatus algorismi. Huius autem artis novem sunt species, silicet numeratio, additio, subtractio, mediatio, duplatio, multiplicatio, divisio, progressio et radicum extractio.'],\n",
                            " 'work': ['book',\n",
                            "  'Simi_A_1995_Jacopo-Riccardiana 2236_S',\n",
                            "  'Simi_A_1995_Jacopo-Riccardiana 2236_S',\n",
                            "  'Simi_A_1995_Jacopo-Riccardiana 2236_S',\n",
                            "  'Simi_A_1995_Jacopo-Riccardiana 2236_S']}"
                        ]
                    },
                    "execution_count": 38,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dataset[\"train\"][:5]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "bla = Dataset.from_dict({\"text\":[\"bla1\", \"bla3\", \"bla2\"], \"year\": [1978,1920,1910]})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'text': Value(dtype='string', id=None),\n",
                            " 'work': Value(dtype='string', id=None)}"
                        ]
                    },
                    "execution_count": 40,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dataset[\"train\"].features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Downloading: 5.27kB [00:00, 1.38MB/s]                   \n",
                        "Downloading: 2.36kB [00:00, 711kB/s]                    \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /Users/Gianni/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Downloading: 30.3MB [00:00, 51.3MB/s]\n",
                        "Downloading: 4.85MB [00:00, 6.41MB/s].66s/it]\n",
                        "100%|██████████| 2/2 [00:02<00:00,  1.46s/it]\n",
                        "100%|██████████| 2/2 [00:00<00:00, 585.31it/s]\n",
                        "                                           "
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset squad downloaded and prepared to /Users/Gianni/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\r"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'id': Value(dtype='string', id=None),\n",
                            " 'title': Value(dtype='string', id=None),\n",
                            " 'context': Value(dtype='string', id=None),\n",
                            " 'question': Value(dtype='string', id=None),\n",
                            " 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from datasets import load_dataset\n",
                "dataset = load_dataset('squad', split='train')\n",
                "dataset.features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['5733be284776f41900661182',\n",
                            " '5733be284776f4190066117f',\n",
                            " '5733be284776f41900661180',\n",
                            " '5733be284776f41900661181',\n",
                            " '5733be284776f4190066117e',\n",
                            " '5733bf84d058e614000b61be',\n",
                            " '5733bf84d058e614000b61bf',\n",
                            " '5733bf84d058e614000b61c0',\n",
                            " '5733bf84d058e614000b61bd',\n",
                            " '5733bf84d058e614000b61c1']"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dataset[\"id\"][:10]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Dataset({\n",
                            "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
                            "    num_rows: 87599\n",
                            "})"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "DatasetDict({\n",
                            "    train: Dataset({\n",
                            "        features: ['text'],\n",
                            "        num_rows: 63660\n",
                            "    })\n",
                            "    dev: Dataset({\n",
                            "        features: ['text'],\n",
                            "        num_rows: 3537\n",
                            "    })\n",
                            "    test: Dataset({\n",
                            "        features: ['text'],\n",
                            "        num_rows: 3537\n",
                            "    })\n",
                            "})"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "semiotic.corpus.dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "DatasetDict({\n",
                            "    train: Dataset({\n",
                            "        features: ['text'],\n",
                            "        num_rows: 20898\n",
                            "    })\n",
                            "    dev: Dataset({\n",
                            "        features: ['text'],\n",
                            "        num_rows: 1161\n",
                            "    })\n",
                            "    test: Dataset({\n",
                            "        features: ['text'],\n",
                            "        num_rows: 1161\n",
                            "    })\n",
                            "})"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "semiotic.corpus.dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def bla_load_dataset(dataset = None, original=False):\n",
                "\n",
                "    if original:\n",
                "        load_path = semiotic.paths.corpus / 'original'\n",
                "    else:\n",
                "        load_path = semiotic.paths.corpus\n",
                "\n",
                "    if dataset == None:\n",
                "        dataset = [fn for fn in listdir(load_path) if fn.endswith(\".txt\")]\n",
                "        if len(dataset) == 1:\n",
                "            dataset = dataset[0]\n",
                "\n",
                "    data = load_dataset(str(load_path), data_files=dataset)\n",
                "    \n",
                "    return data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "bla_dataset = DatasetDict()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "dataset = [fn for fn in listdir(semiotic.paths.corpus / 'original') if fn.endswith(\".txt\")]\n",
                "if len(dataset) == 1:\n",
                "    dataset = dataset[0]\n",
                "split_rate = semiotic.config.corpus.split_rate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bla_dataset = bla_load_dataset(dataset, original=True)\n",
                "print(f\"\\nSLG: Dataset loaded from the following files: {dataset}.\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "from nltk.tokenize import sent_tokenize\n",
                "\n",
                "corpus_sents = []\n",
                "for passage in bla_dataset[\"train\"][\"text\"]:\n",
                "    corpus_sents += sent_tokenize(passage)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 2.118035078048706 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.00043272972106933594 secs.\n",
                        "\n",
                        "Alphabet Size: 56\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 1939\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "951ca7fd5f5a441aaaacaffdb1f412c5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/1939 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Compute freq...\n",
                        "... computed in 0.05408000946044922 secs.\n",
                        "\n",
                        "Vocabulary built\n",
                        "\n",
                        "Syntagmatic and Paradigmatic updated with the new vocabulary\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "semiotic.vocab.build(\n",
                "    save = False,\n",
                "    vocab_size=2000,\n",
                "    parallel = True,\n",
                "    save_step=20\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['i',\n",
                            " 'have',\n",
                            " 'm',\n",
                            " 'ad',\n",
                            " 'em',\n",
                            " 'y',\n",
                            " 'pl',\n",
                            " 'an',\n",
                            " 'sand',\n",
                            " 'im',\n",
                            " 'ust',\n",
                            " 'st',\n",
                            " 'ic',\n",
                            " 'k',\n",
                            " 'tothe',\n",
                            " 'm']"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sent = \"I have made my plans and I must stick to them\"\n",
                "sent_seq = semiotic(sent)\n",
                "sent_seq.chain.labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['ihave', 'made', 'my', 'plan', 'sand', 'i', 'must', 'st', 'ick', 'to', 'them']"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sent_seq = semiotic(sent)\n",
                "sent_seq.chain.labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
        },
        "kernelspec": {
            "display_name": "Python 3.9.5 64-bit",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
