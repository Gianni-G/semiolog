{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import datasets\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "\n",
    "import semiolog as slg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    rc = {'figure.figsize':(25,20)},\n",
    "    font=\"Courier\"\n",
    "    )\n",
    "\n",
    "def plot(vector,\n",
    "    xticklabels = None,\n",
    "    yticklabels = None,\n",
    "    vmin = None,\n",
    "    vmax = None,\n",
    "    labelbottom = False,\n",
    "    labelright = False,\n",
    "    save = None,\n",
    "\n",
    "    ):\n",
    "    if xticklabels == None:\n",
    "        xticklabels = list(range(vector.shape[0]))\n",
    "    if yticklabels == None:\n",
    "        yticklabels = list(range(vector.shape[1]))\n",
    "    hm = sns.heatmap(\n",
    "        vector,\n",
    "        xticklabels=xticklabels,\n",
    "        yticklabels=yticklabels,\n",
    "        linewidths=.5,\n",
    "        cmap=\"coolwarm\",\n",
    "        center = 0,\n",
    "        vmin = vmin,\n",
    "        vmax = vmax,\n",
    "        square=True,\n",
    "        ).tick_params(\n",
    "            axis='both',\n",
    "            which='major',\n",
    "            labelsize=11,\n",
    "            labelbottom = labelbottom,\n",
    "            labelright = labelright, \n",
    "            bottom=False, \n",
    "            top = False, \n",
    "            labeltop=True)\n",
    "    \n",
    "    plt.yticks(rotation=0) \n",
    "    if save != None:\n",
    "        plt.savefig(save)\n",
    "    return hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG [I]: Checking config correctness... Config correct!\n",
      "SLG [I]: Dataset loaded from disk (dataset file)\n",
      "SLG [I]: Vocabulary loaded from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at models/abacus/paradigms/tf_model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG [I]: Paradigmatizer loaded from disk\n"
     ]
    }
   ],
   "source": [
    "semiotic = slg.Cenematic(\"abacus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = slg.syntagmatic.tokenizer.Normalize_menoSLG.normalize_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vocab_encode = defaultdict(int)\n",
    "i = 0\n",
    "for key, val in semiotic.vocab.encode.items():\n",
    "    key = normalizer(key)\n",
    "    if key not in norm_vocab_encode:\n",
    "        norm_vocab_encode[key] = i\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Term-Context Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e31fe1db894b4aaa5fe3e16da74570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_size = 5 # nÂ° of words to each side (L&R)\n",
    "freq_counter = defaultdict(int)\n",
    "\n",
    "for passage in tqdm(semiotic.corpus.train[\"text\"]):\n",
    "    passage = normalizer(passage)\n",
    "    word_list = [\"\"]*context_size + passage.split() + [\"\"]*context_size\n",
    "    for context in zip(*[word_list[i:] for i in range(context_size*2+1)]):\n",
    "        row_i = norm_vocab_encode[context[context_size]]\n",
    "        for c in context[:context_size]+context[context_size+1:]:\n",
    "            if c != \"\":\n",
    "                col_i = norm_vocab_encode[c]\n",
    "                freq_counter[(row_i,col_i)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "row_indxs = []\n",
    "col_indxs = []\n",
    "dat_vals = []\n",
    "for (row,col),value in freq_counter.items():\n",
    "    row_indxs.append(row)\n",
    "    col_indxs.append(col)\n",
    "    dat_vals.append(value)\n",
    "\n",
    "M_freq = csr_matrix((dat_vals, (row_indxs, col_indxs)),shape=(semiotic.vocab.len,semiotic.vocab.len))\n",
    "M_pmi = slg.util.pmi(M_freq, type_pmi=\"npmi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build focus term corpus and matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3746d72dc1f4a6e99622dadff63dcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "focus_term = \"meno\"\n",
    "\n",
    "focus_corpus = semiotic.corpus.train.filter(lambda paragraph: focus_term in paragraph[\"text\"])\n",
    "source_encode = {s:i for i,s in enumerate(sorted(list(set(focus_corpus[\"source\"]))))}\n",
    "source_decode = {i:s for i,s in enumerate(sorted(list(set(focus_corpus[\"source\"]))))}\n",
    "\n",
    "focus_counter = defaultdict(int)\n",
    "focus_collocations = []\n",
    "focus_sources = []\n",
    "i=0\n",
    "for passage in tqdm(focus_corpus):\n",
    "    word_list = [\"\"]*context_size + normalizer(passage[\"text\"]).split() + [\"\"]*context_size\n",
    "    for context in zip(*[word_list[i:] for i in range(context_size*2+1)]):\n",
    "        if context[context_size] == focus_term:\n",
    "            focus_collocations.append(\" \".join(context))\n",
    "            focus_sources.append(source_encode[passage[\"source\"]])\n",
    "            row_i = i\n",
    "            for c in context[:context_size]+context[context_size+1:]:\n",
    "                if c != \"\":\n",
    "                    col_i = norm_vocab_encode[c]\n",
    "                    focus_counter[(row_i,col_i)] += 1\n",
    "            i += 1\n",
    "row_indxs = []\n",
    "col_indxs = []\n",
    "dat_vals = []\n",
    "for (row,col),value in focus_counter.items():\n",
    "    row_indxs.append(row)\n",
    "    col_indxs.append(col)\n",
    "    dat_vals.append(value)\n",
    "\n",
    "M_focus = csr_matrix((dat_vals, (row_indxs, col_indxs)),shape=(i,semiotic.vocab.len))\n",
    "M_focus_pmi = slg.util.pmi(M_focus, type_pmi=\"npmi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slg.util.save_file(focus_collocations, semiotic.paths.corpus / \"meno_collocations.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster focus matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cluster = 2\n",
    "kmeans = KMeans(n_clusters=n_cluster, random_state=0, verbose = 0).fit(M_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "kmeans_pmi = slg.util.pmi(kmeans.cluster_centers_, type_pmi=\"npmi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"single\", \"complete\", \"average\", \"weighted\", \"centroid\", \"median\", \"ward\"]\n",
    "method = \"ward\"\n",
    "\n",
    "row_n = 1400\n",
    "col_n = 1400\n",
    "spec_token_len = len(semiotic.vocab.config.special_tokens)\n",
    "\n",
    "matrix = M_pmi[spec_token_len:row_n,spec_token_len:col_n].toarray()\n",
    "elements = list(norm_vocab_encode.keys())[spec_token_len:col_n]\n",
    "\n",
    "meno_vector = M_freq[norm_vocab_encode[\"meno\"]]\n",
    "# elements_col = [list(semiotic.vocab.freq.keys())[i] for i in meno_vector.indices]\n",
    "elements_col = elements\n",
    "elements_row = elements\n",
    "elements_row_dict = {e:i for i,e in enumerate(elements_row)}\n",
    "# matrix = M_pmi[:,meno_vector.indices][spec_token_len:row_n].toarray()\n",
    "\n",
    "\n",
    "row_linkage = hierarchy.linkage(\n",
    "    distance.pdist(matrix), method=method)\n",
    "col_linkage = hierarchy.linkage(\n",
    "    distance.pdist(matrix.T), method=method)\n",
    "\n",
    "linkage  = col_linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae4e7f436ec4925bc14d8ce977060e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='p', max=20, min=1), Dropdown(description='truncate_modeâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "truncate_label = 200\n",
    "def llf(R, id):\n",
    "    n = len(elements)\n",
    "    if id < n:\n",
    "        return elements[id]\n",
    "    else:\n",
    "        label = [id]\n",
    "        while max(label)>=n:\n",
    "            new_label = []\n",
    "            for l in label:\n",
    "                if l<n:\n",
    "                    new_label.append(l)\n",
    "                else:\n",
    "                    new_label.append(int(R[l-n,0]))\n",
    "                    new_label.append(int(R[l-n,1]))\n",
    "            label = new_label\n",
    "\n",
    "        label = \"{ \" + \" \".join([elements[l] if (i+1)%3>0 else \"\\n\"+elements[l] for i,l in enumerate(label[:truncate_label])]) + (\"...\" if len(label)>truncate_label else \"\") + \" }\"\n",
    "        return label\n",
    "\n",
    "fig = plt.figure(figsize=(25, 5))\n",
    "plt.rcParams.update({'axes.facecolor':'white'})\n",
    "# plt.rcdefaults()\n",
    "@interact(p=(1,min(20,len(elements)),1), truncate_mode = [\"lastp\", \"level\"], save = False)\n",
    "def h_clus_cols(p=min(20,len(elements)),truncate_mode = \"lastp\", save = False):\n",
    "    plt.figure(figsize=(25, 5))\n",
    "    hierarchy.dendrogram(\n",
    "        linkage, #col_linkage,\n",
    "        labels=elements,\n",
    "        p = p,\n",
    "        color_threshold = 3,\n",
    "        truncate_mode = truncate_mode, #\"lastp\", #'lastp', None, \"level\"\n",
    "        leaf_label_func=partial(llf, linkage),\n",
    "        leaf_rotation=0,\n",
    "        get_leaves = True\n",
    "        );\n",
    "    plt.grid(b=True, which='major', axis='y',color='lightgrey', linestyle='-', linewidth=.1)\n",
    "    # plt.xlabel('Clusters',fontsize=16)\n",
    "    plt.ylabel(f'distance ({method})',fontsize=16)\n",
    "    plt.xticks(fontfamily=\"monospace\",fontsize=10),\n",
    "    if save:\n",
    "        plt.savefig(path_media + \"dendrogram.pdf\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_labels(labels, truncate=None, width = 3):\n",
    "    if isinstance(truncate,int) and len(labels)>truncate:\n",
    "        labels = (labels[:truncate])+[\"...\"]\n",
    "    labels = \"{\" + \"|\".join([l if (i+1)%width>0 else \"\\n\"+l for i,l in enumerate(labels)]) + \"}\"\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fb0e16d7074efea11bc0ffd28a6a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='row_thres', min=2), IntSlider(value=5, description='col_â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def type_matrix(row_thres=5, col_thres=5, concepts=\"meno\",separate = False, focus_cluster = False):\n",
    "\n",
    "    cluster_labels_rows = hierarchy.fcluster(row_linkage, row_thres, criterion='maxclust')\n",
    "    cluster_labels_cols = hierarchy.fcluster(col_linkage, col_thres, criterion='maxclust')\n",
    "\n",
    "    max_cluster_rows = max(cluster_labels_rows)\n",
    "    max_cluster_cols = max(cluster_labels_cols)\n",
    "\n",
    "    # This should be simpler without dict, calling elements of lists\n",
    "    row_dict = {row:np.argwhere(cluster_labels_rows==row) for row in range(1,max_cluster_rows+1)}\n",
    "    col_dict = {col:np.argwhere(cluster_labels_cols==col).flatten() for col in range(1,max_cluster_cols+1)}\n",
    "    \n",
    "    if concepts!=None:\n",
    "        concepts = concepts.split()\n",
    "        if separate:\n",
    "            i = 1\n",
    "            for c in concepts:\n",
    "                if c in elements_row:\n",
    "                    row_dict[row_thres+i]= np.array([[elements_row_dict.get(c)]]).T\n",
    "                    i += 1\n",
    "        \n",
    "        else:\n",
    "            row_dict[row_thres+1] = np.array([[elements_row_dict.get(c) for c in concepts if c in elements_row]]).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    label_row = [np.array(elements_row)[row_dict[row].flatten()].tolist() for row in range(1,len(row_dict)+1)]\n",
    "    label_row = [format_labels(row,10,4) for row in label_row]\n",
    "\n",
    "    label_col = [np.array(elements_col)[col_dict[col]].tolist() for col in range(1,max_cluster_cols+1)]\n",
    "    label_col = [format_labels(row,10,1) for row in label_col]\n",
    "\n",
    "\n",
    "    type_m = np.zeros(shape=(len(row_dict), col_thres))\n",
    "    for r in range(len(row_dict)):\n",
    "        for c in range(col_thres):\n",
    "            type_m[r,c] = M_pmi[row_dict[r+1],col_dict[c+1]].mean()\n",
    "\n",
    "    if focus_cluster:\n",
    "        type_m = np.vstack((type_m,kmeans_pmi[:,:type_m.shape[1]].toarray()))\n",
    "        label_row = label_row + [f\"{focus_term}_{i+1}\" for i in range(kmeans_pmi.shape[0])]\n",
    "\n",
    "            \n",
    "    sns.set(\n",
    "        rc = {'figure.figsize':(40,20)},\n",
    "        font=\"Courier\"\n",
    "        )\n",
    "    matrix_plot = plot(type_m,label_col,label_row)\n",
    "    return matrix_plot\n",
    "\n",
    "interact(type_matrix, row_thres=(2, 100, 1), col_thres=(2,100,1), concepts=widgets.Text(\"meno\",description=\"Extra Cluster\", continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quando li cubi, li censi, le cose sono equali al numero, se vole partire le cose per li censi e quello che ne vene recare a ra-dici cuba c ponare sopra il numero; et radici cuba di quello meno il partimento che venne de le cose nelli censi, vale la cosa.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_corpus[100][\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'famme quista ragione uno me dÃ¨ie dare d em 5 pagamenta divisate luno da laltro sÃ¬ cho si chontiene qui da piÃ¨ie la quale ragione vuole redure a uno termene o a doie termene o a doie pagamenta sÃ¬ che sen stan luno laltro a dericto e a ponto devemo recevere da uno dÃ¬ 9 esfiento octovre lib 1317 anche devemo recevere a dÃ¬ 9 esfiente octovre lib 628 anche devemo avere a mego novenbre lib 293 anche devemo entrante decenbre lib 979 anche devemo avere em kl de genaio lib 2594 somma lib 5811 prima dÃ¨ie avere quillo che remane en eli mego 1 d 217463 quista Ã¨ la deritta regola chomo se dÃ¨ie fare quista ragione e tutte le semeglante ragione che se podessero dire de piÃ¹ e de meno che noie devemo fare la somma de tutto el chapetale de la quale deveraie essere partedore la quale somma se dÃ¨ie escrivere sÃ¬ chomo sta qui de sopra e da che noie avemo chusÃ¬ fatto sÃ¬ devemo multiplichare tutte le lib de la prima ragione chom gle dÃ¬ del primo termene em fine a laltro termene che sonno mese 7 e dÃ¬ 1 piÃ¹ gioÃ¨ dÃ¬ 111 contra lib 1317 e quilla multiplichatione sÃ¬ devemo noie escriverla da una parte e quando Ã ie facto luna sÃ¬ devemo fare laltra e da giognere tutte le multiplichatione em n una somma e tutta quilla somma sÃ¬ devemo partire em tutto lo chapetale e'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer(focus_corpus[1000][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
