{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import datasets\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "\n",
    "import semiolog as slg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    rc = {'figure.figsize':(25,20)},\n",
    "    font=\"Courier\"\n",
    "    )\n",
    "\n",
    "def plot(vector,\n",
    "    xticklabels = None,\n",
    "    yticklabels = None,\n",
    "    vmin = None,\n",
    "    vmax = None,\n",
    "    labelbottom = False,\n",
    "    labelright = False,\n",
    "    save = None,\n",
    "\n",
    "    ):\n",
    "    if xticklabels == None:\n",
    "        xticklabels = list(range(vector.shape[0]))\n",
    "    if yticklabels == None:\n",
    "        yticklabels = list(range(vector.shape[1]))\n",
    "    hm = sns.heatmap(\n",
    "        vector,\n",
    "        xticklabels=xticklabels,\n",
    "        yticklabels=yticklabels,\n",
    "        linewidths=.5,\n",
    "        cmap=\"coolwarm\",\n",
    "        center = 0,\n",
    "        vmin = vmin,\n",
    "        vmax = vmax,\n",
    "        square=True,\n",
    "        ).tick_params(\n",
    "            axis='both',\n",
    "            which='major',\n",
    "            labelsize=11,\n",
    "            labelbottom = labelbottom,\n",
    "            labelright = labelright, \n",
    "            bottom=False, \n",
    "            top = False, \n",
    "            labeltop=True)\n",
    "    \n",
    "    plt.yticks(rotation=0) \n",
    "    if save != None:\n",
    "        plt.savefig(save)\n",
    "    return hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG [I]: Checking config correctness... Config correct!\n",
      "SLG [I]: Dataset loaded from disk (dataset file)\n",
      "SLG [I]: Vocabulary loaded from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at models/abacus/paradigms/tf_model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG [I]: Paradigmatizer loaded from disk\n"
     ]
    }
   ],
   "source": [
    "semiotic = slg.Cenematic(\"abacus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = slg.syntagmatic.tokenizer.Normalize_menoSLG.normalize_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vocab_encode = defaultdict(int)\n",
    "i = 0\n",
    "for key, val in semiotic.vocab.encode.items():\n",
    "    key = normalizer(key)\n",
    "    if key not in norm_vocab_encode:\n",
    "        norm_vocab_encode[key] = i\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Term-Context Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e31fe1db894b4aaa5fe3e16da74570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_size = 5 # n° of words to each side (L&R)\n",
    "freq_counter = defaultdict(int)\n",
    "\n",
    "for passage in tqdm(semiotic.corpus.train[\"text\"]):\n",
    "    passage = normalizer(passage)\n",
    "    word_list = [\"\"]*context_size + passage.split() + [\"\"]*context_size\n",
    "    for context in zip(*[word_list[i:] for i in range(context_size*2+1)]):\n",
    "        row_i = norm_vocab_encode[context[context_size]]\n",
    "        for c in context[:context_size]+context[context_size+1:]:\n",
    "            if c != \"\":\n",
    "                col_i = norm_vocab_encode[c]\n",
    "                freq_counter[(row_i,col_i)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "row_indxs = []\n",
    "col_indxs = []\n",
    "dat_vals = []\n",
    "for (row,col),value in freq_counter.items():\n",
    "    row_indxs.append(row)\n",
    "    col_indxs.append(col)\n",
    "    dat_vals.append(value)\n",
    "\n",
    "M_freq = csr_matrix((dat_vals, (row_indxs, col_indxs)),shape=(semiotic.vocab.len,semiotic.vocab.len))\n",
    "M_pmi = slg.util.pmi(M_freq, type_pmi=\"npmi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build focus term corpus and matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3746d72dc1f4a6e99622dadff63dcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "focus_term = \"meno\"\n",
    "\n",
    "focus_corpus = semiotic.corpus.train.filter(lambda paragraph: focus_term in paragraph[\"text\"])\n",
    "source_encode = {s:i for i,s in enumerate(sorted(list(set(focus_corpus[\"source\"]))))}\n",
    "source_decode = {i:s for i,s in enumerate(sorted(list(set(focus_corpus[\"source\"]))))}\n",
    "\n",
    "focus_counter = defaultdict(int)\n",
    "focus_collocations = []\n",
    "focus_sources = []\n",
    "i=0\n",
    "for passage in tqdm(focus_corpus):\n",
    "    word_list = [\"\"]*context_size + normalizer(passage[\"text\"]).split() + [\"\"]*context_size\n",
    "    for context in zip(*[word_list[i:] for i in range(context_size*2+1)]):\n",
    "        if context[context_size] == focus_term:\n",
    "            focus_collocations.append(\" \".join(context))\n",
    "            focus_sources.append(source_encode[passage[\"source\"]])\n",
    "            row_i = i\n",
    "            for c in context[:context_size]+context[context_size+1:]:\n",
    "                if c != \"\":\n",
    "                    col_i = norm_vocab_encode[c]\n",
    "                    focus_counter[(row_i,col_i)] += 1\n",
    "            i += 1\n",
    "row_indxs = []\n",
    "col_indxs = []\n",
    "dat_vals = []\n",
    "for (row,col),value in focus_counter.items():\n",
    "    row_indxs.append(row)\n",
    "    col_indxs.append(col)\n",
    "    dat_vals.append(value)\n",
    "\n",
    "M_focus = csr_matrix((dat_vals, (row_indxs, col_indxs)),shape=(i,semiotic.vocab.len))\n",
    "M_focus_pmi = slg.util.pmi(M_focus, type_pmi=\"npmi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slg.util.save_file(focus_collocations, semiotic.paths.corpus / \"meno_collocations.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster focus matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cluster = 2\n",
    "kmeans = KMeans(n_clusters=n_cluster, random_state=0, verbose = 0).fit(M_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "kmeans_pmi = slg.util.pmi(kmeans.cluster_centers_, type_pmi=\"npmi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"single\", \"complete\", \"average\", \"weighted\", \"centroid\", \"median\", \"ward\"]\n",
    "method = \"ward\"\n",
    "\n",
    "row_n = 1400\n",
    "col_n = 1400\n",
    "spec_token_len = len(semiotic.vocab.config.special_tokens)\n",
    "\n",
    "matrix = M_pmi[spec_token_len:row_n,spec_token_len:col_n].toarray()\n",
    "elements = list(norm_vocab_encode.keys())[spec_token_len:col_n]\n",
    "\n",
    "meno_vector = M_freq[norm_vocab_encode[\"meno\"]]\n",
    "# elements_col = [list(semiotic.vocab.freq.keys())[i] for i in meno_vector.indices]\n",
    "elements_col = elements\n",
    "elements_row = elements\n",
    "elements_row_dict = {e:i for i,e in enumerate(elements_row)}\n",
    "# matrix = M_pmi[:,meno_vector.indices][spec_token_len:row_n].toarray()\n",
    "\n",
    "\n",
    "row_linkage = hierarchy.linkage(\n",
    "    distance.pdist(matrix), method=method)\n",
    "col_linkage = hierarchy.linkage(\n",
    "    distance.pdist(matrix.T), method=method)\n",
    "\n",
    "linkage  = col_linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae4e7f436ec4925bc14d8ce977060e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='p', max=20, min=1), Dropdown(description='truncate_mode…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "truncate_label = 200\n",
    "def llf(R, id):\n",
    "    n = len(elements)\n",
    "    if id < n:\n",
    "        return elements[id]\n",
    "    else:\n",
    "        label = [id]\n",
    "        while max(label)>=n:\n",
    "            new_label = []\n",
    "            for l in label:\n",
    "                if l<n:\n",
    "                    new_label.append(l)\n",
    "                else:\n",
    "                    new_label.append(int(R[l-n,0]))\n",
    "                    new_label.append(int(R[l-n,1]))\n",
    "            label = new_label\n",
    "\n",
    "        label = \"{ \" + \" \".join([elements[l] if (i+1)%3>0 else \"\\n\"+elements[l] for i,l in enumerate(label[:truncate_label])]) + (\"...\" if len(label)>truncate_label else \"\") + \" }\"\n",
    "        return label\n",
    "\n",
    "fig = plt.figure(figsize=(25, 5))\n",
    "plt.rcParams.update({'axes.facecolor':'white'})\n",
    "# plt.rcdefaults()\n",
    "@interact(p=(1,min(20,len(elements)),1), truncate_mode = [\"lastp\", \"level\"], save = False)\n",
    "def h_clus_cols(p=min(20,len(elements)),truncate_mode = \"lastp\", save = False):\n",
    "    plt.figure(figsize=(25, 5))\n",
    "    hierarchy.dendrogram(\n",
    "        linkage, #col_linkage,\n",
    "        labels=elements,\n",
    "        p = p,\n",
    "        color_threshold = 3,\n",
    "        truncate_mode = truncate_mode, #\"lastp\", #'lastp', None, \"level\"\n",
    "        leaf_label_func=partial(llf, linkage),\n",
    "        leaf_rotation=0,\n",
    "        get_leaves = True\n",
    "        );\n",
    "    plt.grid(b=True, which='major', axis='y',color='lightgrey', linestyle='-', linewidth=.1)\n",
    "    # plt.xlabel('Clusters',fontsize=16)\n",
    "    plt.ylabel(f'distance ({method})',fontsize=16)\n",
    "    plt.xticks(fontfamily=\"monospace\",fontsize=10),\n",
    "    if save:\n",
    "        plt.savefig(path_media + \"dendrogram.pdf\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_labels(labels, truncate=None, width = 3):\n",
    "    if isinstance(truncate,int) and len(labels)>truncate:\n",
    "        labels = (labels[:truncate])+[\"...\"]\n",
    "    labels = \"{\" + \"|\".join([l if (i+1)%width>0 else \"\\n\"+l for i,l in enumerate(labels)]) + \"}\"\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fb0e16d7074efea11bc0ffd28a6a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='row_thres', min=2), IntSlider(value=5, description='col_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def type_matrix(row_thres=5, col_thres=5, concepts=\"meno\",separate = False, focus_cluster = False):\n",
    "\n",
    "    cluster_labels_rows = hierarchy.fcluster(row_linkage, row_thres, criterion='maxclust')\n",
    "    cluster_labels_cols = hierarchy.fcluster(col_linkage, col_thres, criterion='maxclust')\n",
    "\n",
    "    max_cluster_rows = max(cluster_labels_rows)\n",
    "    max_cluster_cols = max(cluster_labels_cols)\n",
    "\n",
    "    # This should be simpler without dict, calling elements of lists\n",
    "    row_dict = {row:np.argwhere(cluster_labels_rows==row) for row in range(1,max_cluster_rows+1)}\n",
    "    col_dict = {col:np.argwhere(cluster_labels_cols==col).flatten() for col in range(1,max_cluster_cols+1)}\n",
    "    \n",
    "    if concepts!=None:\n",
    "        concepts = concepts.split()\n",
    "        if separate:\n",
    "            i = 1\n",
    "            for c in concepts:\n",
    "                if c in elements_row:\n",
    "                    row_dict[row_thres+i]= np.array([[elements_row_dict.get(c)]]).T\n",
    "                    i += 1\n",
    "        \n",
    "        else:\n",
    "            row_dict[row_thres+1] = np.array([[elements_row_dict.get(c) for c in concepts if c in elements_row]]).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    label_row = [np.array(elements_row)[row_dict[row].flatten()].tolist() for row in range(1,len(row_dict)+1)]\n",
    "    label_row = [format_labels(row,10,4) for row in label_row]\n",
    "\n",
    "    label_col = [np.array(elements_col)[col_dict[col]].tolist() for col in range(1,max_cluster_cols+1)]\n",
    "    label_col = [format_labels(row,10,1) for row in label_col]\n",
    "\n",
    "\n",
    "    type_m = np.zeros(shape=(len(row_dict), col_thres))\n",
    "    for r in range(len(row_dict)):\n",
    "        for c in range(col_thres):\n",
    "            type_m[r,c] = M_pmi[row_dict[r+1],col_dict[c+1]].mean()\n",
    "\n",
    "    if focus_cluster:\n",
    "        type_m = np.vstack((type_m,kmeans_pmi[:,:type_m.shape[1]].toarray()))\n",
    "        label_row = label_row + [f\"{focus_term}_{i+1}\" for i in range(kmeans_pmi.shape[0])]\n",
    "\n",
    "            \n",
    "    sns.set(\n",
    "        rc = {'figure.figsize':(40,20)},\n",
    "        font=\"Courier\"\n",
    "        )\n",
    "    matrix_plot = plot(type_m,label_col,label_row)\n",
    "    return matrix_plot\n",
    "\n",
    "interact(type_matrix, row_thres=(2, 100, 1), col_thres=(2,100,1), concepts=widgets.Text(\"meno\",description=\"Extra Cluster\", continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quando li cubi, li censi, le cose sono equali al numero, se vole partire le cose per li censi e quello che ne vene recare a ra-dici cuba c ponare sopra il numero; et radici cuba di quello meno il partimento che venne de le cose nelli censi, vale la cosa.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_corpus[100][\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'famme quista ragione uno me dèie dare d em 5 pagamenta divisate luno da laltro sì cho si chontiene qui da pièie la quale ragione vuole redure a uno termene o a doie termene o a doie pagamenta sì che sen stan luno laltro a dericto e a ponto devemo recevere da uno dì 9 esfiento octovre lib 1317 anche devemo recevere a dì 9 esfiente octovre lib 628 anche devemo avere a mego novenbre lib 293 anche devemo entrante decenbre lib 979 anche devemo avere em kl de genaio lib 2594 somma lib 5811 prima dèie avere quillo che remane en eli mego 1 d 217463 quista è la deritta regola chomo se dèie fare quista ragione e tutte le semeglante ragione che se podessero dire de più e de meno che noie devemo fare la somma de tutto el chapetale de la quale deveraie essere partedore la quale somma se dèie escrivere sì chomo sta qui de sopra e da che noie avemo chusì fatto sì devemo multiplichare tutte le lib de la prima ragione chom gle dì del primo termene em fine a laltro termene che sonno mese 7 e dì 1 più gioè dì 111 contra lib 1317 e quilla multiplichatione sì devemo noie escriverla da una parte e quando àie facto luna sì devemo fare laltra e da giognere tutte le multiplichatione em n una somma e tutta quilla somma sì devemo partire em tutto lo chapetale e'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer(focus_corpus[1000][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
