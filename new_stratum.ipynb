{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration corpus-a27362c15ef31df6\n",
      "Reusing dataset text (/Users/Gianni/.cache/huggingface/datasets/text/corpus-a27362c15ef31df6/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "100%|██████████| 3/3 [00:00<00:00, 393.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import semiolog as slg\n",
    "\n",
    "semiotic = slg.Cenematic(\"en_bnc_old_sgs_new_stratum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semiolog.syntagmatic.tokenizer import Tokenizer\n",
    "semiotic.config.syntagmatic.processor = \"SequenceSLG\"\n",
    "tokenizer = Tokenizer(semiotic.config.syntagmatic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"hello world\"\n",
    "sent_seq = semiotic(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Functive(hello world,(0, 11))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_seq.chain.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'semiotic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-591330a46fec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/semiolog/semiolog/syntagmatic/tokenizer/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, chain)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemiotic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_pretokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_pretokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/semiolog/semiolog/syntagmatic/tokenizer/processors.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, sequence, semiotic, is_pretokenized)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mspans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mFunctive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msemiotic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/semiolog/semiolog/syntagmatic/tokenizer/processors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mspans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mFunctive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msemiotic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'semiotic'"
     ]
    }
   ],
   "source": [
    "tokenizer(sent_seq.chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing in parallel\n",
      "Normalize and jobs data...\n",
      "... computed in 0.045984745025634766 secs.\n",
      "\n",
      "Build alphabet...\n",
      "... computed in 0.0002429485321044922 secs.\n",
      "\n",
      "Alphabet Size: 40\n",
      "Special Tokens Size: 5\n",
      "Terms to compute: 155\n",
      "\n",
      "Enter loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pair: ('x', '‘'), 119: 100%|██████████| 155/155 [00:00<00:00, 287.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute freq...\n",
      "... computed in 0.015986919403076172 secs.\n",
      "\n",
      "Vocabulary built\n",
      "Vocabulary saved to models/en_bnc_old_sgs_new_stratum/vocabulary\n"
     ]
    }
   ],
   "source": [
    "semiotic.vocab.build(\n",
    "    save = True,\n",
    "    parallel = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'x', '£', '8', '5', 'f', '6', 'i', '′', '‘', 'q', 'h', 'j', '4d', 'y', 'c', 'n', 'k', '1', 'o', 'r', 'b', 's', 'p', 'e', 'm', 'd', '0a', '3', '7', '9', 'u', 'v', '4', 'pm', '0', 'xg', 'ev', 'é', 'l', 'cv', 't', 'cg', 'w', 'g', 'w£', 'z', '2', 'a', 'wm', 'cl', 't7', 'n£', 'or', 'px', 'ur', 'p5', '9‘', 'w5', 'pr', '3m', 'xx', 'nm', 'o5', 'té', 'b1', '0g', 'om', 'ox', 'ss', 'evl', 's‘', 'p3', '£′', 'h1', 'xq', 'yf', 'zz', 'x£', 'vl', '4df', 'qr', 'ou', '′x', '4éd', '1é', 'tk', 'cvl', 'p3m', 't0a', 'zzzz', 'ww', 'yx', '22', 'pu', 'cvg', '66', 'nr', 'kf', '3£', 'cc', 'm′', 'zzz', 'fj', 'evg', '2£', 'é‘', '90a', '38', '2z', '′w£', 'wx', 'iu', '4d6', '88', '‘6', 'o3', 'ww£', 'qm', 'hf', 'eva', 'pm′', '80a', 'rq', 'nx', '0q', 'vv', 'k6', 'our', 'll', 'ix', 'cll', 'yp', 'ccg', 'é3', 'bb', '′£', '′f', 'evv', 'im', '8k', 'n5', 'nw£', 'x5', '17', 'é7', 'uu', 'q5', '‘f', 'of', 'n3', '11', 'q£', 't7′', 'p£', 'eg', 'cx', 'nu', 'el', 'ir', 'sl', 'qx', 'zx', 'pw5', 'é0a', '8f', 'df', 'ex', 'ow5', 'qu', 'éd', '55', 'pw', 'h4d', 'yp5', 'vg', 'hs', '′w', 'ow', '1k', 'nw', 'bf', '4‘', '6y', 'pwm', 'i£', 'if', '′m', 'jk', 'cq', '9‘f', 'scl', 'evq', 'xd', 'xgm', 'x‘'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semiotic.vocab.encode.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4 d', 'e v', '0 a', 'z z', 'c v', 'w £', 'p m', 'x g', 'c g', 'c l', 't 7', 'w 5', 'w m', '3 m', 'u r', 'p 5', '9 ‘', 'n £', 'o r', 'p x', 'x x', 'p r', 's s', 'n m', 'o 5', 't é', 'b 1', '0 g', 'o m', 'o x', 'ev l', 's ‘', 'p 3', '£ ′', 'é d', 'h 1', 'zz zz', 'x q', 'y f', 'x £', 'v l', '4d f', 'q r', '2 2', 'w w', 'o u', '′ x', '4 éd', '1 é', 't k', 'cv l', 'p 3m', 't 0a', '6 6', 'y x', 'c c', 'p u', 'cv g', 'n r', 'k f', '3 £', 'm ′', 'zz z', 'f j', 'ev g', '2 £', 'v v', 'é ‘', '9 0a', '3 8', '2 z', '8 8', '′ w£', 'w x', 'i u', '4d 6', '‘ 6', 'o 3', 'q m', 'w w£', 'l l', 'h f', 'ev a', 'pm ′', '8 0a', 'b b', 'r q', 'n x', '0 q', 'k 6', 'o ur', 'i x', 'u u', 'cl l', 'y p', '1 1', 'c cg', 'é 3', '′ £', '′ f', 'ev v', 'i m', '8 k', 'n 5', 'n w£', 'x 5', '1 7', 'é 7', 'q 5', '‘ f', 'o f', 'n 3', 'q £', 't7 ′', 'p £', 'e g', 'c x', 'n u', 'e l', 'i r', 's l', 'q x', '5 5', 'z x', 'p w5', '8 f', 'é 0a', 'd f', 'e x', 'o w5', 'q u', 'p w', 'h 4d', 'y p5', 'v g', 'h s', '′ w', '1 k', 'o w', 'b f', 'n w', '4 ‘', '6 y', 'i £', 'p wm', 'i f', '′ m', 'j k', 'c q', 's cl', '9‘ f', 'x d', 'ev q', 'xg m', 'x ‘', 'o £']\n"
     ]
    }
   ],
   "source": [
    "print(semiotic.vocab.merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
