{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semiolog as slg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.69it/s]\n"
     ]
    }
   ],
   "source": [
    "semiotic = slg.Cenematic(\"en_bnc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "tokenized_datasets = datasets.load_from_disk(semiotic.paths.paradigms / \"tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'special_tokens_mask', 'token_type_ids'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'special_tokens_mask', 'token_type_ids'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'special_tokens_mask', 'token_type_ids'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_datasets = datasets.DatasetDict({\n",
    "    \"train\":tokenized_datasets[\"train\"].select(range(20)),\n",
    "    \"dev\": tokenized_datasets[\"dev\"].select(range(20))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'special_tokens_mask', 'token_type_ids'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'special_tokens_mask', 'token_type_ids'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets_trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as keys in the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG: Compiling model\n",
      "SLG: Tokenized dataset loaded from disk\n",
      "SLG: Building train set\n",
      "SLG: Building validation set\n",
      "SLG: Starting training...\n",
      "\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 38s 9s/step - loss: 10.2087 - val_loss: 10.3046\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 24s 9s/step - loss: 10.0774 - val_loss: 10.2523\n",
      "SLG: Training finished\n",
      "\n",
      "SLG: Model saved.\n",
      "SLG: Training history saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SLG: Model built!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semiotic.paradigmatic.build(dataset=tokenized_datasets,load_tokenized=True, save_tokenized=True,save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
