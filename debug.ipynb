{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semiolog as slg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG: Checking config correctness...\n",
      "SLG: Config correct!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 130.83it/s]\n",
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at models/en_bnc_test/paradigms/tf_model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "semiotic = slg.Cenematic(\"en_bnc_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as keys in the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG: Compiling model\n",
      "SLG: Tokenized dataset loaded from disk\n",
      "SLG: Filtering rows of length <2\n",
      "SLG: Building train set\n",
      "SLG: Building validation set\n",
      "SLG: Starting training...\n",
      "\n",
      "Epoch 1/2\n",
      "12/12 [==============================] - ETA: 0s - loss: 9.9110\n",
      "SLG: Checkpoint saved as: 0-No.h5\n",
      "12/12 [==============================] - 75s 5s/step - loss: 9.9110\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - ETA: 0s - loss: 9.7931\n",
      "SLG: Checkpoint saved as: 1-No.h5\n",
      "12/12 [==============================] - 64s 5s/step - loss: 9.7931\n",
      "SLG: Training finished\n",
      "\n",
      "SLG: Model saved.\n",
      "SLG: Training history saved.\n",
      "SLG: Model built!\n"
     ]
    }
   ],
   "source": [
    "semiotic.paradigmatic.build(\n",
    "    n_sents=400,\n",
    "    checkpoints=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
