{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import os\n",
                "os.chdir(\"../\")\n",
                "\n",
                "import semiolog as slg\n",
                "\n",
                "semiotic = slg.Cenematic(\"fr_wiki\")\n",
                "\n",
                "import regex as re\n",
                "import numpy as np\n",
                "from collections import Counter\n",
                "from scipy.sparse import csr_matrix, lil_matrix, coo_matrix\n",
                "from tqdm.notebook import tqdm, trange\n",
                "from functools import partial\n",
                "from functools import reduce\n",
                "import operator\n",
                "\n",
                "from pyinstrument import Profiler\n",
                "import sys\n",
                "\n",
                "import time\n",
                "from pyinstrument import Profiler\n",
                "import sys\n",
                "\n",
                "from temp import findall_contexts, findall_contexts_list, find_best_pair, agglutinate_list\n",
                "\n",
                "def build_nb(\n",
                "    corpus = None,\n",
                "    voc_final_length = -30,\n",
                "    # save = False,\n",
                "    # save_step = None,\n",
                "    # progress_bar = True,\n",
                "    # resume_merges = False,\n",
                "    parallel = False,\n",
                "    sparse = True,\n",
                "    sparse_mode = \"csr\",\n",
                "    cpu_count = 4,\n",
                "    corpus_length = None,\n",
                "    normalizer = None,\n",
                "):\n",
                "    def agglutinate_chain(pair, cl_chain):\n",
                "        bigram = re.escape(\" \".join(pair))\n",
                "        p = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
                "        cl_chain = p.sub(\"\".join(pair), cl_chain)\n",
                "        return cl_chain\n",
                "\n",
                "    def extract_drc(pairs, encoder: dict):\n",
                "        data = []\n",
                "        rows = []\n",
                "        columns = []\n",
                "        for (r,c),d in pairs:\n",
                "            data.append(d)\n",
                "            rows.append(encoder[r])\n",
                "            columns.append(encoder[c])\n",
                "        return data, rows, columns\n",
                "\n",
                "    def parallel_chain(chain, n_of_parts, overlap = 0):\n",
                "        \"\"\"\n",
                "        Breaks the chain in n chunks to compute best pair of terms. Chunks are overlapping by one term, so as no pair of terms is lost due to the break.\n",
                "        \"\"\"\n",
                "        if not isinstance(chain,list):\n",
                "            chain = list(chain)\n",
                "        chunk_size = int(len(chain) / n_of_parts)+1\n",
                "        for i in range(0, len(chain), chunk_size):\n",
                "            yield chain[i : i + chunk_size + overlap]\n",
                "\n",
                "    def separate_chain(chain, n_of_parts, best_pair: list):\n",
                "        \"\"\"\n",
                "        Separate a chain (in list form) for parallel processing of regex findall of pair, taking care that the cuts of the chunks don't fall in the neiborhood of the pair, affecting the final counts\n",
                "        \"\"\"\n",
                "        chunk_size = int(len(chain) / n_of_parts)+1\n",
                "        b = 0\n",
                "        n = chunk_size\n",
                "        chain_len = len(chain)\n",
                "        for i in range(n_of_parts):\n",
                "            n = (i+1)*chunk_size\n",
                "            if chain_len > n:\n",
                "                while chain[n-2:n] == best_pair or chain[n-1:n+1] == best_pair:\n",
                "                    n = n+1\n",
                "            yield (\"[SEP_i] \" if i!=0 else \"\") + \" \".join(chain[b:n]) + (\" [SEP_i]\" if i!=n_of_parts-1 else \"\")\n",
                "            b = n-1\n",
                "        \n",
                "        \n",
                "    # normalizer = eval(f\"slg.syntagmatic.tokenizer.normalizers.Sequence({semiotic.config.vocabulary.normalizer})\")\n",
                "    \n",
                "    if parallel:\n",
                "        \n",
                "        par_corpus = parallel_chain(corpus[:corpus_length], cpu_count)\n",
                "\n",
                "        result = slg.util.multiprocessing_tqdm(partial(semiotic.vocab.chain_list_alpha, normalizer), par_corpus, cores=cpu_count, desc=\"Normalize & Alphabet\")\n",
                "        \n",
                "        chain_list = []\n",
                "        alphabet = Counter()\n",
                "        for chain_l, alpha in result:\n",
                "            chain_list += chain_l\n",
                "            alphabet += alpha\n",
                "            \n",
                "    else:\n",
                "        chain_list, alphabet = semiotic.vocab.chain_list_alpha(normalizer, semiotic.corpus.train[:corpus_length], progress_bar=True)\n",
                "\n",
                "    # cl, alphabet = semiotic.vocab.chain_list_alpha(normalizer, semiotic.corpus.train, progress_bar=True)\n",
                "    cl_chain = \"[SEP] \"+\" \".join(chain_list)+\" [SEP]\"\n",
                "    encode = {k:i for i,(k,v) in enumerate(alphabet.most_common())}\n",
                "    decode = {i:k for k,i in encode.items()}\n",
                "    new_i = len(encode)\n",
                "    if parallel:\n",
                "        \n",
                "        par_chain = parallel_chain(chain_list, cpu_count, overlap=1)\n",
                "        \n",
                "        result = slg.util.multiprocessing(find_best_pair, par_chain, cores=cpu_count) \n",
                "                            \n",
                "        pairs = reduce(operator.add, result)\n",
                "        pairs = pairs.most_common()\n",
                "        \n",
                "    else:\n",
                "        pairs = find_best_pair(chain_list).most_common()\n",
                "    if voc_final_length<0:\n",
                "        voc_final_length = new_i + abs(voc_final_length)\n",
                "        \n",
                "    if sparse:\n",
                "        data, rows, columns = extract_drc(pairs,encode)\n",
                "        voc_matrix = coo_matrix((np.array(data), (np.array(rows),np.array(columns))), shape=(voc_final_length, voc_final_length), dtype=int)\n",
                "\n",
                "    else:\n",
                "        voc_matrix = np.zeros((voc_final_length, voc_final_length), dtype=int)\n",
                "        for (row,column),value in pairs:\n",
                "            voc_matrix[encode[row], encode[column]] = value\n",
                "    merges = []\n",
                "    delta_voc = voc_final_length - new_i\n",
                "    best_pair = \"init\"\n",
                "    pair_count = \"---\"\n",
                "\n",
                "    t = trange(delta_voc) #, disable = not progress_bar)\n",
                "\n",
                "    for _ in t:\n",
                "        t.set_description(f\"Pair: {best_pair}, {pair_count}\")\n",
                "        t.refresh()\n",
                "\n",
                "        if sparse:\n",
                "            max_i = voc_matrix.data.argmax()\n",
                "            pair_row = voc_matrix.row[max_i]\n",
                "            pair_col = voc_matrix.col[max_i]\n",
                "            pair_count = voc_matrix.data[max_i]\n",
                "        else:\n",
                "            pair_row,pair_col = np.unravel_index(np.argmax(voc_matrix, axis=None), voc_matrix.shape)\n",
                "            pair_count = voc_matrix[pair_row,pair_col]\n",
                "        \n",
                "        if pair_count == 0:\n",
                "            break\n",
                "\n",
                "        best_pair = (decode[pair_row], decode[pair_col])\n",
                "        best_pair_string = \" \".join(best_pair)\n",
                "        merges.append(best_pair_string)\n",
                "        best_pair_string_voc = \"\".join(best_pair)\n",
                "        re_voc_l = \"(\"+\"|\".join([\" \"+k+\" \" for k in encode.keys()]+[\"\\[SEP\\] \",\"\\[SEP_i\\] \"])+\")\"\n",
                "        re_voc_r = \"(\"+\"|\".join([\" \"+k+\" \" for k in encode.keys()]+[\" \\[SEP\\]\",\" \\[SEP_i\\]\"])+\")\"\n",
                "        if parallel:\n",
                "            result = slg.util.multiprocessing(\n",
                "                partial(findall_contexts,best_pair_string=best_pair_string,re_voc_l=re_voc_l,re_voc_r=re_voc_r),\n",
                "                separate_chain(cl_chain.split(), cpu_count, list(best_pair)),\n",
                "                cores = cpu_count\n",
                "                )\n",
                "            merge_context = reduce(operator.add, result)\n",
                "        else:\n",
                "            merge_context = re.findall(re_voc_l+best_pair_string+re_voc_r, cl_chain, overlapped=True)\n",
                "        merge_context_count_l = Counter()\n",
                "        merge_context_count_r = Counter()\n",
                "        for l,r in merge_context:\n",
                "            if \"[SEP]\" not in l:\n",
                "                merge_context_count_l[encode[l.strip()]] += 1\n",
                "            if \"[SEP]\" not in r:\n",
                "                merge_context_count_r[encode[r.strip()]] += 1\n",
                "        \n",
                "        if sparse:\n",
                "            # Convert matrix to CSR or LIL, for item attribution and arithmetic \n",
                "            if sparse_mode == \"csr\":\n",
                "                voc_matrix = voc_matrix.tocsr()\n",
                "            else:\n",
                "                voc_matrix = voc_matrix.tolil()\n",
                "        \n",
                "        for row,key in merge_context_count_l.items():\n",
                "            voc_matrix[row,new_i] = key\n",
                "            \n",
                "        for column,key in merge_context_count_r.items():\n",
                "            voc_matrix[new_i,column] = key\n",
                "\n",
                "        # Correct previous counts\n",
                "        \n",
                "        # compute #(l,r)-(l,r)\n",
                "        pair_pair_count = len(re.findall(\" \"+best_pair_string+\" \"+best_pair_string+\" \", cl_chain, overlapped=False))\n",
                "        # remove #(l,r)-(l,r) from (l,r)-l\n",
                "        voc_matrix[new_i,pair_row] -= pair_pair_count\n",
                "        # remove #(l,r)-(l,r) from r-(l,r)\n",
                "        voc_matrix[pair_col,new_i] -= pair_pair_count\n",
                "        # remove #(l,r)-(l,r) from r-l\n",
                "        voc_matrix[pair_col,pair_row] -= pair_pair_count\n",
                "        # substract (l,r)- from r-\n",
                "        voc_matrix[pair_col,:new_i] -= voc_matrix[new_i,:new_i]\n",
                "        # substract -(l,r)- from -l\n",
                "        voc_matrix[:new_i,pair_row] -= voc_matrix[:new_i,new_i]\n",
                "        \n",
                "        # set l-r to 0\n",
                "        voc_matrix[pair_row,pair_col] = 0\n",
                "        # register #(l,r)-(l,r)\n",
                "        voc_matrix[new_i,new_i] = pair_pair_count\n",
                "        \n",
                "        if sparse:\n",
                "            # Convert matrix back to COO, to restart the loop\n",
                "            voc_matrix = voc_matrix.tocoo()\n",
                "        \n",
                "        best_pair_string_voc = \"\".join(best_pair)\n",
                "        encode[best_pair_string_voc] = new_i\n",
                "        decode[new_i] = best_pair_string_voc\n",
                "        new_i += 1\n",
                "        cl_chain = agglutinate_chain(best_pair_string.split(),cl_chain)\n",
                "\n",
                "\n",
                "    if sparse:\n",
                "        freq_values = voc_matrix.sum(axis=1).T.tolist()[0]\n",
                "    else:\n",
                "        freq_values = voc_matrix.sum(axis=1).T.tolist()\n",
                "    vocabulary = {decode[i]:v for i,v in enumerate(freq_values) if v>0} # Make sure dimension of matrix and size of voc coincide\n",
                "    vocabulary = sorted(vocabulary.items(), key=lambda x: x[1], reverse=True)\n",
                "    \n",
                "    return merges, vocabulary\n",
                "\n",
                "normalizer = slg.syntagmatic.tokenizer.normalizers.Sequence(semiotic.config.vocabulary.normalizer)\n",
                "    \n",
                "profile = Profiler()\n",
                "profile.start()\n",
                "\n",
                "merges, vocabulary = build_nb(\n",
                "    semiotic.corpus.train,\n",
                "    normalizer=normalizer,\n",
                "    voc_final_length = -30,\n",
                "    parallel=True)\n",
                "\n",
                "profile.stop()\n",
                "print(profile.output_text(unicode=True, color=True))\n",
                "\n",
                "print(merges)\n",
                "\n",
                "print(vocabulary[:100])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Last build vocabulary before first used version (0.1)\n",
                "\n",
                "\n",
                "    def build_old(\n",
                "        self,\n",
                "        corpus = None,\n",
                "        vocab_size = None,\n",
                "        special_tokens = None,\n",
                "        save = False,\n",
                "        save_step = None,\n",
                "        truncate_best_size = None,\n",
                "        progress_bar = True,\n",
                "        resume_merges = False,\n",
                "        parallel = False,\n",
                "        corpus_length = None\n",
                "        ):\n",
                "\n",
                "        if corpus == None:\n",
                "            corpus = self.name\n",
                "        \n",
                "        if vocab_size == None:\n",
                "            vocab_size = self.config.size\n",
                "\n",
                "        if truncate_best_size == None:\n",
                "            truncate_best_size = self.config.truncate_best_size\n",
                "\n",
                "        if special_tokens == None:\n",
                "            special_tokens = self.config.special_tokens\n",
                "        \n",
                "        if corpus_length == None:\n",
                "            corpus_length = self.corpus.train_len\n",
                "        \n",
                "        if save == True and save_step != None:\n",
                "            saveQ = True\n",
                "            \n",
                "            if not isdir(self.path):\n",
                "                makedirs(self.path)\n",
                "                \n",
                "        else:\n",
                "            saveQ = False\n",
                "\n",
                "        def pre_process(corpus_chunk, normalizer):\n",
                "            # Normalize\n",
                "            chain_zip = normalizer(corpus_chunk)\n",
                "            # Build list of pairs\n",
                "            chain_zip = list(zip(chain_zip,chain_zip[1:]))\n",
                "            # Create a lookup table of all the positions where a pair appears in a corpus\n",
                "            pair_pos = defaultdict(set)\n",
                "            for i,k in list(enumerate(chain_zip)):\n",
                "                pair_pos[k].add(i)\n",
                "            # From the previous lookup table, create another lookup table of the frequency of each pair (given by the size of the set of its positions)\n",
                "            pair_len = Counter()\n",
                "            for k,pos in pair_pos.items():\n",
                "                pair_len[k] = len(pos)\n",
                "            \n",
                "            return (chain_zip, pair_pos, pair_len)\n",
                "\n",
                "        def process_best_pair(job_data, best_pair):\n",
                "            chain_zip, pair_pos, pair_len = job_data\n",
                "            chain_zip_len = len(chain_zip)\n",
                "\n",
                "            for i in pair_pos[best_pair]:\n",
                "                # Skip iteration if position corresponds to a modified set of positions during the iteration. This can happen if there is overlap of pairs, such as \"000\", where (\"0\",\"0\") has itself as right pair. Note that, due to unordered implementation of sets, this entails a lack of systematicity in overlapping cases: \"000\" can be counted randomly as (\"00\",\"0\") or (\"0\",\"00\").\n",
                "                # TODO: Investigate the cost of ordering sets. In which case, the following \"if\" condition might only be needed for right pairs.\n",
                "                if chain_zip[i]!=best_pair:\n",
                "                    continue\n",
                "                ## merge best pair with left unit\n",
                "                left_pair_i = i-1\n",
                "                while left_pair_i>=0 and chain_zip[left_pair_i] == None: # if left pair is within chain limits but empty (= None) because already merged previously, shift to the left\n",
                "                    left_pair_i -= 1\n",
                "                if left_pair_i>-1: # proceed only if a left pair was found on the left\n",
                "                    # Remove from left pair positions, the current position (of the pair to be merged)\n",
                "                    left_pair = chain_zip[left_pair_i]\n",
                "                    # Skip update of left_pair position set if left_pair = best_pair, to avoid modification of iterating set. This can happen if there is overlap of pairs. No consequences on final result (right?) since right after the loop, the key corresponding to the best pair is deleted, and chain_zip is indeed updated so the problematic cases can be captured at the beginning of the loop.\n",
                "                    if left_pair != best_pair:\n",
                "                        left_pair_pos = pair_pos[left_pair]\n",
                "                        left_pair_pos.discard(left_pair_i)\n",
                "                    new_pair = (left_pair[0],\"\".join(best_pair)) # construct new left pair\n",
                "                    pair_pos[new_pair].add(left_pair_i) # add new pair (if non existing) and its position to the pair_pos lookup table\n",
                "                    # update the counts in the pair_len lookuptable\n",
                "                    pair_len[left_pair] -= 1\n",
                "                    pair_len[new_pair] += 1\n",
                "                    # update the list of pairs\n",
                "                    chain_zip[left_pair_i] = new_pair\n",
                "\n",
                "                ## merge best pair with right unit.\n",
                "                # Code is symmetric to left_pair but on the right. Comments are omitted\n",
                "                right_pair_i = i+1\n",
                "                while right_pair_i<chain_zip_len and chain_zip[right_pair_i] == None:\n",
                "                    right_pair_i += 1\n",
                "                if right_pair_i<chain_zip_len:\n",
                "                    right_pair = chain_zip[right_pair_i]\n",
                "                    if right_pair != best_pair:\n",
                "                        right_pair_pos = pair_pos[right_pair]\n",
                "                        right_pair_pos.discard(right_pair_i)\n",
                "                    new_pair = (\"\".join(best_pair), right_pair[1])\n",
                "                    pair_pos[new_pair].add(right_pair_i)\n",
                "                    pair_len[right_pair] -= 1\n",
                "                    pair_len[new_pair] += 1\n",
                "                    chain_zip[right_pair_i] = new_pair\n",
                "\n",
                "                # Empty best pair position in list of pairs\n",
                "                chain_zip[i] = None\n",
                "\n",
                "            # Remove best pair from lookuptables\n",
                "            del pair_pos[best_pair]\n",
                "            del pair_len[best_pair]\n",
                "\n",
                "            return (chain_zip, pair_pos, pair_len)\n",
                "\n",
                "        def compute_freq(chain_zip):\n",
                "            # TODO: add the last unit to the decoupling\n",
                "            freq = [pair[0] for pair in chain_zip if pair != None]\n",
                "            if chain_zip[-1]!=None: \n",
                "                freq.append(chain_zip[-1][-1])\n",
                "            freq = Counter(freq)\n",
                "            return freq\n",
                "        \n",
                "\n",
                "        if parallel:\n",
                "            chunksize = int(corpus_length/self.cpu_count)\n",
                "\n",
                "            corpus_chunks = [\"\".join(self.corpus.train[i*chunksize:i*chunksize+chunksize]) for i in range(0,self.cpu_count)]\n",
                "\n",
                "            with Parallel(n_jobs=self.cpu_count, require='sharedmem') as parallel_pool:\n",
                "                print(\"Computing in parallel\")\n",
                "                print(\"Normalize and jobs data...\")\n",
                "                start = time.time()\n",
                "                jobs_data = parallel_pool(delayed(pre_process)(chunk,self.normalizer.normalize) for chunk in corpus_chunks)\n",
                "\n",
                "                pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                "\n",
                "                # When pair_len_global has more than 1 max, the first encountered is chosen, introducing possible discrepancies between implementations (because each choice modifies global statistics). However, multiple max is less likely to appear in big corpora and relatively small vocabularies, and mostly at the tail of vocabularies (ie. low frequencies), so the impact of this divergence is expected to be marginal.\n",
                "                best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                "\n",
                "                merges = [\" \".join(best_pair)]\n",
                "                print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "                print(\"Build alphabet...\")\n",
                "                start = time.time()\n",
                "                alphabet = Counter()\n",
                "                for (l,r),v in pair_len_global.items():\n",
                "                    alphabet[l] += v\n",
                "                # In extreme cases, right characters of pairs might not be left characters. If there are such chars, they're added with freq 1\n",
                "                left_out_chars = {r for l,r in pair_len_global.keys()}-alphabet.keys()\n",
                "                if len(left_out_chars)>0:\n",
                "                    print(f\"Adding characters: {left_out_chars}\")\n",
                "                    for char in left_out_chars:\n",
                "                        alphabet[char] += 1\n",
                "                print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "                alpha_len = len(alphabet)\n",
                "                special_tokens_len = 0 if special_tokens == None else len(special_tokens)\n",
                "                \n",
                "                print(f\"Alphabet Size: {alpha_len}\")\n",
                "                print(f\"Special Tokens Size: {special_tokens_len}\")\n",
                "\n",
                "                \n",
                "                if vocab_size<0:\n",
                "                    voc_final_length = alpha_len + abs(vocab_size) + special_tokens_len\n",
                "                else:\n",
                "                    voc_final_length = vocab_size\n",
                "\n",
                "                delta_voc = voc_final_length - alpha_len - special_tokens_len\n",
                "\n",
                "                print(f\"Terms to compute: {delta_voc}\\n\")\n",
                "\n",
                "                print(\"Enter loop\")\n",
                "\n",
                "                # for _ in trange(delta_voc):\n",
                "                t = trange(delta_voc, disable = not progress_bar)\n",
                "                for _ in t:\n",
                "                    t.set_description(f\"Pair: {best_pair}, {best_pair_len}\")\n",
                "                    t.refresh()\n",
                "\n",
                "                    jobs_data = parallel_pool(delayed(process_best_pair)(job_data, best_pair) for job_data in jobs_data)\n",
                "\n",
                "                    if truncate_best_size==None:\n",
                "                        pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                "                    else:\n",
                "                        pair_len_global = reduce(operator.add,[Counter(dict(i[-1].most_common(truncate_best_size))) for i in jobs_data])\n",
                "\n",
                "                    # When pair_len_global has more than 1 max, the first encountered is chosen, introducing possible discrepancies between implementations (because each choice modifies global statistics). However, multiple max is less likely to appear in big corpora and relatively small vocabularies, and mostly at the tail of vocabularies (ie. low frequencies), so the impact of this divergence is expected to be marginal.\n",
                "                    best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                "\n",
                "                    merges.append(\" \".join(best_pair))\n",
                "                \n",
                "                    if saveQ == True:\n",
                "                        voc_partial_len = alpha_len + special_tokens_len + _ + 1\n",
                "                        if voc_partial_len % save_step == 0 and voc_partial_len != voc_final_length:\n",
                "\n",
                "                            print(\"Saving intermediate results...\")\n",
                "                            start = time.time()\n",
                "                            freqs = parallel_pool(delayed(compute_freq)(job_data[0]) for job_data in jobs_data)\n",
                "                            freq = reduce(operator.add, freqs)\n",
                "\n",
                "                            vocabulary = freq.most_common()\n",
                "                            \n",
                "                            if special_tokens != None:\n",
                "                                vocabulary = vocabulary + [(token,0) for token in special_tokens]\n",
                "                            \n",
                "                            self.merges = merges\n",
                "                            self.encode = {k:i for i,(k,v) in enumerate(vocabulary)}\n",
                "                            self.freq = dict(vocabulary)\n",
                "                            self.alpha = dict(alphabet.most_common())\n",
                "                            step_path = self.path / str(voc_partial_len)\n",
                "                            self.save(step_path)\n",
                "                            print(f\"... computed in {time.time()-start} secs.\")\n",
                "                            print(f\"Intermediate vocabulary saved to {step_path}\\n\")\n",
                "\n",
                "                print(\"Compute freq...\")\n",
                "                start = time.time()\n",
                "                freqs = parallel_pool(delayed(compute_freq)(job_data[0]) for job_data in jobs_data)\n",
                "                freq = reduce(operator.add, freqs)\n",
                "                print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "        \n",
                "        else:\n",
                "            print(\"Computing sequentially\")\n",
                "            print(\"Normalize and jobs data...\")\n",
                "            start = time.time()\n",
                "            corpus_chain = \"\".join(self.corpus.train[:corpus_length])\n",
                "            job_data = pre_process(corpus_chain,self.normalizer.normalize)\n",
                "\n",
                "            pair_len_global = job_data[-1]\n",
                "\n",
                "            # When pair_len_global has more than 1 max, the first encountered is chosen, introducing possible discrepancies between implementations (because each choice modifies global statistics). However, multiple max is less likely to appear in big corpora and relatively small vocabularies, and mostly at the tail of vocabularies (ie. low frequencies), so the impact of this divergence is expected to be marginal.\n",
                "            best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                "\n",
                "            merges = [\" \".join(best_pair)]\n",
                "            print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "            print(\"Build alphabet...\")\n",
                "            start = time.time()\n",
                "            alphabet = Counter()\n",
                "            for (l,r),v in pair_len_global.items():\n",
                "                alphabet[l] =+ v\n",
                "            # In extreme cases, right characters of pairs might not be left characters. If there are such chars, they're added with freq 1\n",
                "            left_out_chars = {r for l,r in pair_len_global.keys()}-alphabet.keys()\n",
                "            if len(left_out_chars)>0:\n",
                "                print(f\"Adding characters: {left_out_chars}\")\n",
                "                for char in left_out_chars:\n",
                "                    alphabet[char] =+ 1\n",
                "            print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "            alpha_len = len(alphabet)\n",
                "            special_tokens_len = 0 if special_tokens == None else len(special_tokens)\n",
                "            \n",
                "            print(f\"Alphabet Size: {alpha_len}\")\n",
                "            print(f\"Special Tokens Size: {special_tokens_len}\")\n",
                "            \n",
                "            if vocab_size<0:\n",
                "                voc_final_length = alpha_len + abs(vocab_size) + special_tokens_len\n",
                "            else:\n",
                "                voc_final_length = vocab_size\n",
                "\n",
                "            delta_voc = voc_final_length - alpha_len - special_tokens_len\n",
                "            \n",
                "            print(f\"Terms to compute: {delta_voc}\\n\")\n",
                "\n",
                "            print(\"Enter loop\")\n",
                "\n",
                "            # for _ in trange(delta_voc):\n",
                "            t = trange(delta_voc, disable = not progress_bar)\n",
                "            for _ in t:\n",
                "                t.set_description(f\"Pair: {best_pair}, {best_pair_len}\")\n",
                "                t.refresh()\n",
                "\n",
                "                # print(f\"{_+1+alpha_len+special_tokens_len}/{voc_final_length}: {best_pair}...\")\n",
                "                # start = time.time()\n",
                "                job_data = process_best_pair(job_data, best_pair)\n",
                "\n",
                "                pair_len_global = job_data[-1]\n",
                "\n",
                "                # When pair_len_global has more than 1 max, the first encountered is chosen, introducing possible discrepancies between implementations (because each choice modifies global statistics). However, multiple max is less likely to appear in big corpora and relatively small vocabularies, and mostly at the tail of vocabularies (ie. low frequencies), so the impact of this divergence is expected to be marginal.\n",
                "                best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                "\n",
                "                merges.append(\" \".join(best_pair))\n",
                "                # print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "                if saveQ == True:\n",
                "                    voc_partial_len = alpha_len + special_tokens_len + _ + 1\n",
                "                    if voc_partial_len % save_step == 0 and voc_partial_len != voc_final_length:\n",
                "\n",
                "                        print(\"Saving intermediate results...\")\n",
                "                        start = time.time()\n",
                "                        freq = compute_freq(job_data[0])\n",
                "\n",
                "                        vocabulary = freq.most_common()\n",
                "                        \n",
                "                        if special_tokens != None:\n",
                "                            vocabulary = vocabulary + [(token,0) for token in special_tokens]\n",
                "                        \n",
                "                        self.merges = merges\n",
                "                        self.encode = {k:i for i,(k,v) in enumerate(vocabulary)}\n",
                "                        self.freq = dict(vocabulary)\n",
                "                        self.alpha = dict(alphabet.most_common())\n",
                "                        step_path = self.path / str(voc_partial_len)\n",
                "                        self.save(step_path)\n",
                "                        print(f\"... computed in {time.time()-start} secs.\")\n",
                "                        print(f\"Intermediate vocabulary saved to {step_path}\\n\")\n",
                "            \n",
                "            print(\"Compute freq...\")\n",
                "            start = time.time()\n",
                "            freq = compute_freq(job_data[0])\n",
                "            print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "        vocabulary = freq.most_common()\n",
                "        \n",
                "        if special_tokens != None:\n",
                "            vocabulary = vocabulary + [(token,0) for token in special_tokens]\n",
                "        \n",
                "        self.merges = merges\n",
                "        self.encode = {k:i for i,(k,v) in enumerate(vocabulary)}\n",
                "        self.freq = dict(vocabulary)\n",
                "        self.alpha = dict(alphabet.most_common())\n",
                "\n",
                "        self.decode = {i:k for k,i in self.encode.items()}\n",
                "        \n",
                "        self.len = len(vocabulary)     \n",
                "        self.freq_mass = sum(self.freq.values())\n",
                "        self.prob = {k:v/self.freq_mass for k,v in self.freq.items()}\n",
                "\n",
                "        print(\"Vocabulary built\")\n",
                "        \n",
                "        if save == True:\n",
                "            self.save()\n",
                "            print(f\"Vocabulary saved to {self.path}\")"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit"
        },
        "interpreter": {
            "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}