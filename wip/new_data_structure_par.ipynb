{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import os\n",
                "os.chdir(\"../\")\n",
                "\n",
                "from tqdm.notebook import tqdm, trange\n",
                "from pyinstrument import Profiler\n",
                "from joblib import Parallel, delayed, parallel_backend\n",
                "\n",
                "import semiolog as slg\n",
                "\n",
                "semiotic = slg.Cenematic(\"en_bnc\",requested_cpu=4)\n",
                "\n",
                "from collections import Counter, defaultdict\n",
                "from functools import reduce\n",
                "import operator\n",
                "import time"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Warning: models/en_bnc/vocabulary/merges.txt does not exist.\n",
                        "Vocabulary will not be loaded from file.\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "%load_ext line_profiler"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "semiotic.vocab.build_new(vocab_size = 400, parallel = True, corpus_length = 20000)\n",
                "new_freq = semiotic.vocab.freq\n",
                "new_merges = semiotic.vocab.merges\n",
                "semiotic.vocab.build(vocab_size = 400, parallel = True, corpus_length = 20000)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 1.1566030979156494 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.0004169940948486328 secs.\n",
                        "\n",
                        "Alphabet Size: 78\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 317\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, max=317.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "07fb8993d269457d8d8193e3baa7e852"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "Compute freq...\n",
                        "... computed in 0.3080778121948242 secs.\n",
                        "\n",
                        "Vocabulary built\n",
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 0.9098470211029053 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.0004911422729492188 secs.\n",
                        "\n",
                        "Alphabet Size: 78\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 317\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, max=317.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "0ce7738ce8d941c09e96dfe61d958caa"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "Compute freq...\n",
                        "... computed in 0.29079699516296387 secs.\n",
                        "\n",
                        "Vocabulary built\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "for n,o in zip(new_merges,semiotic.vocab.merges):\n",
                "    if n!=o:\n",
                "        print(n,\"-\",o)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "c t 1432 - m ent 1432\n",
                        "m ent 1432 - c t 1432\n",
                        "p e 892 - n o 892\n",
                        "n o 892 - p e 892\n",
                        "u re 854 - oun d 854\n",
                        "oun d 854 - u re 854\n",
                        "wh en 674 - com p 674\n",
                        "com p 674 - wh en 674\n",
                        "the s 642 - wor k 642\n",
                        "wor k 642 - the s 642\n",
                        "i p 634 - as s 634\n",
                        "as s 634 - i p 634\n",
                        "e p 629 - im e 629\n",
                        "im e 624 - e p 624\n",
                        "ou s 621 - it ion 621\n",
                        "it ion 621 - ou s 621\n",
                        "i re 619 - a ve 619\n",
                        "a ve 619 - i re 619\n",
                        "v i 611 - w ay 611\n",
                        "w ay 611 - v i 611\n",
                        "p res 600 - p res 601\n",
                        "p re 568 - p re 569\n",
                        "c oun 567 - k now 567\n",
                        "k now 567 - c oun 567\n",
                        "d u 525 - be t 525\n",
                        "be t 525 - d u 525\n",
                        "it is 519 - am e 519\n",
                        "am e 519 - it is 519\n",
                        "an c 504 - ct ion 504\n",
                        "ct ion 504 - an c 504\n",
                        "d ay 501 - you r 501\n",
                        "you r 501 - d ay 501\n",
                        "in k 490 - and the 490\n",
                        "and the 490 - in k 490\n",
                        "it t 463 - h ow 463\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "for key in new_freq.keys():\n",
                "    if new_freq.get(key,\"-\")!=semiotic.vocab.freq.get(key,\"-\"):\n",
                "        print(key,new_freq.get(key,\"-\"),semiotic.vocab.freq.get(key,\"-\"))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "s 7 5\n",
                        "e 6 4\n",
                        "ti 5 3\n",
                        "o 5 4\n",
                        "n 5 7\n",
                        "f 4 3\n",
                        "p 4 3\n",
                        "ers 4 3\n",
                        "c 3 2\n",
                        "r 3 5\n",
                        "d 3 5\n",
                        "i 2 6\n",
                        "and 2 -\n",
                        "ri 2 -\n",
                        "thersname 2 1\n",
                        "ni 1 -\n",
                        "j 1 -\n",
                        "iv 1 -\n",
                        "in 1 -\n",
                        "is 1 -\n",
                        "at 1 -\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "# clean\n",
                "%lprun -f semiotic.vocab.build_new semiotic.vocab.build_new(vocab_size = 200, parallel = True, corpus_length = 2000)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 0.4826979637145996 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.014801025390625 secs.\n",
                        "\n",
                        "Alphabet Size: 49\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 146\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, max=146.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "62c6832c1e97453aae890513eb55c9e2"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "Compute freq...\n",
                        "... computed in 0.07721495628356934 secs.\n",
                        "\n",
                        "Vocabulary built\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Timer unit: 1e-06 s\n",
                        "\n",
                        "Total time: 1.74738 s\n",
                        "File: /Users/Gianni/semiolog/semiolog/vocabulary.py\n",
                        "Function: build_new at line 433\n",
                        "\n",
                        "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
                        "==============================================================\n",
                        "   433                                               def build_new(\n",
                        "   434                                                   self,\n",
                        "   435                                                   corpus = None,\n",
                        "   436                                                   vocab_size = None,\n",
                        "   437                                                   special_tokens = None,\n",
                        "   438                                                   save = False,\n",
                        "   439                                                   save_step = None,\n",
                        "   440                                                   progress_bar = True,\n",
                        "   441                                                   resume_merges = False,\n",
                        "   442                                                   parallel = False,\n",
                        "   443                                                   corpus_length = None\n",
                        "   444                                                   ):\n",
                        "   445                                           \n",
                        "   446         1        394.0    394.0      0.0          if corpus == None:\n",
                        "   447         1          5.0      5.0      0.0              corpus = self.name\n",
                        "   448                                                   \n",
                        "   449         1          3.0      3.0      0.0          if vocab_size == None:\n",
                        "   450                                                       vocab_size = self.config.size\n",
                        "   451                                           \n",
                        "   452         1          3.0      3.0      0.0          if special_tokens == None:\n",
                        "   453         1          3.0      3.0      0.0              special_tokens = self.config.special_tokens\n",
                        "   454                                                   \n",
                        "   455         1          3.0      3.0      0.0          if corpus_length == None:\n",
                        "   456                                                       corpus_length = self.corpus.train_len\n",
                        "   457                                                   \n",
                        "   458         1          3.0      3.0      0.0          if save == True and save_step != None:\n",
                        "   459                                                       saveQ = True\n",
                        "   460                                                       \n",
                        "   461                                                       if not isdir(self.path):\n",
                        "   462                                                           makedirs(self.path)\n",
                        "   463                                                           \n",
                        "   464                                                   else:\n",
                        "   465         1          3.0      3.0      0.0              saveQ = False\n",
                        "   466                                           \n",
                        "   467         1          4.0      4.0      0.0          def pre_process(corpus_chunk, normalizer):\n",
                        "   468                                                       # Normalize\n",
                        "   469                                                       chain_zip = normalizer(corpus_chunk)\n",
                        "   470                                                       # Build list of pairs\n",
                        "   471                                                       chain_zip = list(zip(chain_zip,chain_zip[1:]))\n",
                        "   472                                                       # Create a lookup table of all the positions where a pair appears in a corpus\n",
                        "   473                                                       pair_pos = defaultdict(set)\n",
                        "   474                                                       for i,k in list(enumerate(chain_zip)):\n",
                        "   475                                                           pair_pos[k].add(i)\n",
                        "   476                                                       # From the previous lookup table, create another lookup table of the frequency of each pair (given by the size of the set of its positions)\n",
                        "   477                                                       pair_len = Counter()\n",
                        "   478                                                       for k,pos in pair_pos.items():\n",
                        "   479                                                           pair_len[k] = len(pos)\n",
                        "   480                                                       \n",
                        "   481                                                       return (chain_zip, pair_pos, pair_len)\n",
                        "   482                                           \n",
                        "   483         1          3.0      3.0      0.0          def process_best_pair(chain_zip, pair_pos, best_pair):\n",
                        "   484                                                       chain_zip_len = len(chain_zip)\n",
                        "   485                                                       pair_len_delta = Counter()\n",
                        "   486                                           \n",
                        "   487                                                       for i in pair_pos[best_pair]:\n",
                        "   488                                                           # Skip iteration if position corresponds to a modified set of positions during the iteration. This can happen if there is overlap of pairs, such as \"000\", where (\"0\",\"0\") has itself as right pair. Note that, due to unordered implementation of sets, this entails a lack of systematicity in overlapping cases: \"000\" can be counted randomly as (\"00\",\"0\") or (\"0\",\"00\").\n",
                        "   489                                                           # TODO: Investigate the cost of ordering sets. In which case, the following \"if\" condition might only be needed for right pairs.\n",
                        "   490                                                           if chain_zip[i]!=best_pair:\n",
                        "   491                                                               continue\n",
                        "   492                                                           ## merge best pair with left unit\n",
                        "   493                                                           left_pair_i = i-1\n",
                        "   494                                                           while left_pair_i>=0 and chain_zip[left_pair_i] == None: # if left pair is within chain limits but empty (= None) because already merged previously, shift to the left\n",
                        "   495                                                               left_pair_i -= 1\n",
                        "   496                                                           if left_pair_i>-1: # proceed only if a left pair was found on the left\n",
                        "   497                                                               # Remove from left pair positions, the current position (of the pair to be merged)\n",
                        "   498                                                               left_pair = chain_zip[left_pair_i]\n",
                        "   499                                                               # Skip update of left_pair position set if left_pair = best_pair, to avoid modification of iterating set. This can happen if there is overlap of pairs. No consequences on final result (right?) since right after the loop, the key corresponding to the best pair is deleted, and chain_zip is indeed updated so the problematic cases can be captured at the beginning of the loop.\n",
                        "   500                                                               if left_pair != best_pair:\n",
                        "   501                                                                   left_pair_pos = pair_pos[left_pair]\n",
                        "   502                                                                   left_pair_pos.discard(left_pair_i)\n",
                        "   503                                                               new_pair = (left_pair[0],\"\".join(best_pair)) # construct new left pair\n",
                        "   504                                                               pair_pos[new_pair].add(left_pair_i) # add new pair (if non existing) and its position to the pair_pos lookup table\n",
                        "   505                                                               # update the counts in the pair_len lookuptable\n",
                        "   506                                                               pair_len_delta[left_pair] -= 1\n",
                        "   507                                                               pair_len_delta[new_pair] += 1\n",
                        "   508                                           \n",
                        "   509                                                               # update the list of pairs\n",
                        "   510                                                               chain_zip[left_pair_i] = new_pair\n",
                        "   511                                           \n",
                        "   512                                                           ## merge best pair with right unit.\n",
                        "   513                                                           # Code is symmetric to left_pair but on the right. Comments are omitted\n",
                        "   514                                                           right_pair_i = i+1\n",
                        "   515                                                           while right_pair_i<chain_zip_len and chain_zip[right_pair_i] == None:\n",
                        "   516                                                               right_pair_i += 1\n",
                        "   517                                                           if right_pair_i<chain_zip_len:\n",
                        "   518                                                               right_pair = chain_zip[right_pair_i]\n",
                        "   519                                                               if right_pair != best_pair:\n",
                        "   520                                                                   right_pair_pos = pair_pos[right_pair]\n",
                        "   521                                                                   right_pair_pos.discard(right_pair_i)\n",
                        "   522                                                               new_pair = (\"\".join(best_pair), right_pair[1])\n",
                        "   523                                                               pair_pos[new_pair].add(right_pair_i)\n",
                        "   524                                                               pair_len_delta[right_pair] -= 1\n",
                        "   525                                                               pair_len_delta[new_pair] += 1\n",
                        "   526                                           \n",
                        "   527                                                               chain_zip[right_pair_i] = new_pair\n",
                        "   528                                           \n",
                        "   529                                                           # Empty best pair position in list of pairs\n",
                        "   530                                                           chain_zip[i] = None\n",
                        "   531                                           \n",
                        "   532                                                       # Remove best pair from lookuptables\n",
                        "   533                                                       del pair_pos[best_pair]\n",
                        "   534                                           \n",
                        "   535                                                       return (chain_zip, pair_pos, pair_len_delta)\n",
                        "   536                                           \n",
                        "   537         1          2.0      2.0      0.0          def compute_freq(chain_zip):\n",
                        "   538                                                       # TODO: add the last unit to the decoupling\n",
                        "   539                                                       freq = [pair[0] for pair in chain_zip if pair != None]\n",
                        "   540                                                       if chain_zip[-1]!=None: \n",
                        "   541                                                           freq.append(chain_zip[-1][-1])\n",
                        "   542                                                       freq = Counter(freq)\n",
                        "   543                                                       return freq\n",
                        "   544                                                   \n",
                        "   545                                           \n",
                        "   546         1          2.0      2.0      0.0          if parallel:\n",
                        "   547         1          6.0      6.0      0.0              chunksize = int(corpus_length/self.cpu_count)\n",
                        "   548                                           \n",
                        "   549         1        397.0    397.0      0.0              corpus_chunks = [\"\".join(self.corpus.train[i*chunksize:i*chunksize+chunksize]) for i in range(0,self.cpu_count)]\n",
                        "   550                                           \n",
                        "   551         1        331.0    331.0      0.0              with Parallel(n_jobs=self.cpu_count, require='sharedmem') as parallel_pool:\n",
                        "   552         1        432.0    432.0      0.0                  print(\"Computing in parallel\")\n",
                        "   553         1         38.0     38.0      0.0                  print(\"Normalize and jobs data...\")\n",
                        "   554         1          5.0      5.0      0.0                  start = time.time()\n",
                        "   555         1     478981.0 478981.0     27.4                  jobs_data = parallel_pool(delayed(pre_process)(chunk,self.normalizer.normalize) for chunk in corpus_chunks)\n",
                        "   556                                           \n",
                        "   557         1       3631.0   3631.0      0.2                  pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                        "   558                                           \n",
                        "   559         1         52.0     52.0      0.0                  best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                        "   560                                           \n",
                        "   561         1          4.0      4.0      0.0                  merges = [\" \".join(best_pair)]\n",
                        "   562         1        248.0    248.0      0.0                  print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   563                                           \n",
                        "   564         1         37.0     37.0      0.0                  print(\"Build alphabet...\")\n",
                        "   565         1          3.0      3.0      0.0                  start = time.time()\n",
                        "   566         1          8.0      8.0      0.0                  alphabet = Counter()\n",
                        "   567      1061       2982.0      2.8      0.2                  for (l,r),v in pair_len_global.items():\n",
                        "   568      1060       3754.0      3.5      0.2                      alphabet[l] += v\n",
                        "   569                                                           # In extreme cases, right characters of pairs might not be left characters. If there are such chars, they're added with freq 1\n",
                        "   570         1        221.0    221.0      0.0                  left_out_chars = {r for l,r in pair_len_global.keys()}-alphabet.keys()\n",
                        "   571         1          5.0      5.0      0.0                  if len(left_out_chars)>0:\n",
                        "   572                                                               print(f\"Adding characters: {left_out_chars}\")\n",
                        "   573                                                               for char in left_out_chars:\n",
                        "   574                                                                   alphabet[char] += 1\n",
                        "   575         1        243.0    243.0      0.0                  print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   576                                           \n",
                        "   577         1          4.0      4.0      0.0                  alpha_len = len(alphabet)\n",
                        "   578         1          3.0      3.0      0.0                  special_tokens_len = 0 if special_tokens == None else len(special_tokens)\n",
                        "   579                                                           \n",
                        "   580         1         27.0     27.0      0.0                  print(f\"Alphabet Size: {alpha_len}\")\n",
                        "   581         1         26.0     26.0      0.0                  print(f\"Special Tokens Size: {special_tokens_len}\")\n",
                        "   582                                           \n",
                        "   583                                                           \n",
                        "   584         1          3.0      3.0      0.0                  if vocab_size<0:\n",
                        "   585                                                               voc_final_length = alpha_len + abs(vocab_size) + special_tokens_len\n",
                        "   586                                                           else:\n",
                        "   587         1          3.0      3.0      0.0                      voc_final_length = vocab_size\n",
                        "   588                                           \n",
                        "   589         1          3.0      3.0      0.0                  delta_voc = voc_final_length - alpha_len - special_tokens_len\n",
                        "   590                                           \n",
                        "   591         1         23.0     23.0      0.0                  print(f\"Terms to compute: {delta_voc}\\n\")\n",
                        "   592                                           \n",
                        "   593         1        174.0    174.0      0.0                  print(\"Enter loop\")\n",
                        "   594                                           \n",
                        "   595         1      32549.0  32549.0      1.9                  t = trange(delta_voc, disable = not progress_bar)\n",
                        "   596       147      46296.0    314.9      2.6                  for _ in t:\n",
                        "   597       146     206327.0   1413.2     11.8                      t.set_description(f\"Pair: {best_pair}, {best_pair_len}\")\n",
                        "   598       146      30454.0    208.6      1.7                      t.refresh()\n",
                        "   599                                           \n",
                        "   600       146     647368.0   4434.0     37.0                      jobs_data = parallel_pool(delayed(process_best_pair)(chain_zip, pair_pos, best_pair) for chain_zip, pair_pos, pair_len_delta in jobs_data)\n",
                        "   601                                           \n",
                        "   602       730       4919.0      6.7      0.3                      for chain_zip, pair_pos, pair_len_delta in jobs_data:\n",
                        "   603       584      67823.0    116.1      3.9                          pair_len_global.update(pair_len_delta)\n",
                        "   604                                           \n",
                        "   605                                                               # Remove best_pair from pair_len\n",
                        "   606       146       1299.0      8.9      0.1                      del pair_len_global[best_pair]\n",
                        "   607                                           \n",
                        "   608       146     137616.0    942.6      7.9                      best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                        "   609                                           \n",
                        "   610       146       1069.0      7.3      0.1                      merges.append(\" \".join(best_pair))\n",
                        "   611                                                               # print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   612                                           \n",
                        "   613         1         33.0     33.0      0.0                  print(\"Compute freq...\")\n",
                        "   614         1          4.0      4.0      0.0                  start = time.time()\n",
                        "   615         1      73583.0  73583.0      4.2                  freqs = parallel_pool(delayed(compute_freq)(job_data[0]) for job_data in jobs_data)\n",
                        "   616         1       3606.0   3606.0      0.2                  freq = reduce(operator.add, freqs)\n",
                        "   617         1       1934.0   1934.0      0.1                  print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   618                                                   \n",
                        "   619                                                   else:\n",
                        "   620                                                       #TODO: Sequential computing not tested\n",
                        "   621                                                       print(\"Computing sequentially\")\n",
                        "   622                                                       print(\"Normalize and jobs data...\")\n",
                        "   623                                                       start = time.time()\n",
                        "   624                                                       corpus_chain = \"\".join(self.corpus.train[:corpus_length])\n",
                        "   625                                                       chain_zip, pair_pos, pair_len_global = pre_process(corpus_chain,self.normalizer.normalize)\n",
                        "   626                                           \n",
                        "   627                                                       best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                        "   628                                                       \n",
                        "   629                                                       merges = [\" \".join(best_pair)]\n",
                        "   630                                                       print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   631                                           \n",
                        "   632                                                       print(\"Build alphabet...\")\n",
                        "   633                                                       start = time.time()\n",
                        "   634                                                       alphabet = Counter()\n",
                        "   635                                                       for (l,r),v in pair_len_global.items():\n",
                        "   636                                                           alphabet[l] =+ v\n",
                        "   637                                                       # In extreme cases, right characters of pairs might not be left characters. If there are such chars, they're added with freq 1\n",
                        "   638                                                       left_out_chars = {r for l,r in pair_len_global.keys()}-alphabet.keys()\n",
                        "   639                                                       if len(left_out_chars)>0:\n",
                        "   640                                                           print(f\"Adding characters: {left_out_chars}\")\n",
                        "   641                                                           for char in left_out_chars:\n",
                        "   642                                                               alphabet[char] =+ 1\n",
                        "   643                                                       print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   644                                           \n",
                        "   645                                                       alpha_len = len(alphabet)\n",
                        "   646                                                       special_tokens_len = 0 if special_tokens == None else len(special_tokens)\n",
                        "   647                                                       \n",
                        "   648                                                       print(f\"Alphabet Size: {alpha_len}\")\n",
                        "   649                                                       print(f\"Special Tokens Size: {special_tokens_len}\")\n",
                        "   650                                                       \n",
                        "   651                                                       if vocab_size<0:\n",
                        "   652                                                           voc_final_length = alpha_len + abs(vocab_size) + special_tokens_len\n",
                        "   653                                                       else:\n",
                        "   654                                                           voc_final_length = vocab_size\n",
                        "   655                                           \n",
                        "   656                                                       delta_voc = voc_final_length - alpha_len - special_tokens_len\n",
                        "   657                                                       \n",
                        "   658                                                       print(f\"Terms to compute: {delta_voc}\\n\")\n",
                        "   659                                           \n",
                        "   660                                                       print(\"Enter loop\")\n",
                        "   661                                           \n",
                        "   662                                                       # for _ in trange(delta_voc):\n",
                        "   663                                                       t = trange(delta_voc, disable = not progress_bar)\n",
                        "   664                                                       for _ in t:\n",
                        "   665                                                           t.set_description(f\"Pair: {best_pair}, {best_pair_len}\")\n",
                        "   666                                                           t.refresh()\n",
                        "   667                                           \n",
                        "   668                                                           chain_zip, pair_pos, pair_len_delta = process_best_pair(chain_zip, pair_pos, best_pair)\n",
                        "   669                                           \n",
                        "   670                                                           # Remove best_pair from pair_len\n",
                        "   671                                                           pair_len_global.update(pair_len_delta)\n",
                        "   672                                           \n",
                        "   673                                                           del pair_len_global[best_pair]\n",
                        "   674                                                           best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                        "   675                                           \n",
                        "   676                                                           merges.append(\" \".join(best_pair))\n",
                        "   677                                                           # print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   678                                           \n",
                        "   679                                                           if saveQ == True:\n",
                        "   680                                                               voc_partial_len = alpha_len + special_tokens_len + _ + 1\n",
                        "   681                                                               if voc_partial_len % save_step == 0 and voc_partial_len != voc_final_length:\n",
                        "   682                                           \n",
                        "   683                                                                   print(\"Saving intermediate results...\")\n",
                        "   684                                                                   start = time.time()\n",
                        "   685                                                                   freq = compute_freq(job_data[0])\n",
                        "   686                                           \n",
                        "   687                                                                   vocabulary = freq.most_common()\n",
                        "   688                                                                   \n",
                        "   689                                                                   if special_tokens != None:\n",
                        "   690                                                                       vocabulary = vocabulary + [(token,0) for token in special_tokens]\n",
                        "   691                                                                   \n",
                        "   692                                                                   self.merges = merges\n",
                        "   693                                                                   self.encode = {k:i for i,(k,v) in enumerate(vocabulary)}\n",
                        "   694                                                                   self.freq = dict(vocabulary)\n",
                        "   695                                                                   self.alpha = dict(alphabet.most_common())\n",
                        "   696                                                                   step_path = self.path / str(voc_partial_len)\n",
                        "   697                                                                   self.save(step_path)\n",
                        "   698                                                                   print(f\"... computed in {time.time()-start} secs.\")\n",
                        "   699                                                                   print(f\"Intermediate vocabulary saved to {step_path}\\n\")\n",
                        "   700                                                       \n",
                        "   701                                                       print(\"Compute freq...\")\n",
                        "   702                                                       start = time.time()\n",
                        "   703                                                       freq = compute_freq(chain_zip)\n",
                        "   704                                                       print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   705                                           \n",
                        "   706         1         99.0     99.0      0.0          vocabulary = freq.most_common()\n",
                        "   707                                                   \n",
                        "   708         1          4.0      4.0      0.0          if special_tokens != None:\n",
                        "   709         1          8.0      8.0      0.0              vocabulary = vocabulary + [(token,0) for token in special_tokens]\n",
                        "   710                                                   \n",
                        "   711         1          4.0      4.0      0.0          self.merges = merges\n",
                        "   712         1         65.0     65.0      0.0          self.encode = {k:i for i,(k,v) in enumerate(vocabulary)}\n",
                        "   713         1         16.0     16.0      0.0          self.freq = dict(vocabulary)\n",
                        "   714         1         14.0     14.0      0.0          self.alpha = dict(alphabet.most_common())\n",
                        "   715                                           \n",
                        "   716         1         37.0     37.0      0.0          self.decode = {i:k for k,i in self.encode.items()}\n",
                        "   717                                                   \n",
                        "   718         1          4.0      4.0      0.0          self.len = len(vocabulary)     \n",
                        "   719         1          7.0      7.0      0.0          self.freq_mass = sum(self.freq.values())\n",
                        "   720         1         67.0     67.0      0.0          self.prob = {k:v/self.freq_mass for k,v in self.freq.items()}\n",
                        "   721                                           \n",
                        "   722         1         97.0     97.0      0.0          print(\"Vocabulary built\")\n",
                        "   723                                                   \n",
                        "   724         1          3.0      3.0      0.0          if save == True:\n",
                        "   725                                                       self.save()\n",
                        "   726                                                       print(f\"Vocabulary saved to {self.path}\")"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "def pre_process(corpus_chunk, normalizer):\n",
                "    # Normalize\n",
                "    chain_zip = normalizer(corpus_chunk)\n",
                "    # Build list of pairs\n",
                "    chain_zip = list(zip(chain_zip,chain_zip[1:]))\n",
                "    # Create a lookup table of all the positions where a pair appears in a corpus\n",
                "    pair_pos = defaultdict(set)\n",
                "    for i,k in list(enumerate(chain_zip)):\n",
                "        pair_pos[k].add(i)\n",
                "    # From the previous lookup table, create another lookup table of the frequency of each pair (given by the size of the set of its positions)\n",
                "    pair_len = Counter()\n",
                "    for k,pos in pair_pos.items():\n",
                "        pair_len[k] = len(pos)\n",
                "    \n",
                "    return (chain_zip, pair_pos, pair_len)\n",
                "\n",
                "def process_best_pair(chain_zip, pair_pos, best_pair):\n",
                "    chain_zip_len = len(chain_zip)\n",
                "    pair_len_delta = Counter()\n",
                "\n",
                "    for i in pair_pos[best_pair]:\n",
                "        # Skip iteration if position corresponds to a modified set of positions during the iteration. This can happen if there is overlap of pairs, such as \"000\", where (\"0\",\"0\") has itself as right pair. Note that, due to unordered implementation of sets, this entails a lack of systematicity in overlapping cases: \"000\" can be counted randomly as (\"00\",\"0\") or (\"0\",\"00\").\n",
                "        # TODO: Investigate the cost of ordering sets. In which case, the following \"if\" condition might only be needed for right pairs.\n",
                "        if chain_zip[i]!=best_pair:\n",
                "            continue\n",
                "        ## merge best pair with left unit\n",
                "        left_pair_i = i-1\n",
                "        while left_pair_i>=0 and chain_zip[left_pair_i] == None: # if left pair is within chain limits but empty (= None) because already merged previously, shift to the left\n",
                "            left_pair_i -= 1\n",
                "        if left_pair_i>-1: # proceed only if a left pair was found on the left\n",
                "            # Remove from left pair positions, the current position (of the pair to be merged)\n",
                "            left_pair = chain_zip[left_pair_i]\n",
                "            # Skip update of left_pair position set if left_pair = best_pair, to avoid modification of iterating set. This can happen if there is overlap of pairs. No consequences on final result (right?) since right after the loop, the key corresponding to the best pair is deleted, and chain_zip is indeed updated so the problematic cases can be captured at the beginning of the loop.\n",
                "            if left_pair != best_pair:\n",
                "                left_pair_pos = pair_pos[left_pair]\n",
                "                left_pair_pos.discard(left_pair_i)\n",
                "            new_pair = (left_pair[0],\"\".join(best_pair)) # construct new left pair\n",
                "            pair_pos[new_pair].add(left_pair_i) # add new pair (if non existing) and its position to the pair_pos lookup table\n",
                "            # update the counts in the pair_len lookuptable\n",
                "            pair_len_delta[left_pair] -= 1\n",
                "            pair_len_delta[new_pair] += 1\n",
                "\n",
                "            # update the list of pairs\n",
                "            chain_zip[left_pair_i] = new_pair\n",
                "\n",
                "        ## merge best pair with right unit.\n",
                "        # Code is symmetric to left_pair but on the right. Comments are omitted\n",
                "        right_pair_i = i+1\n",
                "        while right_pair_i<chain_zip_len and chain_zip[right_pair_i] == None:\n",
                "            right_pair_i += 1\n",
                "        if right_pair_i<chain_zip_len:\n",
                "            right_pair = chain_zip[right_pair_i]\n",
                "            if right_pair != best_pair:\n",
                "                right_pair_pos = pair_pos[right_pair]\n",
                "                right_pair_pos.discard(right_pair_i)\n",
                "            new_pair = (\"\".join(best_pair), right_pair[1])\n",
                "            pair_pos[new_pair].add(right_pair_i)\n",
                "            pair_len_delta[right_pair] -= 1\n",
                "            pair_len_delta[new_pair] += 1\n",
                "\n",
                "            chain_zip[right_pair_i] = new_pair\n",
                "\n",
                "        # Empty best pair position in list of pairs\n",
                "        chain_zip[i] = None\n",
                "\n",
                "    # Remove best pair from lookuptables\n",
                "    del pair_pos[best_pair]\n",
                "\n",
                "    return (chain_zip, pair_pos, pair_len_delta)\n",
                "\n",
                "def compute_freq(chain_zip):\n",
                "    # TODO: add the last unit to the decoupling\n",
                "    freq = [pair[0] for pair in chain_zip if pair != None]\n",
                "    if chain_zip[-1]!=None: \n",
                "        freq.append(chain_zip[-1][-1])\n",
                "    freq = Counter(freq)\n",
                "    return freq"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "vocab_size = 200\n",
                "corpus_length = 2000\n",
                "k_equal_top = 50\n",
                "\n",
                "self = semiotic.vocab\n",
                "corpus = None\n",
                "\n",
                "special_tokens = None\n",
                "save = False\n",
                "save_step = None\n",
                "truncate_best_size = None\n",
                "progress_bar = True\n",
                "resume_merges = False\n",
                "parallel = False\n",
                "\n",
                "if corpus == None:\n",
                "    corpus = self.name\n",
                "\n",
                "if vocab_size == None:\n",
                "    vocab_size = self.config.size\n",
                "\n",
                "if special_tokens == None:\n",
                "    special_tokens = self.config.special_tokens\n",
                "\n",
                "\n",
                "if corpus_length == None:\n",
                "    corpus_length = self.corpus.train_len\n",
                "\n",
                "if k_equal_top == None:\n",
                "    k_equal_top = self.config.k_equal_top\n",
                "    \n",
                "saveQ = False"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "def test(\n",
                "    vocab_size = vocab_size,\n",
                "    corpus_length = corpus_length,\n",
                "\n",
                "    self = self,\n",
                "    corpus = corpus,\n",
                "\n",
                "    special_tokens = special_tokens,\n",
                "    save = save,\n",
                "    save_step = save_step,\n",
                "    progress_bar = progress_bar,\n",
                "    resume_merges = resume_merges,\n",
                "    parallel = parallel,\n",
                "\n",
                "    k_equal_top=k_equal_top,\n",
                "):\n",
                "\n",
                "\n",
                "    chunksize = int(corpus_length/self.cpu_count)\n",
                "\n",
                "    corpus_chunks = [\"\".join(self.corpus.train[i*chunksize:i*chunksize+chunksize]) for i in range(0,self.cpu_count)]\n",
                "\n",
                "    with Parallel(n_jobs=self.cpu_count, require='sharedmem') as parallel_pool:\n",
                "        print(\"Computing in parallel\")\n",
                "        print(\"Normalize and jobs data...\")\n",
                "        start = time.time()\n",
                "        jobs_data = parallel_pool(delayed(pre_process)(chunk,self.normalizer.normalize) for chunk in corpus_chunks)\n",
                "\n",
                "        pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                "        pair_len_global = Counter(dict(sorted(pair_len_global.items())))\n",
                "        \n",
                "        best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                "\n",
                "        merges = [\" \".join(best_pair)]\n",
                "        print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "        print(\"Build alphabet...\")\n",
                "        start = time.time()\n",
                "        alphabet = Counter()\n",
                "        for (l,r),v in pair_len_global.items():\n",
                "            alphabet[l] += v\n",
                "        # In extreme cases, right characters of pairs might not be left characters. If there are such chars, they're added with freq 1\n",
                "        left_out_chars = {r for l,r in pair_len_global.keys()}-alphabet.keys()\n",
                "        if len(left_out_chars)>0:\n",
                "            print(f\"Adding characters: {left_out_chars}\")\n",
                "            for char in left_out_chars:\n",
                "                alphabet[char] += 1\n",
                "        print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "        alpha_len = len(alphabet)\n",
                "        special_tokens_len = 0 if special_tokens == None else len(special_tokens)\n",
                "        \n",
                "        print(f\"Alphabet Size: {alpha_len}\")\n",
                "        print(f\"Special Tokens Size: {special_tokens_len}\")\n",
                "\n",
                "        \n",
                "        if vocab_size<0:\n",
                "            voc_final_length = alpha_len + abs(vocab_size) + special_tokens_len\n",
                "        else:\n",
                "            voc_final_length = vocab_size\n",
                "\n",
                "        delta_voc = voc_final_length - alpha_len - special_tokens_len\n",
                "\n",
                "        print(f\"Terms to compute: {delta_voc}\\n\")\n",
                "\n",
                "        print(\"Enter loop\")\n",
                "\n",
                "        t = trange(delta_voc, disable = not progress_bar)\n",
                "        for _ in t:\n",
                "            t.set_description(f\"Pair: {best_pair}, {best_pair_len}\")\n",
                "            t.refresh()\n",
                "\n",
                "            jobs_data = parallel_pool(delayed(process_best_pair)(chain_zip, pair_pos, best_pair) for chain_zip, pair_pos, pair_len_delta in jobs_data)\n",
                "\n",
                "            for chain_zip, pair_pos, pair_len_delta in jobs_data:\n",
                "                pair_len_global.update(pair_len_delta)\n",
                "\n",
                "            # Remove best_pair from pair_len\n",
                "            del pair_len_global[best_pair]\n",
                "\n",
                "            best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                "\n",
                "            merges.append(\" \".join(best_pair))\n",
                "            # print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "        print(\"Compute freq...\")\n",
                "        start = time.time()\n",
                "        freqs = parallel_pool(delayed(compute_freq)(job_data[0]) for job_data in jobs_data)\n",
                "        freq = reduce(operator.add, freqs)\n",
                "        print(f\"... computed in {time.time()-start} secs.\\n\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "%lprun -f test test(vocab_size = 750, parallel = True, corpus_length = 2000,k_equal_top=10)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 0.11403703689575195 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.0065419673919677734 secs.\n",
                        "\n",
                        "Alphabet Size: 49\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 696\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, max=696.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "d2457069ab1d4b429569d25e1fe56b1b"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "output_type": "error",
                    "ename": "IndexError",
                    "evalue": "list index out of range",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-23-73ff3a4d6211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lprun'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-f test test(vocab_size = 750, parallel = True, corpus_length = 2000,k_equal_top=10)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2346\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2348\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m<decorator-gen-893>\u001b[0m in \u001b[0;36mlprun\u001b[0;34m(self, parameter_s)\u001b[0m\n",
                        "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.9/site-packages/line_profiler/line_profiler.py\u001b[0m in \u001b[0;36mlprun\u001b[0;34m(self, parameter_s)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0mprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m                 \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.9/site-packages/line_profiler/line_profiler.py\u001b[0m in \u001b[0;36mrunctx\u001b[0;34m(self, cmd, globals, locals)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_by_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mexec_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_by_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;32m<ipython-input-22-399640e03930>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(vocab_size, corpus_length, self, corpus, special_tokens, save, save_step, progress_bar, resume_merges, parallel, k_equal_top)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mbest_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mpair_len_top\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mbest_pair_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mbest_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_len_top\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mIndexError\u001b[0m: list index out of range"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                " 1524064.0  10438.8     55.0 \n",
                "\n",
                " \n",
                "21779758.0  31292.8     87.7\n",
                "1970580.0   2831.3     39.4  "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "bla = Counter(\"blksdq\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "bla.most_common(50)[0]\n",
                "bla.most_common(50)[0][1]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "1"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 16
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit"
        },
        "interpreter": {
            "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}