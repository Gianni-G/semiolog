{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import os\n",
                "os.chdir(\"../\")\n",
                "\n",
                "from tqdm.notebook import tqdm, trange\n",
                "from pyinstrument import Profiler\n",
                "from joblib import Parallel, delayed, parallel_backend\n",
                "\n",
                "import semiolog as slg\n",
                "\n",
                "semiotic = slg.Cenematic(\"en_bnc\",requested_cpu=4)\n",
                "\n",
                "from collections import Counter, defaultdict\n",
                "from functools import reduce\n",
                "import operator\n",
                "import time"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Warning: models/en_bnc/vocabulary/merges.txt does not exist.\n",
                        "Vocabulary will not be loaded from file.\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "%load_ext line_profiler"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "semiotic.vocab.build_new(vocab_size = 400, parallel = True, corpus_length = 20000)\n",
                "new_freq = semiotic.vocab.freq\n",
                "new_merges = semiotic.vocab.merges\n",
                "semiotic.vocab.build(vocab_size = 400, parallel = True, corpus_length = 20000)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 0.8924679756164551 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.00046372413635253906 secs.\n",
                        "\n",
                        "Alphabet Size: 78\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 317\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, max=317.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "fc116b1555ad4f19ada5ffc97f1ea0d1"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "Compute freq...\n",
                        "... computed in 0.29940295219421387 secs.\n",
                        "\n",
                        "Vocabulary built\n",
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 1.3269822597503662 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.0007753372192382812 secs.\n",
                        "\n",
                        "Alphabet Size: 78\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 317\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, max=317.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "330e04a58e344cbcb37917d660f22179"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "Compute freq...\n",
                        "... computed in 0.30179309844970703 secs.\n",
                        "\n",
                        "Vocabulary built\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "for n,o in zip(new_merges,semiotic.vocab.merges):\n",
                "    if n!=o:\n",
                "        print(n,\"-\",o)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "c t 1432 - m ent 1432\n",
                        "m ent 1432 - c t 1432\n",
                        "p e 892 - n o 892\n",
                        "n o 892 - p e 892\n",
                        "u re 854 - oun d 854\n",
                        "oun d 854 - u re 854\n",
                        "wh en 674 - com p 674\n",
                        "com p 674 - wh en 674\n",
                        "the s 642 - wor k 642\n",
                        "wor k 642 - the s 642\n",
                        "i p 634 - as s 634\n",
                        "as s 634 - i p 634\n",
                        "e p 629 - im e 629\n",
                        "im e 624 - e p 624\n",
                        "ou s 621 - it ion 621\n",
                        "it ion 621 - ou s 621\n",
                        "i re 619 - a ve 619\n",
                        "a ve 619 - i re 619\n",
                        "v i 611 - w ay 611\n",
                        "w ay 611 - v i 611\n",
                        "p res 600 - p res 601\n",
                        "p re 568 - p re 569\n",
                        "c oun 567 - k now 567\n",
                        "k now 567 - c oun 567\n",
                        "d u 525 - be t 525\n",
                        "be t 525 - d u 525\n",
                        "it is 519 - am e 519\n",
                        "am e 519 - it is 519\n",
                        "an c 504 - ct ion 504\n",
                        "ct ion 504 - an c 504\n",
                        "d ay 501 - you r 501\n",
                        "you r 501 - d ay 501\n",
                        "in k 490 - and the 490\n",
                        "and the 490 - in k 490\n",
                        "it t 463 - h ow 463\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "for key in new_freq.keys():\n",
                "    if new_freq.get(key,\"-\")!=semiotic.vocab.freq.get(key,\"-\"):\n",
                "        print(key,new_freq.get(key,\"-\"),semiotic.vocab.freq.get(key,\"-\"))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "s 7 5\n",
                        "e 6 4\n",
                        "ti 5 3\n",
                        "o 5 4\n",
                        "n 5 7\n",
                        "f 4 3\n",
                        "p 4 3\n",
                        "ers 4 3\n",
                        "c 3 2\n",
                        "r 3 5\n",
                        "d 3 5\n",
                        "i 2 6\n",
                        "and 2 -\n",
                        "ri 2 -\n",
                        "thersname 2 1\n",
                        "ni 1 -\n",
                        "j 1 -\n",
                        "iv 1 -\n",
                        "in 1 -\n",
                        "is 1 -\n",
                        "at 1 -\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "# clean\n",
                "%lprun -f semiotic.vocab.build_new semiotic.vocab.build_new(vocab_size = 200, parallel = True, corpus_length = 2000)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 0.4826979637145996 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.014801025390625 secs.\n",
                        "\n",
                        "Alphabet Size: 49\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 146\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, max=146.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "62c6832c1e97453aae890513eb55c9e2"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "Compute freq...\n",
                        "... computed in 0.07721495628356934 secs.\n",
                        "\n",
                        "Vocabulary built\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Timer unit: 1e-06 s\n",
                        "\n",
                        "Total time: 1.74738 s\n",
                        "File: /Users/Gianni/semiolog/semiolog/vocabulary.py\n",
                        "Function: build_new at line 433\n",
                        "\n",
                        "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
                        "==============================================================\n",
                        "   433                                               def build_new(\n",
                        "   434                                                   self,\n",
                        "   435                                                   corpus = None,\n",
                        "   436                                                   vocab_size = None,\n",
                        "   437                                                   special_tokens = None,\n",
                        "   438                                                   save = False,\n",
                        "   439                                                   save_step = None,\n",
                        "   440                                                   progress_bar = True,\n",
                        "   441                                                   resume_merges = False,\n",
                        "   442                                                   parallel = False,\n",
                        "   443                                                   corpus_length = None\n",
                        "   444                                                   ):\n",
                        "   445                                           \n",
                        "   446         1        394.0    394.0      0.0          if corpus == None:\n",
                        "   447         1          5.0      5.0      0.0              corpus = self.name\n",
                        "   448                                                   \n",
                        "   449         1          3.0      3.0      0.0          if vocab_size == None:\n",
                        "   450                                                       vocab_size = self.config.size\n",
                        "   451                                           \n",
                        "   452         1          3.0      3.0      0.0          if special_tokens == None:\n",
                        "   453         1          3.0      3.0      0.0              special_tokens = self.config.special_tokens\n",
                        "   454                                                   \n",
                        "   455         1          3.0      3.0      0.0          if corpus_length == None:\n",
                        "   456                                                       corpus_length = self.corpus.train_len\n",
                        "   457                                                   \n",
                        "   458         1          3.0      3.0      0.0          if save == True and save_step != None:\n",
                        "   459                                                       saveQ = True\n",
                        "   460                                                       \n",
                        "   461                                                       if not isdir(self.path):\n",
                        "   462                                                           makedirs(self.path)\n",
                        "   463                                                           \n",
                        "   464                                                   else:\n",
                        "   465         1          3.0      3.0      0.0              saveQ = False\n",
                        "   466                                           \n",
                        "   467         1          4.0      4.0      0.0          def pre_process(corpus_chunk, normalizer):\n",
                        "   468                                                       # Normalize\n",
                        "   469                                                       chain_zip = normalizer(corpus_chunk)\n",
                        "   470                                                       # Build list of pairs\n",
                        "   471                                                       chain_zip = list(zip(chain_zip,chain_zip[1:]))\n",
                        "   472                                                       # Create a lookup table of all the positions where a pair appears in a corpus\n",
                        "   473                                                       pair_pos = defaultdict(set)\n",
                        "   474                                                       for i,k in list(enumerate(chain_zip)):\n",
                        "   475                                                           pair_pos[k].add(i)\n",
                        "   476                                                       # From the previous lookup table, create another lookup table of the frequency of each pair (given by the size of the set of its positions)\n",
                        "   477                                                       pair_len = Counter()\n",
                        "   478                                                       for k,pos in pair_pos.items():\n",
                        "   479                                                           pair_len[k] = len(pos)\n",
                        "   480                                                       \n",
                        "   481                                                       return (chain_zip, pair_pos, pair_len)\n",
                        "   482                                           \n",
                        "   483         1          3.0      3.0      0.0          def process_best_pair(chain_zip, pair_pos, best_pair):\n",
                        "   484                                                       chain_zip_len = len(chain_zip)\n",
                        "   485                                                       pair_len_delta = Counter()\n",
                        "   486                                           \n",
                        "   487                                                       for i in pair_pos[best_pair]:\n",
                        "   488                                                           # Skip iteration if position corresponds to a modified set of positions during the iteration. This can happen if there is overlap of pairs, such as \"000\", where (\"0\",\"0\") has itself as right pair. Note that, due to unordered implementation of sets, this entails a lack of systematicity in overlapping cases: \"000\" can be counted randomly as (\"00\",\"0\") or (\"0\",\"00\").\n",
                        "   489                                                           # TODO: Investigate the cost of ordering sets. In which case, the following \"if\" condition might only be needed for right pairs.\n",
                        "   490                                                           if chain_zip[i]!=best_pair:\n",
                        "   491                                                               continue\n",
                        "   492                                                           ## merge best pair with left unit\n",
                        "   493                                                           left_pair_i = i-1\n",
                        "   494                                                           while left_pair_i>=0 and chain_zip[left_pair_i] == None: # if left pair is within chain limits but empty (= None) because already merged previously, shift to the left\n",
                        "   495                                                               left_pair_i -= 1\n",
                        "   496                                                           if left_pair_i>-1: # proceed only if a left pair was found on the left\n",
                        "   497                                                               # Remove from left pair positions, the current position (of the pair to be merged)\n",
                        "   498                                                               left_pair = chain_zip[left_pair_i]\n",
                        "   499                                                               # Skip update of left_pair position set if left_pair = best_pair, to avoid modification of iterating set. This can happen if there is overlap of pairs. No consequences on final result (right?) since right after the loop, the key corresponding to the best pair is deleted, and chain_zip is indeed updated so the problematic cases can be captured at the beginning of the loop.\n",
                        "   500                                                               if left_pair != best_pair:\n",
                        "   501                                                                   left_pair_pos = pair_pos[left_pair]\n",
                        "   502                                                                   left_pair_pos.discard(left_pair_i)\n",
                        "   503                                                               new_pair = (left_pair[0],\"\".join(best_pair)) # construct new left pair\n",
                        "   504                                                               pair_pos[new_pair].add(left_pair_i) # add new pair (if non existing) and its position to the pair_pos lookup table\n",
                        "   505                                                               # update the counts in the pair_len lookuptable\n",
                        "   506                                                               pair_len_delta[left_pair] -= 1\n",
                        "   507                                                               pair_len_delta[new_pair] += 1\n",
                        "   508                                           \n",
                        "   509                                                               # update the list of pairs\n",
                        "   510                                                               chain_zip[left_pair_i] = new_pair\n",
                        "   511                                           \n",
                        "   512                                                           ## merge best pair with right unit.\n",
                        "   513                                                           # Code is symmetric to left_pair but on the right. Comments are omitted\n",
                        "   514                                                           right_pair_i = i+1\n",
                        "   515                                                           while right_pair_i<chain_zip_len and chain_zip[right_pair_i] == None:\n",
                        "   516                                                               right_pair_i += 1\n",
                        "   517                                                           if right_pair_i<chain_zip_len:\n",
                        "   518                                                               right_pair = chain_zip[right_pair_i]\n",
                        "   519                                                               if right_pair != best_pair:\n",
                        "   520                                                                   right_pair_pos = pair_pos[right_pair]\n",
                        "   521                                                                   right_pair_pos.discard(right_pair_i)\n",
                        "   522                                                               new_pair = (\"\".join(best_pair), right_pair[1])\n",
                        "   523                                                               pair_pos[new_pair].add(right_pair_i)\n",
                        "   524                                                               pair_len_delta[right_pair] -= 1\n",
                        "   525                                                               pair_len_delta[new_pair] += 1\n",
                        "   526                                           \n",
                        "   527                                                               chain_zip[right_pair_i] = new_pair\n",
                        "   528                                           \n",
                        "   529                                                           # Empty best pair position in list of pairs\n",
                        "   530                                                           chain_zip[i] = None\n",
                        "   531                                           \n",
                        "   532                                                       # Remove best pair from lookuptables\n",
                        "   533                                                       del pair_pos[best_pair]\n",
                        "   534                                           \n",
                        "   535                                                       return (chain_zip, pair_pos, pair_len_delta)\n",
                        "   536                                           \n",
                        "   537         1          2.0      2.0      0.0          def compute_freq(chain_zip):\n",
                        "   538                                                       # TODO: add the last unit to the decoupling\n",
                        "   539                                                       freq = [pair[0] for pair in chain_zip if pair != None]\n",
                        "   540                                                       if chain_zip[-1]!=None: \n",
                        "   541                                                           freq.append(chain_zip[-1][-1])\n",
                        "   542                                                       freq = Counter(freq)\n",
                        "   543                                                       return freq\n",
                        "   544                                                   \n",
                        "   545                                           \n",
                        "   546         1          2.0      2.0      0.0          if parallel:\n",
                        "   547         1          6.0      6.0      0.0              chunksize = int(corpus_length/self.cpu_count)\n",
                        "   548                                           \n",
                        "   549         1        397.0    397.0      0.0              corpus_chunks = [\"\".join(self.corpus.train[i*chunksize:i*chunksize+chunksize]) for i in range(0,self.cpu_count)]\n",
                        "   550                                           \n",
                        "   551         1        331.0    331.0      0.0              with Parallel(n_jobs=self.cpu_count, require='sharedmem') as parallel_pool:\n",
                        "   552         1        432.0    432.0      0.0                  print(\"Computing in parallel\")\n",
                        "   553         1         38.0     38.0      0.0                  print(\"Normalize and jobs data...\")\n",
                        "   554         1          5.0      5.0      0.0                  start = time.time()\n",
                        "   555         1     478981.0 478981.0     27.4                  jobs_data = parallel_pool(delayed(pre_process)(chunk,self.normalizer.normalize) for chunk in corpus_chunks)\n",
                        "   556                                           \n",
                        "   557         1       3631.0   3631.0      0.2                  pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                        "   558                                           \n",
                        "   559         1         52.0     52.0      0.0                  best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                        "   560                                           \n",
                        "   561         1          4.0      4.0      0.0                  merges = [\" \".join(best_pair)]\n",
                        "   562         1        248.0    248.0      0.0                  print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   563                                           \n",
                        "   564         1         37.0     37.0      0.0                  print(\"Build alphabet...\")\n",
                        "   565         1          3.0      3.0      0.0                  start = time.time()\n",
                        "   566         1          8.0      8.0      0.0                  alphabet = Counter()\n",
                        "   567      1061       2982.0      2.8      0.2                  for (l,r),v in pair_len_global.items():\n",
                        "   568      1060       3754.0      3.5      0.2                      alphabet[l] += v\n",
                        "   569                                                           # In extreme cases, right characters of pairs might not be left characters. If there are such chars, they're added with freq 1\n",
                        "   570         1        221.0    221.0      0.0                  left_out_chars = {r for l,r in pair_len_global.keys()}-alphabet.keys()\n",
                        "   571         1          5.0      5.0      0.0                  if len(left_out_chars)>0:\n",
                        "   572                                                               print(f\"Adding characters: {left_out_chars}\")\n",
                        "   573                                                               for char in left_out_chars:\n",
                        "   574                                                                   alphabet[char] += 1\n",
                        "   575         1        243.0    243.0      0.0                  print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   576                                           \n",
                        "   577         1          4.0      4.0      0.0                  alpha_len = len(alphabet)\n",
                        "   578         1          3.0      3.0      0.0                  special_tokens_len = 0 if special_tokens == None else len(special_tokens)\n",
                        "   579                                                           \n",
                        "   580         1         27.0     27.0      0.0                  print(f\"Alphabet Size: {alpha_len}\")\n",
                        "   581         1         26.0     26.0      0.0                  print(f\"Special Tokens Size: {special_tokens_len}\")\n",
                        "   582                                           \n",
                        "   583                                                           \n",
                        "   584         1          3.0      3.0      0.0                  if vocab_size<0:\n",
                        "   585                                                               voc_final_length = alpha_len + abs(vocab_size) + special_tokens_len\n",
                        "   586                                                           else:\n",
                        "   587         1          3.0      3.0      0.0                      voc_final_length = vocab_size\n",
                        "   588                                           \n",
                        "   589         1          3.0      3.0      0.0                  delta_voc = voc_final_length - alpha_len - special_tokens_len\n",
                        "   590                                           \n",
                        "   591         1         23.0     23.0      0.0                  print(f\"Terms to compute: {delta_voc}\\n\")\n",
                        "   592                                           \n",
                        "   593         1        174.0    174.0      0.0                  print(\"Enter loop\")\n",
                        "   594                                           \n",
                        "   595         1      32549.0  32549.0      1.9                  t = trange(delta_voc, disable = not progress_bar)\n",
                        "   596       147      46296.0    314.9      2.6                  for _ in t:\n",
                        "   597       146     206327.0   1413.2     11.8                      t.set_description(f\"Pair: {best_pair}, {best_pair_len}\")\n",
                        "   598       146      30454.0    208.6      1.7                      t.refresh()\n",
                        "   599                                           \n",
                        "   600       146     647368.0   4434.0     37.0                      jobs_data = parallel_pool(delayed(process_best_pair)(chain_zip, pair_pos, best_pair) for chain_zip, pair_pos, pair_len_delta in jobs_data)\n",
                        "   601                                           \n",
                        "   602       730       4919.0      6.7      0.3                      for chain_zip, pair_pos, pair_len_delta in jobs_data:\n",
                        "   603       584      67823.0    116.1      3.9                          pair_len_global.update(pair_len_delta)\n",
                        "   604                                           \n",
                        "   605                                                               # Remove best_pair from pair_len\n",
                        "   606       146       1299.0      8.9      0.1                      del pair_len_global[best_pair]\n",
                        "   607                                           \n",
                        "   608       146     137616.0    942.6      7.9                      best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                        "   609                                           \n",
                        "   610       146       1069.0      7.3      0.1                      merges.append(\" \".join(best_pair))\n",
                        "   611                                                               # print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   612                                           \n",
                        "   613         1         33.0     33.0      0.0                  print(\"Compute freq...\")\n",
                        "   614         1          4.0      4.0      0.0                  start = time.time()\n",
                        "   615         1      73583.0  73583.0      4.2                  freqs = parallel_pool(delayed(compute_freq)(job_data[0]) for job_data in jobs_data)\n",
                        "   616         1       3606.0   3606.0      0.2                  freq = reduce(operator.add, freqs)\n",
                        "   617         1       1934.0   1934.0      0.1                  print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   618                                                   \n",
                        "   619                                                   else:\n",
                        "   620                                                       #TODO: Sequential computing not tested\n",
                        "   621                                                       print(\"Computing sequentially\")\n",
                        "   622                                                       print(\"Normalize and jobs data...\")\n",
                        "   623                                                       start = time.time()\n",
                        "   624                                                       corpus_chain = \"\".join(self.corpus.train[:corpus_length])\n",
                        "   625                                                       chain_zip, pair_pos, pair_len_global = pre_process(corpus_chain,self.normalizer.normalize)\n",
                        "   626                                           \n",
                        "   627                                                       best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                        "   628                                                       \n",
                        "   629                                                       merges = [\" \".join(best_pair)]\n",
                        "   630                                                       print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   631                                           \n",
                        "   632                                                       print(\"Build alphabet...\")\n",
                        "   633                                                       start = time.time()\n",
                        "   634                                                       alphabet = Counter()\n",
                        "   635                                                       for (l,r),v in pair_len_global.items():\n",
                        "   636                                                           alphabet[l] =+ v\n",
                        "   637                                                       # In extreme cases, right characters of pairs might not be left characters. If there are such chars, they're added with freq 1\n",
                        "   638                                                       left_out_chars = {r for l,r in pair_len_global.keys()}-alphabet.keys()\n",
                        "   639                                                       if len(left_out_chars)>0:\n",
                        "   640                                                           print(f\"Adding characters: {left_out_chars}\")\n",
                        "   641                                                           for char in left_out_chars:\n",
                        "   642                                                               alphabet[char] =+ 1\n",
                        "   643                                                       print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   644                                           \n",
                        "   645                                                       alpha_len = len(alphabet)\n",
                        "   646                                                       special_tokens_len = 0 if special_tokens == None else len(special_tokens)\n",
                        "   647                                                       \n",
                        "   648                                                       print(f\"Alphabet Size: {alpha_len}\")\n",
                        "   649                                                       print(f\"Special Tokens Size: {special_tokens_len}\")\n",
                        "   650                                                       \n",
                        "   651                                                       if vocab_size<0:\n",
                        "   652                                                           voc_final_length = alpha_len + abs(vocab_size) + special_tokens_len\n",
                        "   653                                                       else:\n",
                        "   654                                                           voc_final_length = vocab_size\n",
                        "   655                                           \n",
                        "   656                                                       delta_voc = voc_final_length - alpha_len - special_tokens_len\n",
                        "   657                                                       \n",
                        "   658                                                       print(f\"Terms to compute: {delta_voc}\\n\")\n",
                        "   659                                           \n",
                        "   660                                                       print(\"Enter loop\")\n",
                        "   661                                           \n",
                        "   662                                                       # for _ in trange(delta_voc):\n",
                        "   663                                                       t = trange(delta_voc, disable = not progress_bar)\n",
                        "   664                                                       for _ in t:\n",
                        "   665                                                           t.set_description(f\"Pair: {best_pair}, {best_pair_len}\")\n",
                        "   666                                                           t.refresh()\n",
                        "   667                                           \n",
                        "   668                                                           chain_zip, pair_pos, pair_len_delta = process_best_pair(chain_zip, pair_pos, best_pair)\n",
                        "   669                                           \n",
                        "   670                                                           # Remove best_pair from pair_len\n",
                        "   671                                                           pair_len_global.update(pair_len_delta)\n",
                        "   672                                           \n",
                        "   673                                                           del pair_len_global[best_pair]\n",
                        "   674                                                           best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                        "   675                                           \n",
                        "   676                                                           merges.append(\" \".join(best_pair))\n",
                        "   677                                                           # print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   678                                           \n",
                        "   679                                                           if saveQ == True:\n",
                        "   680                                                               voc_partial_len = alpha_len + special_tokens_len + _ + 1\n",
                        "   681                                                               if voc_partial_len % save_step == 0 and voc_partial_len != voc_final_length:\n",
                        "   682                                           \n",
                        "   683                                                                   print(\"Saving intermediate results...\")\n",
                        "   684                                                                   start = time.time()\n",
                        "   685                                                                   freq = compute_freq(job_data[0])\n",
                        "   686                                           \n",
                        "   687                                                                   vocabulary = freq.most_common()\n",
                        "   688                                                                   \n",
                        "   689                                                                   if special_tokens != None:\n",
                        "   690                                                                       vocabulary = vocabulary + [(token,0) for token in special_tokens]\n",
                        "   691                                                                   \n",
                        "   692                                                                   self.merges = merges\n",
                        "   693                                                                   self.encode = {k:i for i,(k,v) in enumerate(vocabulary)}\n",
                        "   694                                                                   self.freq = dict(vocabulary)\n",
                        "   695                                                                   self.alpha = dict(alphabet.most_common())\n",
                        "   696                                                                   step_path = self.path / str(voc_partial_len)\n",
                        "   697                                                                   self.save(step_path)\n",
                        "   698                                                                   print(f\"... computed in {time.time()-start} secs.\")\n",
                        "   699                                                                   print(f\"Intermediate vocabulary saved to {step_path}\\n\")\n",
                        "   700                                                       \n",
                        "   701                                                       print(\"Compute freq...\")\n",
                        "   702                                                       start = time.time()\n",
                        "   703                                                       freq = compute_freq(chain_zip)\n",
                        "   704                                                       print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "   705                                           \n",
                        "   706         1         99.0     99.0      0.0          vocabulary = freq.most_common()\n",
                        "   707                                                   \n",
                        "   708         1          4.0      4.0      0.0          if special_tokens != None:\n",
                        "   709         1          8.0      8.0      0.0              vocabulary = vocabulary + [(token,0) for token in special_tokens]\n",
                        "   710                                                   \n",
                        "   711         1          4.0      4.0      0.0          self.merges = merges\n",
                        "   712         1         65.0     65.0      0.0          self.encode = {k:i for i,(k,v) in enumerate(vocabulary)}\n",
                        "   713         1         16.0     16.0      0.0          self.freq = dict(vocabulary)\n",
                        "   714         1         14.0     14.0      0.0          self.alpha = dict(alphabet.most_common())\n",
                        "   715                                           \n",
                        "   716         1         37.0     37.0      0.0          self.decode = {i:k for k,i in self.encode.items()}\n",
                        "   717                                                   \n",
                        "   718         1          4.0      4.0      0.0          self.len = len(vocabulary)     \n",
                        "   719         1          7.0      7.0      0.0          self.freq_mass = sum(self.freq.values())\n",
                        "   720         1         67.0     67.0      0.0          self.prob = {k:v/self.freq_mass for k,v in self.freq.items()}\n",
                        "   721                                           \n",
                        "   722         1         97.0     97.0      0.0          print(\"Vocabulary built\")\n",
                        "   723                                                   \n",
                        "   724         1          3.0      3.0      0.0          if save == True:\n",
                        "   725                                                       self.save()\n",
                        "   726                                                       print(f\"Vocabulary saved to {self.path}\")"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "def pre_process(corpus_chunk, normalizer):\n",
                "    # Normalize\n",
                "    chain_zip = normalizer(corpus_chunk)\n",
                "    # Build list of pairs\n",
                "    chain_zip = list(zip(chain_zip,chain_zip[1:]))\n",
                "    # Create a lookup table of all the positions where a pair appears in a corpus\n",
                "    pair_pos = defaultdict(set)\n",
                "    for i,k in list(enumerate(chain_zip)):\n",
                "        pair_pos[k].add(i)\n",
                "    # From the previous lookup table, create another lookup table of the frequency of each pair (given by the size of the set of its positions)\n",
                "    pair_len = Counter()\n",
                "    for k,pos in pair_pos.items():\n",
                "        pair_len[k] = len(pos)\n",
                "    \n",
                "    return (chain_zip, pair_pos, pair_len)\n",
                "\n",
                "def process_best_pair(chain_zip, pair_pos, best_pair):\n",
                "    chain_zip_len = len(chain_zip)\n",
                "    pair_len_delta = Counter()\n",
                "\n",
                "    for i in pair_pos[best_pair]:\n",
                "        # Skip iteration if position corresponds to a modified set of positions during the iteration. This can happen if there is overlap of pairs, such as \"000\", where (\"0\",\"0\") has itself as right pair. Note that, due to unordered implementation of sets, this entails a lack of systematicity in overlapping cases: \"000\" can be counted randomly as (\"00\",\"0\") or (\"0\",\"00\").\n",
                "        # TODO: Investigate the cost of ordering sets. In which case, the following \"if\" condition might only be needed for right pairs.\n",
                "        if chain_zip[i]!=best_pair:\n",
                "            continue\n",
                "        ## merge best pair with left unit\n",
                "        left_pair_i = i-1\n",
                "        while left_pair_i>=0 and chain_zip[left_pair_i] == None: # if left pair is within chain limits but empty (= None) because already merged previously, shift to the left\n",
                "            left_pair_i -= 1\n",
                "        if left_pair_i>-1: # proceed only if a left pair was found on the left\n",
                "            # Remove from left pair positions, the current position (of the pair to be merged)\n",
                "            left_pair = chain_zip[left_pair_i]\n",
                "            # Skip update of left_pair position set if left_pair = best_pair, to avoid modification of iterating set. This can happen if there is overlap of pairs. No consequences on final result (right?) since right after the loop, the key corresponding to the best pair is deleted, and chain_zip is indeed updated so the problematic cases can be captured at the beginning of the loop.\n",
                "            if left_pair != best_pair:\n",
                "                left_pair_pos = pair_pos[left_pair]\n",
                "                left_pair_pos.discard(left_pair_i)\n",
                "            new_pair = (left_pair[0],\"\".join(best_pair)) # construct new left pair\n",
                "            pair_pos[new_pair].add(left_pair_i) # add new pair (if non existing) and its position to the pair_pos lookup table\n",
                "            # update the counts in the pair_len lookuptable\n",
                "            pair_len_delta[left_pair] -= 1\n",
                "            pair_len_delta[new_pair] += 1\n",
                "\n",
                "            # update the list of pairs\n",
                "            chain_zip[left_pair_i] = new_pair\n",
                "\n",
                "        ## merge best pair with right unit.\n",
                "        # Code is symmetric to left_pair but on the right. Comments are omitted\n",
                "        right_pair_i = i+1\n",
                "        while right_pair_i<chain_zip_len and chain_zip[right_pair_i] == None:\n",
                "            right_pair_i += 1\n",
                "        if right_pair_i<chain_zip_len:\n",
                "            right_pair = chain_zip[right_pair_i]\n",
                "            if right_pair != best_pair:\n",
                "                right_pair_pos = pair_pos[right_pair]\n",
                "                right_pair_pos.discard(right_pair_i)\n",
                "            new_pair = (\"\".join(best_pair), right_pair[1])\n",
                "            pair_pos[new_pair].add(right_pair_i)\n",
                "            pair_len_delta[right_pair] -= 1\n",
                "            pair_len_delta[new_pair] += 1\n",
                "\n",
                "            chain_zip[right_pair_i] = new_pair\n",
                "\n",
                "        # Empty best pair position in list of pairs\n",
                "        chain_zip[i] = None\n",
                "\n",
                "    # Remove best pair from lookuptables\n",
                "    del pair_pos[best_pair]\n",
                "\n",
                "    return (chain_zip, pair_pos, pair_len_delta)\n",
                "\n",
                "def compute_freq(chain_zip):\n",
                "    # TODO: add the last unit to the decoupling\n",
                "    freq = [pair[0] for pair in chain_zip if pair != None]\n",
                "    if chain_zip[-1]!=None: \n",
                "        freq.append(chain_zip[-1][-1])\n",
                "    freq = Counter(freq)\n",
                "    return freq"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "vocab_size = 2000\n",
                "corpus_length = 2000\n",
                "\n",
                "self = semiotic.vocab\n",
                "corpus = None\n",
                "\n",
                "special_tokens = None\n",
                "save = False\n",
                "save_step = None\n",
                "truncate_best_size = None\n",
                "progress_bar = True\n",
                "resume_merges = False\n",
                "parallel = False\n",
                "\n",
                "if corpus == None:\n",
                "    corpus = self.name\n",
                "\n",
                "if vocab_size == None:\n",
                "    vocab_size = self.config.size\n",
                "\n",
                "if special_tokens == None:\n",
                "    special_tokens = self.config.special_tokens\n",
                "\n",
                "if corpus_length == None:\n",
                "    corpus_length = self.corpus.train_len\n",
                "\n",
                "saveQ = False"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def test(\n",
                "    vocab_size = vocab_size,\n",
                "    corpus_length = corpus_length,\n",
                "\n",
                "    self = self,\n",
                "    corpus = corpus,\n",
                "\n",
                "    special_tokens = special_tokens,\n",
                "    save = save,\n",
                "    save_step = save_step,\n",
                "    progress_bar = progress_bar,\n",
                "    resume_merges = resume_merges,\n",
                "    parallel = parallel,\n",
                "\n",
                "    \n",
                "):\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "\n",
                "chunksize = int(corpus_length/self.cpu_count)\n",
                "\n",
                "corpus_chunks = [\"\".join(self.corpus.train[i*chunksize:i*chunksize+chunksize]) for i in range(0,self.cpu_count)]\n",
                "\n",
                "with Parallel(n_jobs=self.cpu_count, require='sharedmem') as parallel_pool:\n",
                "    print(\"Computing in parallel\")\n",
                "    print(\"Normalize and jobs data...\")\n",
                "    start = time.time()\n",
                "    jobs_data = parallel_pool(delayed(pre_process)(chunk,self.normalizer.normalize) for chunk in corpus_chunks)\n",
                "\n",
                "    pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                "\n",
                "    best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                "\n",
                "    merges = [\" \".join(best_pair)]\n",
                "    print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "    print(\"Build alphabet...\")\n",
                "    start = time.time()\n",
                "    alphabet = Counter()\n",
                "    for (l,r),v in pair_len_global.items():\n",
                "        alphabet[l] += v\n",
                "    # In extreme cases, right characters of pairs might not be left characters. If there are such chars, they're added with freq 1\n",
                "    left_out_chars = {r for l,r in pair_len_global.keys()}-alphabet.keys()\n",
                "    if len(left_out_chars)>0:\n",
                "        print(f\"Adding characters: {left_out_chars}\")\n",
                "        for char in left_out_chars:\n",
                "            alphabet[char] += 1\n",
                "    print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "    alpha_len = len(alphabet)\n",
                "    special_tokens_len = 0 if special_tokens == None else len(special_tokens)\n",
                "    \n",
                "    print(f\"Alphabet Size: {alpha_len}\")\n",
                "    print(f\"Special Tokens Size: {special_tokens_len}\")\n",
                "\n",
                "    \n",
                "    if vocab_size<0:\n",
                "        voc_final_length = alpha_len + abs(vocab_size) + special_tokens_len\n",
                "    else:\n",
                "        voc_final_length = vocab_size\n",
                "\n",
                "    delta_voc = voc_final_length - alpha_len - special_tokens_len\n",
                "\n",
                "    print(f\"Terms to compute: {delta_voc}\\n\")\n",
                "\n",
                "    print(\"Enter loop\")\n",
                "\n",
                "    t = trange(delta_voc, disable = not progress_bar)\n",
                "    for _ in t:\n",
                "        t.set_description(f\"Pair: {best_pair}, {best_pair_len}\")\n",
                "        t.refresh()\n",
                "\n",
                "        jobs_data = parallel_pool(delayed(process_best_pair)(chain_zip, pair_pos, best_pair) for chain_zip, pair_pos, pair_len_delta in jobs_data)\n",
                "\n",
                "        for chain_zip, pair_pos, pair_len_delta in jobs_data:\n",
                "            pair_len_global.update(pair_len_delta)\n",
                "\n",
                "        # Remove best_pair from pair_len\n",
                "        del pair_len_global[best_pair]\n",
                "\n",
                "        best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                "\n",
                "        merges.append(\" \".join(best_pair))\n",
                "        # print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "    print(\"Compute freq...\")\n",
                "    start = time.time()\n",
                "    freqs = parallel_pool(delayed(compute_freq)(job_data[0]) for job_data in jobs_data)\n",
                "    freq = reduce(operator.add, freqs)\n",
                "    print(f\"... computed in {time.time()-start} secs.\\n\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 0.11342406272888184 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.0003790855407714844 secs.\n",
                        "\n",
                        "Alphabet Size: 49\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 1946\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, max=1946.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "545dec6339a74423bd3578def54fdd4f"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "Compute freq...\n",
                        "... computed in 0.0218658447265625 secs.\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "source": [
                "for key in new_freq.keys():\n",
                "    if new_freq[key]!=semiotic.vocab.freq[key]:\n",
                "        print(key,new_freq[key],freq[key],semiotic.vocab.freq[key])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "and 510 510 457\n",
                        "to 443 431 469\n",
                        "s 432 441 413\n",
                        "in 417 417 376\n",
                        "ed 413 404 383\n",
                        "ing 413 405 389\n",
                        "the 379 379 339\n",
                        "of 378 366 347\n",
                        "as 325 315 302\n",
                        "is 309 310 287\n",
                        "al 283 283 270\n",
                        "er 276 270 290\n",
                        "it 267 267 254\n",
                        "es 267 277 272\n",
                        "at 256 256 272\n",
                        "on 255 248 256\n",
                        "m 254 255 247\n",
                        "an 252 253 232\n",
                        "or 245 245 237\n",
                        "t 244 242 235\n",
                        "that 230 230 212\n",
                        "i 229 229 248\n",
                        "c 224 219 200\n",
                        "‘ 224 224 210\n",
                        "y 220 220 223\n",
                        "ly 219 213 185\n",
                        "p 214 222 230\n",
                        "for 209 206 204\n",
                        "r 209 208 193\n",
                        "g 204 199 199\n",
                        "f 201 196 205\n",
                        "d 195 195 170\n",
                        "a 194 195 184\n",
                        "b 193 191 199\n",
                        "e 192 203 186\n",
                        "ic 180 180 177\n",
                        "n 176 176 180\n",
                        "was 176 176 161\n",
                        "o 175 175 169\n",
                        "l 171 167 174\n",
                        "h 167 173 178\n",
                        "w 166 166 171\n",
                        "am 161 161 164\n",
                        "el 159 156 158\n",
                        "ad 158 157 141\n",
                        "en 157 156 156\n",
                        "re 157 156 139\n",
                        "ac 155 149 143\n",
                        "you 153 153 148\n",
                        "st 151 151 165\n",
                        "are 150 150 148\n",
                        "with 149 149 138\n",
                        "sh 143 143 144\n",
                        "ar 143 143 151\n",
                        "inthe 143 143 142\n",
                        "ch 140 140 132\n",
                        "all 139 140 142\n",
                        "this 139 139 135\n",
                        "but 137 137 139\n",
                        "im 134 134 131\n",
                        "id 131 131 130\n",
                        "k 130 129 125\n",
                        "th 122 122 143\n",
                        "ap 120 120 106\n",
                        "le 120 128 130\n",
                        "un 118 118 127\n",
                        "ation 118 117 106\n",
                        "its 114 114 109\n",
                        "his 114 114 106\n",
                        "em 113 113 115\n",
                        "sand 112 112 107\n",
                        "from 112 112 114\n",
                        "ent 112 112 107\n",
                        "them 112 105 107\n",
                        "be 109 109 111\n",
                        "ec 108 108 92\n",
                        "us 106 105 108\n",
                        "u 104 105 103\n",
                        "sp 103 103 111\n",
                        "se 103 106 104\n",
                        "v 102 102 110\n",
                        "ts 101 101 92\n",
                        "et 99 99 106\n",
                        "ab 97 97 95\n",
                        "sof 96 96 90\n",
                        "up 95 95 96\n",
                        "ag 95 95 96\n",
                        "ow 94 94 88\n",
                        "if 94 94 95\n",
                        "il 93 86 105\n",
                        "ur 93 93 109\n",
                        "tof 92 92 90\n",
                        "ol 89 89 103\n",
                        "there 88 88 78\n",
                        "one 87 87 96\n",
                        "ion 86 77 89\n",
                        "op 86 86 75\n",
                        "ment 85 85 86\n",
                        "not 83 83 88\n",
                        "ay 83 83 82\n",
                        "j 82 82 98\n",
                        "end 82 82 84\n",
                        "our 81 81 82\n",
                        "ep 80 80 73\n",
                        "thep 79 79 78\n",
                        "thes 79 79 85\n",
                        "est 79 79 84\n",
                        "con 78 78 73\n",
                        "ex 78 78 73\n",
                        "ul 77 77 72\n",
                        "ant 76 76 91\n",
                        "ot 76 76 81\n",
                        "he 75 75 74\n",
                        "edto 75 75 70\n",
                        "res 74 74 71\n",
                        "have 74 74 70\n",
                        "mo 71 71 63\n",
                        "ill 70 70 64\n",
                        "om 69 69 64\n",
                        "ro 69 69 64\n",
                        "lo 68 68 60\n",
                        "ies 68 68 62\n",
                        "2 67 67 63\n",
                        "then 66 66 61\n",
                        "ated 66 66 62\n",
                        "thed 66 66 67\n",
                        "me 66 66 58\n",
                        "sc 66 66 74\n",
                        "os 65 65 61\n",
                        "oc 65 65 68\n",
                        "ir 65 66 55\n",
                        "ef 65 65 58\n",
                        "li 64 64 60\n",
                        "ity 64 64 52\n",
                        "thef 63 63 73\n",
                        "were 63 63 55\n",
                        "ter 62 75 83\n",
                        "ou 62 62 40\n",
                        "ist 62 62 90\n",
                        "will 61 61 58\n",
                        "ew 61 61 59\n",
                        "ish 61 61 58\n",
                        "od 58 61 50\n",
                        "off 58 58 57\n",
                        "eand 58 58 57\n",
                        "ell 58 58 53\n",
                        "ical 58 58 64\n",
                        "wh 57 50 44\n",
                        "out 57 57 59\n",
                        "ain 57 57 65\n",
                        "ast 56 50 55\n",
                        "tor 56 56 60\n",
                        "over 56 56 52\n",
                        "like 55 49 51\n",
                        "tw 55 55 39\n",
                        "aw 54 54 57\n",
                        "no 54 54 61\n",
                        "gr 54 54 53\n",
                        "ear 54 54 44\n",
                        "ive 53 57 57\n",
                        "ess 53 53 50\n",
                        "qu 52 52 49\n",
                        "ig 52 52 46\n",
                        "ure 52 52 44\n",
                        "ind 51 51 56\n",
                        "ial 51 51 46\n",
                        "work 51 51 57\n",
                        "ri 51 51 62\n",
                        "been 51 51 52\n",
                        "pro 50 50 45\n",
                        "him 49 49 58\n",
                        "ance 49 49 62\n",
                        "ge 48 48 55\n",
                        "we 48 48 47\n",
                        "ys 48 48 62\n",
                        "form 48 48 44\n",
                        "ach 47 47 53\n",
                        "6 47 47 49\n",
                        "int 46 46 51\n",
                        "4 46 46 56\n",
                        "any 46 46 53\n",
                        "ath 45 45 51\n",
                        "other 45 45 40\n",
                        "ha 44 44 56\n",
                        "ther 43 43 44\n",
                        "would 43 43 57\n",
                        "7 42 42 33\n",
                        "ire 42 42 34\n",
                        "theb 42 42 48\n",
                        "ak 41 41 38\n",
                        "ry 41 41 40\n",
                        "ice 41 47 53\n",
                        "9 41 41 37\n",
                        "um 40 40 60\n",
                        "ingto 40 40 48\n",
                        "able 40 40 39\n",
                        "act 40 40 69\n",
                        "some 40 40 46\n",
                        "old 40 40 42\n",
                        "more 40 40 45\n",
                        "well 39 39 40\n",
                        "ev 39 44 35\n",
                        "thel 39 39 41\n",
                        "tion 39 39 51\n",
                        "ence 38 38 37\n",
                        "edin 38 38 39\n",
                        "fac 38 38 37\n",
                        "cl 37 37 44\n",
                        "than 37 37 43\n",
                        "bl 37 37 32\n",
                        "way 37 37 44\n",
                        "anc 37 37 52\n",
                        "ally 36 36 38\n",
                        "ary 36 36 49\n",
                        "top 36 36 34\n",
                        "year 35 35 32\n",
                        "dis 35 32 49\n",
                        "here 35 35 34\n",
                        "ah 35 35 42\n",
                        "can 34 34 30\n",
                        "ian 34 34 33\n",
                        "su 34 34 29\n",
                        "thing 34 34 41\n",
                        "may 34 34 35\n",
                        "man 34 34 32\n",
                        "com 34 34 31\n",
                        "ith 34 34 30\n",
                        "cr 33 33 32\n",
                        "ition 33 33 45\n",
                        "co 33 33 27\n",
                        "aid 33 33 47\n",
                        "nt 33 33 42\n",
                        "ce 33 39 39\n",
                        "ound 33 33 35\n",
                        "som 32 32 28\n",
                        "ton 31 31 32\n",
                        "ise 31 30 30\n",
                        "ings 30 30 28\n",
                        "oh 30 30 34\n",
                        "enc 30 30 31\n",
                        "min 30 30 36\n",
                        "inter 29 29 36\n",
                        "theg 29 29 25\n",
                        "ust 29 29 28\n",
                        "0 29 29 24\n",
                        "rep 29 29 31\n",
                        "ud 29 29 24\n",
                        "ob 29 29 36\n",
                        "igh 29 29 34\n",
                        "arm 28 28 34\n",
                        "po 28 28 35\n",
                        "ber 28 28 35\n",
                        "str 28 28 35\n",
                        "ame 27 27 33\n",
                        "ors 27 27 25\n",
                        "ser 27 27 33\n",
                        "tim 27 27 35\n",
                        "ous 27 26 26\n",
                        "iz 27 26 19\n",
                        "ons 26 26 25\n",
                        "ong 26 26 32\n",
                        "comm 26 26 33\n",
                        "rem 26 26 41\n",
                        "ould 26 26 24\n",
                        "thatthe 25 25 26\n",
                        "ities 25 25 31\n",
                        "ands 25 25 26\n",
                        "car 25 25 31\n",
                        "ions 25 25 35\n",
                        "esh 25 24 32\n",
                        "uc 25 25 20\n",
                        "edthe 25 25 29\n",
                        "bel 25 12 31\n",
                        "ase 25 14 17\n",
                        "look 25 25 34\n",
                        "ices 25 25 20\n",
                        "iv 25 29 27\n",
                        "sub 25 25 32\n",
                        "yes 24 24 30\n",
                        "who 24 24 43\n",
                        "bo 24 24 33\n",
                        "tom 24 24 33\n",
                        "day 24 24 37\n",
                        "years 24 24 27\n",
                        "19 24 24 30\n",
                        "mar 24 24 26\n",
                        "leg 23 23 17\n",
                        "app 23 23 36\n",
                        "comp 23 29 26\n",
                        "ates 23 24 30\n",
                        "read 23 23 24\n",
                        "pos 23 23 30\n",
                        "so 23 23 30\n",
                        "ren 23 23 29\n",
                        "ather 22 22 30\n",
                        "pe 22 22 18\n",
                        "ret 22 22 31\n",
                        "bes 22 22 28\n",
                        "si 22 22 28\n",
                        "ction 22 22 7\n",
                        "198 22 22 30\n",
                        "bu 22 22 29\n",
                        "hes 22 22 21\n",
                        "set 22 22 23\n",
                        "au 21 21 27\n",
                        "att 21 21 31\n",
                        "ome 21 26 26\n",
                        "most 21 21 27\n",
                        "cont 21 21 28\n",
                        "cor 21 21 27\n",
                        "coun 21 21 24\n",
                        "vi 21 21 27\n",
                        "qual 21 21 17\n",
                        "first 21 21 19\n",
                        "ref 21 21 23\n",
                        "every 21 21 28\n",
                        "sl 21 21 22\n",
                        "see 21 25 25\n",
                        "ph 20 20 26\n",
                        "ays 20 20 15\n",
                        "fin 20 20 33\n",
                        "ared 20 20 19\n",
                        "ised 20 20 19\n",
                        "eas 20 24 33\n",
                        "view 20 20 27\n",
                        "row 20 20 16\n",
                        "arg 20 20 26\n",
                        "ments 20 20 19\n",
                        "ofthem 20 20 21\n",
                        "try 20 20 21\n",
                        "good 20 20 16\n",
                        "ected 19 19 17\n",
                        "ble 19 19 21\n",
                        "could 19 19 25\n",
                        "itt 19 19 18\n",
                        "tern 19 19 13\n",
                        "cur 19 19 15\n",
                        "ose 19 19 8\n",
                        "people 19 19 25\n",
                        "ake 19 19 28\n",
                        "made 19 18 18\n",
                        "def 19 19 18\n",
                        "adv 19 19 26\n",
                        "ail 19 19 23\n",
                        "fl 18 18 14\n",
                        "fe 18 17 17\n",
                        "used 18 20 20\n",
                        "ie 18 15 15\n",
                        "atch 18 18 25\n",
                        "ting 18 18 15\n",
                        "two 18 18 17\n",
                        "de 18 18 33\n",
                        "ib 18 20 26\n",
                        "oun 18 18 29\n",
                        "stand 18 18 20\n",
                        "emp 17 17 27\n",
                        "dep 17 17 16\n",
                        "ened 17 17 19\n",
                        "ection 17 16 11\n",
                        "theres 17 17 16\n",
                        "iam 17 17 16\n",
                        "og 17 17 16\n",
                        "sthe 17 17 18\n",
                        "tors 17 17 15\n",
                        "iss 17 17 24\n",
                        "ved 17 17 28\n",
                        "mind 16 16 13\n",
                        "reat 16 16 22\n",
                        "ks 16 16 18\n",
                        "tofthe 16 16 13\n",
                        "eds 16 16 17\n",
                        "pat 16 16 11\n",
                        "ced 16 16 8\n",
                        "gest 16 16 12\n",
                        "don 16 16 15\n",
                        "ord 16 16 25\n",
                        "let 16 16 15\n",
                        "col 16 16 21\n",
                        "cess 16 16 15\n",
                        "develop 16 16 14\n",
                        "ving 16 16 22\n",
                        "ext 16 16 23\n",
                        "prov 16 16 22\n",
                        "ater 16 16 20\n",
                        "remain 15 15 11\n",
                        "sthat 15 15 20\n",
                        "peri 15 15 16\n",
                        "down 15 15 10\n",
                        "isto 15 15 16\n",
                        "enceof 15 15 11\n",
                        "edthat 15 15 17\n",
                        "went 15 15 16\n",
                        "wew 15 15 12\n",
                        "country 15 15 14\n",
                        "etr 15 15 14\n",
                        "rom 15 15 14\n",
                        "care 15 15 14\n",
                        "mem 14 14 12\n",
                        "week 14 14 17\n",
                        "inf 14 14 17\n"
                    ]
                },
                {
                    "output_type": "error",
                    "ename": "KeyError",
                    "evalue": "'ct'",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-29-df8e1a04555d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_freq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnew_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0msemiotic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msemiotic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mKeyError\u001b[0m: 'ct'"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# clean\n",
                "%lprun -f test test()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 10.095965147018433 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.013791084289550781 secs.\n",
                        "\n",
                        "Alphabet Size: 121\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 1874\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, max=1874.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "b778ddcafc9143d190f4998604fe3e48"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "Compute freq...\n",
                        "... computed in 3.718216896057129 secs.\n",
                        "\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Timer unit: 1e-06 s\n",
                        "\n",
                        "Total time: 225.717 s\n",
                        "File: <ipython-input-4-d971a3e1e977>\n",
                        "Function: test at line 1\n",
                        "\n",
                        "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
                        "==============================================================\n",
                        "     1                                           def test(\n",
                        "     2                                               vocab_size = vocab_size,\n",
                        "     3                                               corpus_length = corpus_length,\n",
                        "     4                                           \n",
                        "     5                                               self = self,\n",
                        "     6                                               corpus = corpus,\n",
                        "     7                                           \n",
                        "     8                                               special_tokens = special_tokens,\n",
                        "     9                                               save = save,\n",
                        "    10                                               save_step = save_step,\n",
                        "    11                                               progress_bar = progress_bar,\n",
                        "    12                                               resume_merges = resume_merges,\n",
                        "    13                                               parallel = parallel,\n",
                        "    14                                           \n",
                        "    15                                               \n",
                        "    16                                           ):\n",
                        "    17                                           \n",
                        "    18                                           \n",
                        "    19         1          8.0      8.0      0.0      chunksize = int(corpus_length/self.cpu_count)\n",
                        "    20                                           \n",
                        "    21         1      39712.0  39712.0      0.0      corpus_chunks = [\"\".join(self.corpus.train[i*chunksize:i*chunksize+chunksize]) for i in range(0,self.cpu_count)]\n",
                        "    22                                           \n",
                        "    23         1        436.0    436.0      0.0      with Parallel(n_jobs=self.cpu_count, require='sharedmem') as parallel_pool:\n",
                        "    24         1        216.0    216.0      0.0          print(\"Computing in parallel\")\n",
                        "    25         1         26.0     26.0      0.0          print(\"Normalize and jobs data...\")\n",
                        "    26         1          3.0      3.0      0.0          start = time.time()\n",
                        "    27         1   10085977.0 10085977.0      4.5          jobs_data = parallel_pool(delayed(pre_process)(chunk,self.normalizer.normalize) for chunk in corpus_chunks)\n",
                        "    28                                           \n",
                        "    29         1       9846.0   9846.0      0.0          pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                        "    30                                           \n",
                        "    31         1        118.0    118.0      0.0          best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                        "    32                                           \n",
                        "    33         1          7.0      7.0      0.0          merges = [\" \".join(best_pair)]\n",
                        "    34         1        177.0    177.0      0.0          print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "    35                                           \n",
                        "    36         1         26.0     26.0      0.0          print(\"Build alphabet...\")\n",
                        "    37         1          2.0      2.0      0.0          start = time.time()\n",
                        "    38         1          6.0      6.0      0.0          alphabet = Counter()\n",
                        "    39      2316       3072.0      1.3      0.0          for (l,r),v in pair_len_global.items():\n",
                        "    40      2315       3334.0      1.4      0.0              alphabet[l] += v\n",
                        "    41                                                   # In extreme cases, right characters of pairs might not be left characters. If there are such chars, they're added with freq 1\n",
                        "    42         1        556.0    556.0      0.0          left_out_chars = {r for l,r in pair_len_global.keys()}-alphabet.keys()\n",
                        "    43         1          3.0      3.0      0.0          if len(left_out_chars)>0:\n",
                        "    44                                                       print(f\"Adding characters: {left_out_chars}\")\n",
                        "    45                                                       for char in left_out_chars:\n",
                        "    46                                                           alphabet[char] += 1\n",
                        "    47         1        175.0    175.0      0.0          print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "    48                                           \n",
                        "    49         1          2.0      2.0      0.0          alpha_len = len(alphabet)\n",
                        "    50         1          2.0      2.0      0.0          special_tokens_len = 0 if special_tokens == None else len(special_tokens)\n",
                        "    51                                                   \n",
                        "    52         1         25.0     25.0      0.0          print(f\"Alphabet Size: {alpha_len}\")\n",
                        "    53         1         21.0     21.0      0.0          print(f\"Special Tokens Size: {special_tokens_len}\")\n",
                        "    54                                           \n",
                        "    55                                                   \n",
                        "    56         1          2.0      2.0      0.0          if vocab_size<0:\n",
                        "    57                                                       voc_final_length = alpha_len + abs(vocab_size) + special_tokens_len\n",
                        "    58                                                   else:\n",
                        "    59         1          1.0      1.0      0.0              voc_final_length = vocab_size\n",
                        "    60                                           \n",
                        "    61         1          1.0      1.0      0.0          delta_voc = voc_final_length - alpha_len - special_tokens_len\n",
                        "    62                                           \n",
                        "    63         1         21.0     21.0      0.0          print(f\"Terms to compute: {delta_voc}\\n\")\n",
                        "    64                                           \n",
                        "    65         1         18.0     18.0      0.0          print(\"Enter loop\")\n",
                        "    66                                           \n",
                        "    67         1      37537.0  37537.0      0.0          t = trange(delta_voc, disable = not progress_bar)\n",
                        "    68      1875    3159718.0   1685.2      1.4          for _ in t:\n",
                        "    69      1874    2323301.0   1239.8      1.0              t.set_description(f\"Pair: {best_pair}, {best_pair_len}\")\n",
                        "    70      1874     394096.0    210.3      0.2              t.refresh()\n",
                        "    71                                           \n",
                        "    72      1874   77891370.0  41564.2     34.5              jobs_data = parallel_pool(delayed(process_best_pair)(chain_zip, pair_pos, best_pair) for chain_zip, pair_pos, pair_len_delta in jobs_data)\n",
                        "    73                                           \n",
                        "    74      9370     164346.0     17.5      0.1              for chain_zip, pair_pos, pair_len_delta in jobs_data:\n",
                        "    75      7496    4693862.0    626.2      2.1                  pair_len_global.update(pair_len_delta)\n",
                        "    76                                           \n",
                        "    77                                                       # Remove best_pair from pair_len\n",
                        "    78      1874      15675.0      8.4      0.0              del pair_len_global[best_pair]\n",
                        "    79                                           \n",
                        "    80      1874  123160837.0  65720.8     54.6              best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                        "    81                                           \n",
                        "    82      1874      12828.0      6.8      0.0              merges.append(\" \".join(best_pair))\n",
                        "    83                                                       # print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "    84                                           \n",
                        "    85         1         43.0     43.0      0.0          print(\"Compute freq...\")\n",
                        "    86         1          4.0      4.0      0.0          start = time.time()\n",
                        "    87         1    3698361.0 3698361.0      1.6          freqs = parallel_pool(delayed(compute_freq)(job_data[0]) for job_data in jobs_data)\n",
                        "    88         1      19834.0  19834.0      0.0          freq = reduce(operator.add, freqs)\n",
                        "    89         1       1237.0   1237.0      0.0          print(f\"... computed in {time.time()-start} secs.\\n\")"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "# max\n",
                "%lprun -f test test()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 11.31916093826294 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.01871466636657715 secs.\n",
                        "\n",
                        "Alphabet Size: 121\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 1874\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, max=1874.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "001ed0383107447eb6792e2a8c3c0854"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "Compute freq...\n",
                        "... computed in 4.060492992401123 secs.\n",
                        "\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Timer unit: 1e-06 s\n",
                        "\n",
                        "Total time: 248.754 s\n",
                        "File: <ipython-input-16-3b7b167b1b1a>\n",
                        "Function: test at line 1\n",
                        "\n",
                        "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
                        "==============================================================\n",
                        "     1                                           def test(\n",
                        "     2                                               vocab_size = vocab_size,\n",
                        "     3                                               corpus_length = corpus_length,\n",
                        "     4                                           \n",
                        "     5                                               self = self,\n",
                        "     6                                               corpus = corpus,\n",
                        "     7                                           \n",
                        "     8                                               special_tokens = special_tokens,\n",
                        "     9                                               save = save,\n",
                        "    10                                               save_step = save_step,\n",
                        "    11                                               truncate_best_size = truncate_best_size,\n",
                        "    12                                               progress_bar = progress_bar,\n",
                        "    13                                               resume_merges = resume_merges,\n",
                        "    14                                               parallel = parallel,\n",
                        "    15                                           \n",
                        "    16                                               \n",
                        "    17                                           ):\n",
                        "    18                                           \n",
                        "    19                                           \n",
                        "    20         1          4.0      4.0      0.0      chunksize = int(corpus_length/self.cpu_count)\n",
                        "    21                                           \n",
                        "    22         1      39416.0  39416.0      0.0      corpus_chunks = [\"\".join(self.corpus.train[i*chunksize:i*chunksize+chunksize]) for i in range(0,self.cpu_count)]\n",
                        "    23                                           \n",
                        "    24         1        255.0    255.0      0.0      with Parallel(n_jobs=self.cpu_count, require='sharedmem') as parallel_pool:\n",
                        "    25         1        227.0    227.0      0.0          print(\"Computing in parallel\")\n",
                        "    26         1         71.0     71.0      0.0          print(\"Normalize and jobs data...\")\n",
                        "    27         1          4.0      4.0      0.0          start = time.time()\n",
                        "    28         1   11306082.0 11306082.0      4.5          jobs_data = parallel_pool(delayed(pre_process)(chunk,self.normalizer.normalize) for chunk in corpus_chunks)\n",
                        "    29                                           \n",
                        "    30         1      12858.0  12858.0      0.0          pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                        "    31         1        193.0    193.0      0.0          best_pair, best_pair_len = pair_len_global.most_common(1)[0]\n",
                        "    32                                                   \n",
                        "    33         1          4.0      4.0      0.0          merges = [\" \".join(best_pair)]\n",
                        "    34         1        192.0    192.0      0.0          print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "    35                                           \n",
                        "    36         1         34.0     34.0      0.0          print(\"Build alphabet...\")\n",
                        "    37         1          2.0      2.0      0.0          start = time.time()\n",
                        "    38         1          8.0      8.0      0.0          alphabet = Counter()\n",
                        "    39      2316       4722.0      2.0      0.0          for (l,r),v in pair_len_global.items():\n",
                        "    40      2315       4196.0      1.8      0.0              alphabet[l] += v\n",
                        "    41                                                   # In extreme cases, right characters of pairs might not be left characters. If there are such chars, they're added with freq 1\n",
                        "    42         1        635.0    635.0      0.0          left_out_chars = {r for l,r in pair_len_global.keys()}-alphabet.keys()\n",
                        "    43         1          3.0      3.0      0.0          if len(left_out_chars)>0:\n",
                        "    44                                                       print(f\"Adding characters: {left_out_chars}\")\n",
                        "    45                                                       for char in left_out_chars:\n",
                        "    46                                                           alphabet[char] += 1\n",
                        "    47         1        185.0    185.0      0.0          print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "    48                                           \n",
                        "    49         1          3.0      3.0      0.0          alpha_len = len(alphabet)\n",
                        "    50         1          2.0      2.0      0.0          special_tokens_len = 0 if special_tokens == None else len(special_tokens)\n",
                        "    51                                                   \n",
                        "    52         1         30.0     30.0      0.0          print(f\"Alphabet Size: {alpha_len}\")\n",
                        "    53         1         28.0     28.0      0.0          print(f\"Special Tokens Size: {special_tokens_len}\")\n",
                        "    54                                           \n",
                        "    55                                                   \n",
                        "    56         1          2.0      2.0      0.0          if vocab_size<0:\n",
                        "    57                                                       voc_final_length = alpha_len + abs(vocab_size) + special_tokens_len\n",
                        "    58                                                   else:\n",
                        "    59         1          1.0      1.0      0.0              voc_final_length = vocab_size\n",
                        "    60                                           \n",
                        "    61         1          2.0      2.0      0.0          delta_voc = voc_final_length - alpha_len - special_tokens_len\n",
                        "    62                                           \n",
                        "    63         1         28.0     28.0      0.0          print(f\"Terms to compute: {delta_voc}\\n\")\n",
                        "    64                                           \n",
                        "    65         1         25.0     25.0      0.0          print(\"Enter loop\")\n",
                        "    66                                           \n",
                        "    67                                           \n",
                        "    68                                           \n",
                        "    69                                           \n",
                        "    70         1      31925.0  31925.0      0.0          t = trange(delta_voc, disable = not progress_bar)\n",
                        "    71      1875    3141965.0   1675.7      1.3          for _ in t:\n",
                        "    72      1874    2318624.0   1237.3      0.9              t.set_description(f\"Pair: {best_pair}, {best_pair_len}\")\n",
                        "    73      1874     367271.0    196.0      0.1              t.refresh()\n",
                        "    74                                           \n",
                        "    75                                                       # print(f\"{_+1+alpha_len+special_tokens_len}/{voc_final_length}: {best_pair} {best_pair_len}...\")\n",
                        "    76                                                       # start = time.time()\n",
                        "    77      1874   95234961.0  50819.1     38.3              jobs_data = parallel_pool(delayed(process_best_pair)(job_data, best_pair) for job_data in jobs_data)\n",
                        "    78                                                       \n",
                        "    79      1874       6926.0      3.7      0.0              if truncate_best_size==None:\n",
                        "    80                                                           # pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                        "    81                                                               \n",
                        "    82                                                           # delta_pair_len = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                        "    83                                                           # pair_len_global.update(delta_pair_len)\n",
                        "    84                                           \n",
                        "    85      9370     202689.0     21.6      0.1                  for a,b, pair_len_delta in jobs_data:\n",
                        "    86      7496    4278167.0    570.7      1.7                      pair_len_global.update(pair_len_delta)\n",
                        "    87                                                           \n",
                        "    88                                                   #     else:\n",
                        "    89                                                   #         top_counter = [Counter(dict(i[-1].most_common(truncate_best_size))) for i in jobs_data]\n",
                        "    90                                                   #         pair_len_global = reduce(operator.add, top_counter)\n",
                        "    91                                                       # best_pair, best_pair_len = pair_len_global.most_common(1)[0]\n",
                        "    92      1874  127726550.0  68157.2     51.3              best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                        "    93                                           \n",
                        "    94                                           \n",
                        "    95      1874      13483.0      7.2      0.0              merges.append(\" \".join(best_pair))\n",
                        "    96                                                       # print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "    97                                           \n",
                        "    98         1         56.0     56.0      0.0          print(\"Compute freq...\")\n",
                        "    99         1          4.0      4.0      0.0          start = time.time()\n",
                        "   100         1    4045718.0 4045718.0      1.6          freqs = parallel_pool(delayed(compute_freq)(job_data[0]) for job_data in jobs_data)\n",
                        "   101         1      14753.0  14753.0      0.0          freq = reduce(operator.add, freqs)\n",
                        "   102         1       1806.0   1806.0      0.0          print(f\"... computed in {time.time()-start} secs.\\n\")"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "# most_common\n",
                "%lprun -f test test()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 10.37221622467041 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.013827085494995117 secs.\n",
                        "\n",
                        "Alphabet Size: 121\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 3874\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, max=3874.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "eccba8e5fc5e4cc29f960924d0f8a1cb"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "*** KeyboardInterrupt exception caught in code being profiled."
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Timer unit: 1e-06 s\n",
                        "\n",
                        "Total time: 268.574 s\n",
                        "File: <ipython-input-13-f8e5f9cef15b>\n",
                        "Function: test at line 1\n",
                        "\n",
                        "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
                        "==============================================================\n",
                        "     1                                           def test(\n",
                        "     2                                               vocab_size = vocab_size,\n",
                        "     3                                               corpus_length = corpus_length,\n",
                        "     4                                           \n",
                        "     5                                               self = self,\n",
                        "     6                                               corpus = corpus,\n",
                        "     7                                           \n",
                        "     8                                               special_tokens = special_tokens,\n",
                        "     9                                               save = save,\n",
                        "    10                                               save_step = save_step,\n",
                        "    11                                               truncate_best_size = truncate_best_size,\n",
                        "    12                                               progress_bar = progress_bar,\n",
                        "    13                                               resume_merges = resume_merges,\n",
                        "    14                                               parallel = parallel,\n",
                        "    15                                           \n",
                        "    16                                               \n",
                        "    17                                           ):\n",
                        "    18                                           \n",
                        "    19                                           \n",
                        "    20         1          6.0      6.0      0.0      chunksize = int(corpus_length/self.cpu_count)\n",
                        "    21                                           \n",
                        "    22         1      75571.0  75571.0      0.0      corpus_chunks = [\"\".join(self.corpus.train[i*chunksize:i*chunksize+chunksize]) for i in range(0,self.cpu_count)]\n",
                        "    23                                           \n",
                        "    24         1        328.0    328.0      0.0      with Parallel(n_jobs=self.cpu_count, require='sharedmem') as parallel_pool:\n",
                        "    25         1        231.0    231.0      0.0          print(\"Computing in parallel\")\n",
                        "    26         1         63.0     63.0      0.0          print(\"Normalize and jobs data...\")\n",
                        "    27         1          9.0      9.0      0.0          start = time.time()\n",
                        "    28         1   10362058.0 10362058.0      3.9          jobs_data = parallel_pool(delayed(pre_process)(chunk,self.normalizer.normalize) for chunk in corpus_chunks)\n",
                        "    29                                           \n",
                        "    30         1      10007.0  10007.0      0.0          pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                        "    31         1        128.0    128.0      0.0          best_pair, best_pair_len = pair_len_global.most_common(1)[0]\n",
                        "    32                                                   \n",
                        "    33         1          2.0      2.0      0.0          merges = [\" \".join(best_pair)]\n",
                        "    34         1        507.0    507.0      0.0          print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "    35                                           \n",
                        "    36         1         28.0     28.0      0.0          print(\"Build alphabet...\")\n",
                        "    37         1          2.0      2.0      0.0          start = time.time()\n",
                        "    38         1          5.0      5.0      0.0          alphabet = Counter()\n",
                        "    39      2316       2958.0      1.3      0.0          for (l,r),v in pair_len_global.items():\n",
                        "    40      2315       3661.0      1.6      0.0              alphabet[l] += v\n",
                        "    41                                                   # In extreme cases, right characters of pairs might not be left characters. If there are such chars, they're added with freq 1\n",
                        "    42         1        445.0    445.0      0.0          left_out_chars = {r for l,r in pair_len_global.keys()}-alphabet.keys()\n",
                        "    43         1          2.0      2.0      0.0          if len(left_out_chars)>0:\n",
                        "    44                                                       print(f\"Adding characters: {left_out_chars}\")\n",
                        "    45                                                       for char in left_out_chars:\n",
                        "    46                                                           alphabet[char] += 1\n",
                        "    47         1        185.0    185.0      0.0          print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "    48                                           \n",
                        "    49         1          2.0      2.0      0.0          alpha_len = len(alphabet)\n",
                        "    50         1          2.0      2.0      0.0          special_tokens_len = 0 if special_tokens == None else len(special_tokens)\n",
                        "    51                                                   \n",
                        "    52         1         26.0     26.0      0.0          print(f\"Alphabet Size: {alpha_len}\")\n",
                        "    53         1         23.0     23.0      0.0          print(f\"Special Tokens Size: {special_tokens_len}\")\n",
                        "    54                                           \n",
                        "    55                                                   \n",
                        "    56         1          1.0      1.0      0.0          if vocab_size<0:\n",
                        "    57                                                       voc_final_length = alpha_len + abs(vocab_size) + special_tokens_len\n",
                        "    58                                                   else:\n",
                        "    59         1          2.0      2.0      0.0              voc_final_length = vocab_size\n",
                        "    60                                           \n",
                        "    61         1          1.0      1.0      0.0          delta_voc = voc_final_length - alpha_len - special_tokens_len\n",
                        "    62                                           \n",
                        "    63         1         22.0     22.0      0.0          print(f\"Terms to compute: {delta_voc}\\n\")\n",
                        "    64                                           \n",
                        "    65         1         20.0     20.0      0.0          print(\"Enter loop\")\n",
                        "    66                                           \n",
                        "    67                                           \n",
                        "    68                                           \n",
                        "    69                                           \n",
                        "    70         1      31760.0  31760.0      0.0          t = trange(delta_voc, disable = not progress_bar)\n",
                        "    71      2103    3603185.0   1713.4      1.3          for _ in t:\n",
                        "    72      2103    2514527.0   1195.7      0.9              t.set_description(f\"Pair: {best_pair}, {best_pair_len}\")\n",
                        "    73      2103     420802.0    200.1      0.2              t.refresh()\n",
                        "    74                                           \n",
                        "    75                                                       # print(f\"{_+1+alpha_len+special_tokens_len}/{voc_final_length}: {best_pair} {best_pair_len}...\")\n",
                        "    76                                                       # start = time.time()\n",
                        "    77      2103   89373094.0  42497.9     33.3              jobs_data = parallel_pool(delayed(process_best_pair)(job_data, best_pair) for job_data in jobs_data)\n",
                        "    78                                                       \n",
                        "    79      2102       7378.0      3.5      0.0              if truncate_best_size==None:\n",
                        "    80                                                           # pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                        "    81                                                               \n",
                        "    82                                                           # delta_pair_len = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                        "    83                                                           # pair_len_global.update(delta_pair_len)\n",
                        "    84                                           \n",
                        "    85     10510     211888.0     20.2      0.1                  for a,b, pair_len_delta in jobs_data:\n",
                        "    86      8408    4612868.0    548.6      1.7                      pair_len_global.update(pair_len_delta)\n",
                        "    87                                                           \n",
                        "    88                                                   #     else:\n",
                        "    89                                                   #         top_counter = [Counter(dict(i[-1].most_common(truncate_best_size))) for i in jobs_data]\n",
                        "    90                                                   #         pair_len_global = reduce(operator.add, top_counter)\n",
                        "    91      2102  157328784.0  74847.2     58.6              best_pair, best_pair_len = pair_len_global.most_common(1)[0]\n",
                        "    92                                                       # best_pair, best_pair_len = max(pair_len_global.items(), key=operator.itemgetter(1))\n",
                        "    93                                           \n",
                        "    94                                           \n",
                        "    95      2102      13349.0      6.4      0.0              merges.append(\" \".join(best_pair))\n",
                        "    96                                                       # print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "    97                                           \n",
                        "    98                                                   print(\"Compute freq...\")\n",
                        "    99                                                   start = time.time()\n",
                        "   100                                                   freqs = parallel_pool(delayed(compute_freq)(job_data[0]) for job_data in jobs_data)\n",
                        "   101                                                   freq = reduce(operator.add, freqs)\n",
                        "   102         1         13.0     13.0      0.0          print(f\"... computed in {time.time()-start} secs.\\n\")"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit"
        },
        "interpreter": {
            "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}