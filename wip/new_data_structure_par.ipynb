{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import os\n",
                "os.chdir(\"../\")\n",
                "\n",
                "from tqdm.notebook import tqdm, trange\n",
                "from pyinstrument import Profiler\n",
                "from joblib import Parallel, delayed, parallel_backend\n",
                "\n",
                "import semiolog as slg\n",
                "\n",
                "semiotic = slg.Cenematic(\"en_bnc\",requested_cpu=4)\n",
                "\n",
                "from collections import Counter, defaultdict\n",
                "from functools import reduce\n",
                "import operator\n",
                "import time"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Warning: models/en_bnc/vocabulary/merges.txt does not exist.\n",
                        "Vocabulary will not be loaded from file.\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "def pre_process(corpus_chunk, normalizer):\n",
                "    # Normalize\n",
                "    chain_zip = normalizer(corpus_chunk)\n",
                "    # Build list of pairs\n",
                "    chain_zip = list(zip(chain_zip,chain_zip[1:]))\n",
                "    # Create a lookup table of all the positions where a pair appears in a corpus\n",
                "    pair_pos = defaultdict(set)\n",
                "    for i,k in list(enumerate(chain_zip)):\n",
                "        pair_pos[k].add(i)\n",
                "    # From the previous lookup table, create another lookup table of the frequency of each pair (given by the size of the set of its positions)\n",
                "    pair_len = Counter()\n",
                "    for k,pos in pair_pos.items():\n",
                "        pair_len[k] = len(pos)\n",
                "    \n",
                "    return (chain_zip, pair_pos, pair_len)\n",
                "\n",
                "\n",
                "def process_best_pair(job_data, best_pair):\n",
                "    chain_zip, pair_pos, pair_len = job_data\n",
                "    chain_zip_len = len(chain_zip)\n",
                "    \n",
                "    for i in pair_pos[best_pair]:\n",
                "        # Skip iteration if position corresponds to a modified set of positions during the iteration. This can happen if there is overlap of pairs, such as \"000\", where (\"0\",\"0\") has itself as right pair. Note that, due to unordered implementation of sets, this entails a lack of systematicity in overlapping cases: \"000\" can be counted randomly as (\"00\",\"0\") or (\"0\",\"00\").\n",
                "        # TODO: Investigate the cost of ordering sets. In which case, the following \"if\" condition might only be needed for right pairs.\n",
                "        if chain_zip[i]!=best_pair:\n",
                "            continue\n",
                "        ## merge best pair with left unit\n",
                "        left_pair_i = i-1\n",
                "        while left_pair_i>=0 and chain_zip[left_pair_i] == None: # if left pair is within chain limits but empty (= None) because already merged previously, shift to the left\n",
                "            left_pair_i -= 1\n",
                "        if left_pair_i>-1: # proceed only if a left pair was found on the left\n",
                "            # Remove from left pair positions, the current position (of the pair to be merged)\n",
                "            left_pair = chain_zip[left_pair_i]\n",
                "            # Skip update of left_pair position set if left_pair = best_pair, to avoid modification of iterating set. This can happen if there is overlap of pairs. No consequences on final result (right?) since right after the loop, the key corresponding to the best pair is deleted, and chain_zip is indeed updated so the problematic cases can be captured at the beginning of the loop.\n",
                "            if left_pair != best_pair:\n",
                "                left_pair_pos = pair_pos[left_pair]\n",
                "                left_pair_pos.discard(left_pair_i)\n",
                "            new_pair = (left_pair[0],\"\".join(best_pair)) # construct new left pair\n",
                "            pair_pos[new_pair].add(left_pair_i) # add new pair (if non existing) and its position to the pair_pos lookup table\n",
                "            # update the counts in the pair_len lookuptable\n",
                "            pair_len[left_pair] -= 1\n",
                "            pair_len[new_pair] += 1\n",
                "            # update the list of pairs\n",
                "            chain_zip[left_pair_i] = new_pair\n",
                "\n",
                "        ## merge best pair with right unit\n",
                "        right_pair_i = i+1\n",
                "        while right_pair_i<chain_zip_len and chain_zip[right_pair_i] == None: # if right pair is within chain limits but empty (= None) because already merged previously, shift to the right\n",
                "            right_pair_i += 1\n",
                "        if right_pair_i<chain_zip_len: # proceed only if a left pair was found on the right\n",
                "            # Remove from right pair positions, the current position (of the pair to be merged)\n",
                "            right_pair = chain_zip[right_pair_i]\n",
                "            if right_pair != best_pair:\n",
                "                right_pair_pos = pair_pos[right_pair]\n",
                "                right_pair_pos.discard(right_pair_i)\n",
                "            new_pair = (\"\".join(best_pair), right_pair[1]) # construct new right pair\n",
                "            pair_pos[new_pair].add(right_pair_i) # add new pair (if non existing) and its position to the pair_pos lookup table\n",
                "            # update the counts in the pair_len lookuptable\n",
                "            pair_len[right_pair] -= 1\n",
                "            pair_len[new_pair] += 1\n",
                "            # update the list of pairs\n",
                "            chain_zip[right_pair_i] = new_pair\n",
                "\n",
                "        # Empty best pair position in list of pairs\n",
                "        chain_zip[i] = None\n",
                "\n",
                "    # Remove best pair from lookuptables\n",
                "    del pair_pos[best_pair]\n",
                "    del pair_len[best_pair]\n",
                "\n",
                "    return (chain_zip, pair_pos, pair_len)\n",
                "\n",
                "def compute_freq(chain_zip):\n",
                "    # TODO: add the last unit to the decoupling\n",
                "    freq = [pair[0] for pair in chain_zip if pair != None]\n",
                "    if chain_zip[-1]!=None: \n",
                "        freq.append(chain_zip[-1][-1])\n",
                "    freq = Counter(freq)\n",
                "    return freq"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "delta_voc = 10"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "requested_cpu = 4 # self.cpu_count\n",
                "chunksize = int(semiotic.corpus.train_len/5000)\n",
                "\n",
                "corpus_chunks = [\"\".join(semiotic.corpus.train[i*chunksize:i*chunksize+chunksize]) for i in range(0,requested_cpu)]\n",
                "\n",
                "normalizer = semiotic.vocab.normalizer.normalize"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "with Parallel(n_jobs=requested_cpu) as parallel_pool:\n",
                "    print(\"Normalize and jobs data...\")\n",
                "    start = time.time()\n",
                "    jobs_data = parallel_pool(delayed(pre_process)(chunk,normalizer) for chunk in corpus_chunks)\n",
                "\n",
                "    pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                "    \n",
                "    best_pair = pair_len_global.most_common(1)[0][0]\n",
                "    \n",
                "    merges = [best_pair]\n",
                "    print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "    print(\"Build alphabet...\")\n",
                "    start = time.time()\n",
                "    alphabet = {l for l,r in pair_len_global.keys()}\n",
                "    alphabet = alphabet.union({r for l,r in pair_len_global.keys()})\n",
                "    print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "    print(\"Enter loop\")\n",
                "    for _ in range(delta_voc):\n",
                "\n",
                "        print(f\"{_+1}/{delta_voc}: {best_pair}...\")\n",
                "        start = time.time()\n",
                "        jobs_data = parallel_pool(delayed(process_best_pair)(job_data, best_pair) for job_data in jobs_data)\n",
                "\n",
                "        pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                "\n",
                "        best_pair = pair_len_global.most_common(1)[0][0]\n",
                "\n",
                "        merges.append(best_pair)\n",
                "        print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "    \n",
                "    print(\"Compute freq...\")\n",
                "    start = time.time()\n",
                "    freqs = parallel_pool(delayed(compute_freq)(job_data[0]) for job_data in jobs_data)\n",
                "    freq = reduce(operator.add, freqs)\n",
                "    print(f\"... computed in {time.time()-start} secs.\\n\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Normalize and jobs data...\n",
                        "... computed in 3.850148916244507 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.00017595291137695312 secs.\n",
                        "\n",
                        "Build alphabet 2...\n",
                        "... computed in 0.005192995071411133 secs.\n",
                        "\n",
                        "Enter loop\n",
                        "1/10: ('t', 'h')...\n",
                        "... computed in 0.24786806106567383 secs.\n",
                        "\n",
                        "2/10: ('i', 'n')...\n",
                        "... computed in 0.2697458267211914 secs.\n",
                        "\n",
                        "3/10: ('th', 'e')...\n",
                        "... computed in 0.2520318031311035 secs.\n",
                        "\n",
                        "4/10: ('r', 'e')...\n",
                        "... computed in 0.18147683143615723 secs.\n",
                        "\n",
                        "5/10: ('a', 'n')...\n",
                        "... computed in 0.15643906593322754 secs.\n",
                        "\n",
                        "6/10: ('o', 'n')...\n",
                        "... computed in 0.14203214645385742 secs.\n",
                        "\n",
                        "7/10: ('e', 'r')...\n",
                        "... computed in 0.19243907928466797 secs.\n",
                        "\n",
                        "8/10: ('a', 't')...\n",
                        "... computed in 0.17896604537963867 secs.\n",
                        "\n",
                        "9/10: ('e', 'n')...\n",
                        "... computed in 0.7634010314941406 secs.\n",
                        "\n",
                        "10/10: ('t', 'o')...\n",
                        "... computed in 0.1505880355834961 secs.\n",
                        "\n",
                        "Compute freq...\n",
                        "... computed in 0.0633089542388916 secs.\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "alphabet2 == alphabet"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 6
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "freq.most_common(20)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[('e', 4248845),\n",
                            " ('a', 3255073),\n",
                            " ('o', 3085943),\n",
                            " ('s', 2625840),\n",
                            " ('t', 2575704),\n",
                            " ('r', 2500714),\n",
                            " ('i', 2169901),\n",
                            " ('n', 2074927),\n",
                            " ('l', 1684089),\n",
                            " ('d', 1561717),\n",
                            " ('c', 1266467),\n",
                            " ('u', 1140697),\n",
                            " ('m', 1006417),\n",
                            " ('h', 961428),\n",
                            " ('f', 903832),\n",
                            " ('p', 835466),\n",
                            " ('g', 810828),\n",
                            " ('w', 780760),\n",
                            " ('in', 778858),\n",
                            " ('the', 759571)]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 6
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "merges"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[('t', 'h'), ('i', 'n'), ('th', 'e'), ('a', 'n')]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 8
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit"
        },
        "interpreter": {
            "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}