{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import os\n",
                "os.chdir(\"../\")\n",
                "\n",
                "from tqdm.notebook import tqdm, trange\n",
                "from pyinstrument import Profiler\n",
                "from joblib import Parallel, delayed, parallel_backend\n",
                "\n",
                "import semiolog as slg\n",
                "\n",
                "semiotic = slg.Cenematic(\"en_bnc\",requested_cpu=4)\n",
                "\n",
                "from collections import Counter, defaultdict\n",
                "from functools import reduce\n",
                "import operator\n",
                "import time"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Warning: models/en_bnc/vocabulary/merges.txt does not exist.\n",
                        "Vocabulary will not be loaded from file.\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "def pre_process(corpus_chunk, normalizer):\n",
                "    # Normalize\n",
                "    chain_zip = normalizer(corpus_chunk)\n",
                "    # Build list of pairs\n",
                "    chain_zip = list(zip(chain_zip,chain_zip[1:]))\n",
                "    # Create a lookup table of all the positions where a pair appears in a corpus\n",
                "    pair_pos = defaultdict(set)\n",
                "    for i,k in list(enumerate(chain_zip)):\n",
                "        pair_pos[k].add(i)\n",
                "    # From the previous lookup table, create another lookup table of the frequency of each pair (given by the size of the set of its positions)\n",
                "    pair_len = Counter()\n",
                "    for k,pos in pair_pos.items():\n",
                "        pair_len[k] = len(pos)\n",
                "    \n",
                "    return (chain_zip, pair_pos, pair_len)\n",
                "\n",
                "\n",
                "def process_best_pair(job_data, best_pair):\n",
                "    chain_zip, pair_pos, pair_len = job_data\n",
                "    chain_zip_len = len(chain_zip)\n",
                "\n",
                "    for i in pair_pos[best_pair]:\n",
                "        ## merge best pair with left unit\n",
                "        left_pair_i = i-1\n",
                "        while left_pair_i>=0 and chain_zip[left_pair_i] == None: # if left pair is within chain limits but empty (= None) because already merged previously, shift to the left\n",
                "            left_pair_i -= 1\n",
                "        if left_pair_i>-1: # proceed only if a left pair was found on the left\n",
                "            # Remove from left pair positions, the current position (of the pair to be merged)\n",
                "            left_pair = chain_zip[left_pair_i]\n",
                "            left_pair_pos = pair_pos[left_pair]\n",
                "            left_pair_pos.discard(left_pair_i)\n",
                "            new_pair = (left_pair[0],\"\".join(best_pair)) # construct new left pair\n",
                "            pair_pos[new_pair].add(left_pair_i) # add new pair (if non existing) and its position to the pair_pos lookup table\n",
                "            # update the counts in the pair_len lookuptable\n",
                "            pair_len[left_pair] -= 1\n",
                "            pair_len[new_pair] += 1\n",
                "            # update the list of pairs\n",
                "            chain_zip[left_pair_i] = new_pair\n",
                "\n",
                "        ## merge best pair with right unit\n",
                "        right_pair_i = i+1\n",
                "        while right_pair_i<chain_zip_len and chain_zip[right_pair_i] == None: # if right pair is within chain limits but empty (= None) because already merged previously, shift to the right\n",
                "            right_pair_i += 1\n",
                "        if right_pair_i<chain_zip_len: # proceed only if a left pair was found on the right\n",
                "            # Remove from right pair positions, the current position (of the pair to be merged)\n",
                "            right_pair = chain_zip[right_pair_i]\n",
                "            right_pair_pos = pair_pos[right_pair]\n",
                "            right_pair_pos.discard(right_pair_i)\n",
                "            new_pair = (\"\".join(best_pair), right_pair[1]) # construct new right pair\n",
                "            pair_pos[new_pair].add(right_pair_i) # add new pair (if non existing) and its position to the pair_pos lookup table\n",
                "            # update the counts in the pair_len lookuptable\n",
                "            pair_len[right_pair] -= 1\n",
                "            pair_len[new_pair] += 1\n",
                "            # update the list of pairs\n",
                "            chain_zip[right_pair_i] = new_pair\n",
                "\n",
                "        # Empty best pair position in list of pairs\n",
                "        chain_zip[i] = None\n",
                "\n",
                "    # Remove best pair from lookuptables\n",
                "    del pair_pos[best_pair]\n",
                "    del pair_len[best_pair]\n",
                "\n",
                "    return (chain_zip, pair_pos, pair_len)\n",
                "\n",
                "def compute_freq(chain_zip):\n",
                "    # TODO: add the last unit to the decoupling\n",
                "    freq = [pair[0] for pair in chain_zip if pair != None]\n",
                "    if chain_zip[-1]!=None: \n",
                "        freq.append(chain_zip[-1][-1])\n",
                "    freq = Counter(freq)\n",
                "    return freq"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "delta_voc = 3"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "requested_cpu = 4 # self.cpu_count\n",
                "chunksize = int(semiotic.corpus.train_len/40)\n",
                "\n",
                "corpus_chunks = [\"\".join(semiotic.corpus.train[i*chunksize:i*chunksize+chunksize]) for i in range(0,requested_cpu)]\n",
                "\n",
                "normalizer = semiotic.vocab.normalizer.normalize"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "with Parallel(n_jobs=requested_cpu) as parallel_pool:\n",
                "    print(\"Normalize and jobs data...\")\n",
                "    start = time.time()\n",
                "    jobs_data = parallel_pool(delayed(pre_process)(chunk,normalizer) for chunk in corpus_chunks)\n",
                "\n",
                "    pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                "    best_pair = pair_len_global.most_common(1)[0][0]\n",
                "    \n",
                "    merges = [best_pair]\n",
                "    print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "\n",
                "    print(\"Enter loop\")\n",
                "    for _ in range(delta_voc):\n",
                "\n",
                "        print(f\"{_+1}/{delta_voc}: {best_pair}...\")\n",
                "        start = time.time()\n",
                "        jobs_data = parallel_pool(delayed(process_best_pair)(job_data, best_pair) for job_data in jobs_data)\n",
                "\n",
                "        pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                "        best_pair = pair_len_global.most_common(1)[0][0]\n",
                "\n",
                "        merges.append(best_pair)\n",
                "        print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                "    \n",
                "    print(\"Compute freq...\")\n",
                "    start = time.time()\n",
                "    freqs = parallel_pool(delayed(compute_freq)(job_data[0]) for job_data in jobs_data)\n",
                "    freq = reduce(operator.add, freqs)\n",
                "    print(f\"... computed in {time.time()-start} secs.\\n\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Normalize and jobs data\n",
                        "... computed in 36.27325797080994 secs.\n",
                        "\n",
                        "Enter loop\n",
                        "0/3: ('t', 'h')...\n",
                        "0 reduce\n",
                        "... computed in 29.919811248779297 secs.\n",
                        "\n",
                        "1/3: ('i', 'n')...\n",
                        "1 reduce\n",
                        "... computed in 37.25240898132324 secs.\n",
                        "\n",
                        "2/3: ('th', 'e')...\n",
                        "2 reduce\n",
                        "... computed in 38.88198518753052 secs.\n",
                        "\n",
                        "Compute freq...\n",
                        "... computed in 16.694128274917603 secs.\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "freq.most_common(20)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[('e', 4248845),\n",
                            " ('a', 3255073),\n",
                            " ('o', 3085943),\n",
                            " ('s', 2625840),\n",
                            " ('t', 2575704),\n",
                            " ('r', 2500714),\n",
                            " ('i', 2169901),\n",
                            " ('n', 2074927),\n",
                            " ('l', 1684089),\n",
                            " ('d', 1561717),\n",
                            " ('c', 1266467),\n",
                            " ('u', 1140697),\n",
                            " ('m', 1006417),\n",
                            " ('h', 961428),\n",
                            " ('f', 903832),\n",
                            " ('p', 835466),\n",
                            " ('g', 810828),\n",
                            " ('w', 780760),\n",
                            " ('in', 778858),\n",
                            " ('the', 759571)]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 6
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "merges"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[('t', 'h'), ('i', 'n'), ('th', 'e'), ('a', 'n')]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 8
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit"
        },
        "interpreter": {
            "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}