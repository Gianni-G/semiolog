{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import os\n",
                "os.chdir(\"../\")\n",
                "\n",
                "import semiolog as slg"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "semiotic = slg.Cenematic(\"en_bnc\",requested_cpu=4)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Warning: models/en_bnc/vocabulary/merges.txt does not exist.\n",
                        "Vocabulary will not be loaded from file.\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "semiotic.vocab.build(vocab_size = -50, parallel=True, corpus_length=80000, save=True, save_step=10, truncate_best_lens=500)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 4.011059999465942 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.0005099773406982422 secs.\n",
                        "\n",
                        "Alphabet Size: 103\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 50\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "c963e644d8be4191838788af52ed833a"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Saving intermediate results...\n",
                        "... computed in 0.7170660495758057 secs.\n",
                        "Intermediate vocabulary saved to models/en_bnc/vocabulary/110\n",
                        "\n",
                        "Saving intermediate results...\n",
                        "... computed in 0.9167921543121338 secs.\n",
                        "Intermediate vocabulary saved to models/en_bnc/vocabulary/120\n",
                        "\n",
                        "Saving intermediate results...\n",
                        "... computed in 1.430049180984497 secs.\n",
                        "Intermediate vocabulary saved to models/en_bnc/vocabulary/130\n",
                        "\n",
                        "Saving intermediate results...\n",
                        "... computed in 1.067716121673584 secs.\n",
                        "Intermediate vocabulary saved to models/en_bnc/vocabulary/140\n",
                        "\n",
                        "Saving intermediate results...\n",
                        "... computed in 1.0785210132598877 secs.\n",
                        "Intermediate vocabulary saved to models/en_bnc/vocabulary/150\n",
                        "\n",
                        "\n",
                        "Compute freq...\n",
                        "... computed in 1.1700410842895508 secs.\n",
                        "\n",
                        "Vocabulary built\n",
                        "Vocabulary saved to models/en_bnc/vocabulary\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "%load_ext line_profiler"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "%lprun -f semiotic.vocab.build semiotic.vocab.build(vocab_size = 500, parallel=True, corpus_length=80000, save=False, save_step=10, truncate_best_lens=500)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Computing in parallel\n",
                        "Normalize and jobs data...\n",
                        "... computed in 4.66138482093811 secs.\n",
                        "\n",
                        "Build alphabet...\n",
                        "... computed in 0.02866983413696289 secs.\n",
                        "\n",
                        "Alphabet Size: 103\n",
                        "Special Tokens Size: 5\n",
                        "Terms to compute: 392\n",
                        "\n",
                        "Enter loop\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, max=392.0), HTML(value='')))"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "4a2ed782b3ef481fbdf329d90f07ce46"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "Compute freq...\n",
                        "... computed in 1.5186328887939453 secs.\n",
                        "\n",
                        "Vocabulary built\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Timer unit: 1e-06 s\n",
                        "\n",
                        "Total time: 93.7908 s\n",
                        "File: /Users/Gianni/semiolog/semiolog/vocabulary.py\n",
                        "Function: build at line 1108\n",
                        "\n",
                        "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
                        "==============================================================\n",
                        "  1108                                               def build(\n",
                        "  1109                                                   self,\n",
                        "  1110                                                   corpus = None,\n",
                        "  1111                                                   vocab_size = None,\n",
                        "  1112                                                   special_tokens = None,\n",
                        "  1113                                                   save = False,\n",
                        "  1114                                                   save_step = None,\n",
                        "  1115                                                   truncate_best_lens = None,\n",
                        "  1116                                                   progress_bar = True,\n",
                        "  1117                                                   resume_merges = False,\n",
                        "  1118                                                   parallel = True,\n",
                        "  1119                                                   corpus_length = None\n",
                        "  1120                                                   ):\n",
                        "  1121                                           \n",
                        "  1122         1          8.0      8.0      0.0          if corpus == None:\n",
                        "  1123         1          4.0      4.0      0.0              corpus = self.name\n",
                        "  1124                                                   \n",
                        "  1125         1          3.0      3.0      0.0          if vocab_size == None:\n",
                        "  1126                                                       vocab_size = self.config.size\n",
                        "  1127                                                   \n",
                        "  1128         1          3.0      3.0      0.0          if special_tokens == None:\n",
                        "  1129         1          4.0      4.0      0.0              special_tokens = self.config.special_tokens\n",
                        "  1130                                                   \n",
                        "  1131         1          3.0      3.0      0.0          if corpus_length == None:\n",
                        "  1132                                                       corpus_length = self.corpus.train_len\n",
                        "  1133                                                   \n",
                        "  1134         1          3.0      3.0      0.0          if save == True and save_step != None:\n",
                        "  1135                                                       saveQ = True\n",
                        "  1136                                                       \n",
                        "  1137                                                       if not isdir(self.path):\n",
                        "  1138                                                           makedirs(self.path)\n",
                        "  1139                                                           \n",
                        "  1140                                                   else:\n",
                        "  1141         1          2.0      2.0      0.0              saveQ = False\n",
                        "  1142                                           \n",
                        "  1143         1          3.0      3.0      0.0          def pre_process(corpus_chunk, normalizer):\n",
                        "  1144                                                       # Normalize\n",
                        "  1145                                                       chain_zip = normalizer(corpus_chunk)\n",
                        "  1146                                                       # Build list of pairs\n",
                        "  1147                                                       chain_zip = list(zip(chain_zip,chain_zip[1:]))\n",
                        "  1148                                                       # Create a lookup table of all the positions where a pair appears in a corpus\n",
                        "  1149                                                       pair_pos = defaultdict(set)\n",
                        "  1150                                                       for i,k in list(enumerate(chain_zip)):\n",
                        "  1151                                                           pair_pos[k].add(i)\n",
                        "  1152                                                       # From the previous lookup table, create another lookup table of the frequency of each pair (given by the size of the set of its positions)\n",
                        "  1153                                                       pair_len = Counter()\n",
                        "  1154                                                       for k,pos in pair_pos.items():\n",
                        "  1155                                                           pair_len[k] = len(pos)\n",
                        "  1156                                                       \n",
                        "  1157                                                       return (chain_zip, pair_pos, pair_len)\n",
                        "  1158                                           \n",
                        "  1159                                           \n",
                        "  1160         1          3.0      3.0      0.0          def process_best_pair(job_data, best_pair):\n",
                        "  1161                                                       chain_zip, pair_pos, pair_len = job_data\n",
                        "  1162                                                       chain_zip_len = len(chain_zip)\n",
                        "  1163                                           \n",
                        "  1164                                                       for i in pair_pos[best_pair]:\n",
                        "  1165                                                           # Skip iteration if position corresponds to a modified set of positions during the iteration. This can happen if there is overlap of pairs, such as \"000\", where (\"0\",\"0\") has itself as right pair. Note that, due to unordered implementation of sets, this entails a lack of systematicity in overlapping cases: \"000\" can be counted randomly as (\"00\",\"0\") or (\"0\",\"00\").\n",
                        "  1166                                                           # TODO: Investigate the cost of ordering sets. In which case, the following \"if\" condition might only be needed for right pairs.\n",
                        "  1167                                                           if chain_zip[i]!=best_pair:\n",
                        "  1168                                                               continue\n",
                        "  1169                                                           ## merge best pair with left unit\n",
                        "  1170                                                           left_pair_i = i-1\n",
                        "  1171                                                           while left_pair_i>=0 and chain_zip[left_pair_i] == None: # if left pair is within chain limits but empty (= None) because already merged previously, shift to the left\n",
                        "  1172                                                               left_pair_i -= 1\n",
                        "  1173                                                           if left_pair_i>-1: # proceed only if a left pair was found on the left\n",
                        "  1174                                                               # Remove from left pair positions, the current position (of the pair to be merged)\n",
                        "  1175                                                               left_pair = chain_zip[left_pair_i]\n",
                        "  1176                                                               # Skip update of left_pair position set if left_pair = best_pair, to avoid modification of iterating set. This can happen if there is overlap of pairs. No consequences on final result (right?) since right after the loop, the key corresponding to the best pair is deleted, and chain_zip is indeed updated so the problematic cases can be captured at the beginning of the loop.\n",
                        "  1177                                                               if left_pair != best_pair:\n",
                        "  1178                                                                   left_pair_pos = pair_pos[left_pair]\n",
                        "  1179                                                                   left_pair_pos.discard(left_pair_i)\n",
                        "  1180                                                               new_pair = (left_pair[0],\"\".join(best_pair)) # construct new left pair\n",
                        "  1181                                                               pair_pos[new_pair].add(left_pair_i) # add new pair (if non existing) and its position to the pair_pos lookup table\n",
                        "  1182                                                               # update the counts in the pair_len lookuptable\n",
                        "  1183                                                               pair_len[left_pair] -= 1\n",
                        "  1184                                                               pair_len[new_pair] += 1\n",
                        "  1185                                                               # update the list of pairs\n",
                        "  1186                                                               chain_zip[left_pair_i] = new_pair\n",
                        "  1187                                           \n",
                        "  1188                                                           ## merge best pair with right unit.\n",
                        "  1189                                                           # Code is symmetric to left_pair but on the right. Comments are omitted\n",
                        "  1190                                                           right_pair_i = i+1\n",
                        "  1191                                                           while right_pair_i<chain_zip_len and chain_zip[right_pair_i] == None:\n",
                        "  1192                                                               right_pair_i += 1\n",
                        "  1193                                                           if right_pair_i<chain_zip_len:\n",
                        "  1194                                                               right_pair = chain_zip[right_pair_i]\n",
                        "  1195                                                               if right_pair != best_pair:\n",
                        "  1196                                                                   right_pair_pos = pair_pos[right_pair]\n",
                        "  1197                                                                   right_pair_pos.discard(right_pair_i)\n",
                        "  1198                                                               new_pair = (\"\".join(best_pair), right_pair[1])\n",
                        "  1199                                                               pair_pos[new_pair].add(right_pair_i)\n",
                        "  1200                                                               pair_len[right_pair] -= 1\n",
                        "  1201                                                               pair_len[new_pair] += 1\n",
                        "  1202                                                               chain_zip[right_pair_i] = new_pair\n",
                        "  1203                                           \n",
                        "  1204                                                           # Empty best pair position in list of pairs\n",
                        "  1205                                                           chain_zip[i] = None\n",
                        "  1206                                           \n",
                        "  1207                                                       # Remove best pair from lookuptables\n",
                        "  1208                                                       del pair_pos[best_pair]\n",
                        "  1209                                                       del pair_len[best_pair]\n",
                        "  1210                                           \n",
                        "  1211                                                       return (chain_zip, pair_pos, pair_len)\n",
                        "  1212                                           \n",
                        "  1213         1          3.0      3.0      0.0          def compute_freq(chain_zip):\n",
                        "  1214                                                       # TODO: add the last unit to the decoupling\n",
                        "  1215                                                       freq = [pair[0] for pair in chain_zip if pair != None]\n",
                        "  1216                                                       if chain_zip[-1]!=None: \n",
                        "  1217                                                           freq.append(chain_zip[-1][-1])\n",
                        "  1218                                                       freq = Counter(freq)\n",
                        "  1219                                                       return freq\n",
                        "  1220                                                   \n",
                        "  1221                                           \n",
                        "  1222                                                   # TODO: Include feature computing delta voc as difference from vocab_size and alphabet\n",
                        "  1223         1          3.0      3.0      0.0          delta_voc = vocab_size\n",
                        "  1224                                           \n",
                        "  1225         1          3.0      3.0      0.0          if parallel:\n",
                        "  1226         1          5.0      5.0      0.0              chunksize = int(corpus_length/self.cpu_count)\n",
                        "  1227                                           \n",
                        "  1228         1      13496.0  13496.0      0.0              corpus_chunks = [\"\".join(self.corpus.train[i*chunksize:i*chunksize+chunksize]) for i in range(0,self.cpu_count)]\n",
                        "  1229                                           \n",
                        "  1230         1        250.0    250.0      0.0              with Parallel(n_jobs=self.cpu_count, require='sharedmem') as parallel_pool:\n",
                        "  1231         1        142.0    142.0      0.0                  print(\"Computing in parallel\")\n",
                        "  1232         1         43.0     43.0      0.0                  print(\"Normalize and jobs data...\")\n",
                        "  1233         1          5.0      5.0      0.0                  start = time.time()\n",
                        "  1234         1    4654322.0 4654322.0      5.0                  jobs_data = parallel_pool(delayed(pre_process)(chunk,self.normalizer.normalize) for chunk in corpus_chunks)\n",
                        "  1235                                           \n",
                        "  1236         1       6861.0   6861.0      0.0                  pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                        "  1237         1        109.0    109.0      0.0                  best_pair, best_pair_len = pair_len_global.most_common(1)[0]\n",
                        "  1238                                                           \n",
                        "  1239         1          7.0      7.0      0.0                  merges = [\" \".join(best_pair)]\n",
                        "  1240         1        215.0    215.0      0.0                  print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "  1241                                           \n",
                        "  1242         1         24.0     24.0      0.0                  print(\"Build alphabet...\")\n",
                        "  1243         1          4.0      4.0      0.0                  start = time.time()\n",
                        "  1244         1          7.0      7.0      0.0                  alphabet = Counter()\n",
                        "  1245      1935       6148.0      3.2      0.0                  for (l,r),v in pair_len_global.items():\n",
                        "  1246      1934       6540.0      3.4      0.0                      alphabet[l] += v\n",
                        "  1247                                                           # In extreme cases, right characters of pairs might not be left characters. If there are such chars, they're added with freq 1\n",
                        "  1248         1        348.0    348.0      0.0                  left_out_chars = {r for l,r in pair_len_global.keys()}-alphabet.keys()\n",
                        "  1249         1          4.0      4.0      0.0                  if len(left_out_chars)>0:\n",
                        "  1250                                                               print(f\"Adding characters: {left_out_chars}\")\n",
                        "  1251                                                               for char in left_out_chars:\n",
                        "  1252                                                                   alphabet[char] += 1\n",
                        "  1253         1        140.0    140.0      0.0                  print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "  1254                                           \n",
                        "  1255         1          4.0      4.0      0.0                  alpha_len = len(alphabet)\n",
                        "  1256         1          3.0      3.0      0.0                  special_tokens_len = 0 if special_tokens == None else len(special_tokens)\n",
                        "  1257                                                           \n",
                        "  1258         1         26.0     26.0      0.0                  print(f\"Alphabet Size: {alpha_len}\")\n",
                        "  1259         1         23.0     23.0      0.0                  print(f\"Special Tokens Size: {special_tokens_len}\")\n",
                        "  1260                                           \n",
                        "  1261                                                           \n",
                        "  1262         1         14.0     14.0      0.0                  if vocab_size<0:\n",
                        "  1263                                                               voc_final_length = alpha_len + abs(vocab_size) + special_tokens_len\n",
                        "  1264                                                           else:\n",
                        "  1265         1          3.0      3.0      0.0                      voc_final_length = vocab_size\n",
                        "  1266                                           \n",
                        "  1267         1          3.0      3.0      0.0                  delta_voc = voc_final_length - alpha_len - special_tokens_len\n",
                        "  1268                                           \n",
                        "  1269         1         24.0     24.0      0.0                  print(f\"Terms to compute: {delta_voc}\\n\")\n",
                        "  1270                                           \n",
                        "  1271         1         21.0     21.0      0.0                  print(\"Enter loop\")\n",
                        "  1272                                           \n",
                        "  1273                                                           # for _ in trange(delta_voc):\n",
                        "  1274         1      36290.0  36290.0      0.0                  t = trange(delta_voc, disable = not progress_bar)\n",
                        "  1275       393     648798.0   1650.9      0.7                  for _ in t:\n",
                        "  1276       392     295559.0    754.0      0.3                      t.set_description(f\"Pair: {best_pair}, {best_pair_len}\")\n",
                        "  1277       392      51773.0    132.1      0.1                      t.refresh()\n",
                        "  1278                                           \n",
                        "  1279                                                               # print(f\"{_+1+alpha_len+special_tokens_len}/{voc_final_length}: {best_pair} {best_pair_len}...\")\n",
                        "  1280                                                               # start = time.time()\n",
                        "  1281       392   15569876.0  39719.1     16.6                      jobs_data = parallel_pool(delayed(process_best_pair)(job_data, best_pair) for job_data in jobs_data)\n",
                        "  1282                                           \n",
                        "  1283       392       1815.0      4.6      0.0                      if truncate_best_lens==None:\n",
                        "  1284                                                                   pair_len_global = reduce(operator.add,[i[-1] for i in jobs_data])\n",
                        "  1285                                                               else:\n",
                        "  1286       392   70953561.0 181004.0     75.7                          pair_len_global = reduce(operator.add,[Counter(dict(i[-1].most_common(truncate_best_lens))) for i in jobs_data])\n",
                        "  1287       392      20079.0     51.2      0.0                      best_pair, best_pair_len = pair_len_global.most_common(1)[0]\n",
                        "  1288                                           \n",
                        "  1289                                           \n",
                        "  1290       392       2143.0      5.5      0.0                      merges.append(\" \".join(best_pair))\n",
                        "  1291                                                               # print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "  1292                                                           \n",
                        "  1293       392       1261.0      3.2      0.0                      if saveQ == True:\n",
                        "  1294                                                                   voc_partial_len = alpha_len + special_tokens_len + _ + 1\n",
                        "  1295                                                                   if voc_partial_len % save_step == 0 and voc_partial_len != voc_final_length:\n",
                        "  1296                                           \n",
                        "  1297                                                                       print(\"Saving intermediate results...\")\n",
                        "  1298                                                                       start = time.time()\n",
                        "  1299                                                                       freqs = parallel_pool(delayed(compute_freq)(job_data[0]) for job_data in jobs_data)\n",
                        "  1300                                                                       freq = reduce(operator.add, freqs)\n",
                        "  1301                                           \n",
                        "  1302                                                                       vocabulary = freq.most_common()\n",
                        "  1303                                                                       \n",
                        "  1304                                                                       if special_tokens != None:\n",
                        "  1305                                                                           vocabulary = vocabulary + [(token,0) for token in special_tokens]\n",
                        "  1306                                                                       \n",
                        "  1307                                                                       self.merges = merges\n",
                        "  1308                                                                       self.encode = {k:i for i,(k,v) in enumerate(vocabulary)}\n",
                        "  1309                                                                       self.freq = dict(vocabulary)\n",
                        "  1310                                                                       self.alpha = dict(alphabet.most_common())\n",
                        "  1311                                                                       step_path = self.path / str(voc_partial_len)\n",
                        "  1312                                                                       self.save(step_path)\n",
                        "  1313                                                                       print(f\"... computed in {time.time()-start} secs.\")\n",
                        "  1314                                                                       print(f\"Intermediate vocabulary saved to {step_path}\\n\")\n",
                        "  1315                                           \n",
                        "  1316         1         33.0     33.0      0.0                  print(\"Compute freq...\")\n",
                        "  1317         1          4.0      4.0      0.0                  start = time.time()\n",
                        "  1318         1    1515876.0 1515876.0      1.6                  freqs = parallel_pool(delayed(compute_freq)(job_data[0]) for job_data in jobs_data)\n",
                        "  1319         1       2728.0   2728.0      0.0                  freq = reduce(operator.add, freqs)\n",
                        "  1320         1       1256.0   1256.0      0.0                  print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "  1321                                                   \n",
                        "  1322                                                   else:\n",
                        "  1323                                                       print(\"Computing sequentially\")\n",
                        "  1324                                                       print(\"Normalize and jobs data...\")\n",
                        "  1325                                                       start = time.time()\n",
                        "  1326                                                       corpus_chain = \"\".join(self.corpus.train[:corpus_length])\n",
                        "  1327                                                       job_data = pre_process(corpus_chain,self.normalizer.normalize)\n",
                        "  1328                                           \n",
                        "  1329                                                       pair_len_global = job_data[-1]\n",
                        "  1330                                                       best_pair = pair_len_global.most_common(1)[0][0]\n",
                        "  1331                                                       \n",
                        "  1332                                                       merges = [\" \".join(best_pair)]\n",
                        "  1333                                                       print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "  1334                                           \n",
                        "  1335                                                       print(\"Build alphabet...\")\n",
                        "  1336                                                       start = time.time()\n",
                        "  1337                                                       alphabet = Counter()\n",
                        "  1338                                                       for (l,r),v in pair_len_global.items():\n",
                        "  1339                                                           alphabet[l] =+ v\n",
                        "  1340                                                       # In extreme cases, right characters of pairs might not be left characters. If there are such chars, they're added with freq 1\n",
                        "  1341                                                       left_out_chars = {r for l,r in pair_len_global.keys()}-alphabet.keys()\n",
                        "  1342                                                       if len(left_out_chars)>0:\n",
                        "  1343                                                           print(f\"Adding characters: {left_out_chars}\")\n",
                        "  1344                                                           for char in left_out_chars:\n",
                        "  1345                                                               alphabet[char] =+ 1\n",
                        "  1346                                                       print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "  1347                                           \n",
                        "  1348                                                       alpha_len = len(alphabet)\n",
                        "  1349                                                       special_tokens_len = 0 if special_tokens == None else len(special_tokens)\n",
                        "  1350                                                       \n",
                        "  1351                                                       print(f\"Alphabet Size: {alpha_len}\")\n",
                        "  1352                                                       print(f\"Special Tokens Size: {special_tokens_len}\")\n",
                        "  1353                                                       \n",
                        "  1354                                                       if vocab_size<0:\n",
                        "  1355                                                           voc_final_length = alpha_len + abs(vocab_size) + special_tokens_len\n",
                        "  1356                                                       else:\n",
                        "  1357                                                           voc_final_length = vocab_size\n",
                        "  1358                                           \n",
                        "  1359                                                       delta_voc = voc_final_length - alpha_len - special_tokens_len\n",
                        "  1360                                                       \n",
                        "  1361                                                       print(f\"Terms to compute: {delta_voc}\\n\")\n",
                        "  1362                                           \n",
                        "  1363                                                       print(\"Enter loop\")\n",
                        "  1364                                                       for _ in range(delta_voc):\n",
                        "  1365                                           \n",
                        "  1366                                                           print(f\"{_+1+alpha_len+special_tokens_len}/{voc_final_length}: {best_pair}...\")\n",
                        "  1367                                                           start = time.time()\n",
                        "  1368                                                           job_data = process_best_pair(job_data, best_pair)\n",
                        "  1369                                           \n",
                        "  1370                                                           pair_len_global = job_data[-1]\n",
                        "  1371                                                           best_pair = pair_len_global.most_common(1)[0][0]\n",
                        "  1372                                           \n",
                        "  1373                                                           merges.append(\" \".join(best_pair))\n",
                        "  1374                                                           print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "  1375                                           \n",
                        "  1376                                                           if saveQ == True:\n",
                        "  1377                                                               voc_partial_len = alpha_len + special_tokens_len + _ + 1\n",
                        "  1378                                                               if voc_partial_len % save_step == 0 and voc_partial_len != voc_final_length:\n",
                        "  1379                                           \n",
                        "  1380                                                                   print(\"Saving intermediate results...\")\n",
                        "  1381                                                                   start = time.time()\n",
                        "  1382                                                                   freq = compute_freq(job_data[0])\n",
                        "  1383                                           \n",
                        "  1384                                                                   vocabulary = freq.most_common()\n",
                        "  1385                                                                   \n",
                        "  1386                                                                   if special_tokens != None:\n",
                        "  1387                                                                       vocabulary = vocabulary + [(token,0) for token in special_tokens]\n",
                        "  1388                                                                   \n",
                        "  1389                                                                   self.merges = merges\n",
                        "  1390                                                                   self.encode = {k:i for i,(k,v) in enumerate(vocabulary)}\n",
                        "  1391                                                                   self.freq = dict(vocabulary)\n",
                        "  1392                                                                   self.alpha = dict(alphabet.most_common())\n",
                        "  1393                                                                   step_path = self.path / str(voc_partial_len)\n",
                        "  1394                                                                   self.save(step_path)\n",
                        "  1395                                                                   print(f\"... computed in {time.time()-start} secs.\")\n",
                        "  1396                                                                   print(f\"Intermediate vocabulary saved to {step_path}\\n\")\n",
                        "  1397                                                       \n",
                        "  1398                                                       print(\"Compute freq...\")\n",
                        "  1399                                                       start = time.time()\n",
                        "  1400                                                       freq = compute_freq(job_data[0])\n",
                        "  1401                                                       print(f\"... computed in {time.time()-start} secs.\\n\")\n",
                        "  1402                                           \n",
                        "  1403         1        162.0    162.0      0.0          vocabulary = freq.most_common()\n",
                        "  1404                                                   \n",
                        "  1405         1          5.0      5.0      0.0          if special_tokens != None:\n",
                        "  1406         1         12.0     12.0      0.0              vocabulary = vocabulary + [(token,0) for token in special_tokens]\n",
                        "  1407                                                   \n",
                        "  1408         1         14.0     14.0      0.0          self.merges = merges\n",
                        "  1409         1        164.0    164.0      0.0          self.encode = {k:i for i,(k,v) in enumerate(vocabulary)}\n",
                        "  1410         1         51.0     51.0      0.0          self.freq = dict(vocabulary)\n",
                        "  1411         1         36.0     36.0      0.0          self.alpha = dict(alphabet.most_common())\n",
                        "  1412                                           \n",
                        "  1413         1        109.0    109.0      0.0          self.decode = {i:k for k,i in self.encode.items()}\n",
                        "  1414                                                   \n",
                        "  1415         1          4.0      4.0      0.0          self.len = len(vocabulary)     \n",
                        "  1416         1         14.0     14.0      0.0          self.freq_mass = sum(self.freq.values())\n",
                        "  1417         1        197.0    197.0      0.0          self.prob = {k:v/self.freq_mass for k,v in self.freq.items()}\n",
                        "  1418                                           \n",
                        "  1419         1        146.0    146.0      0.0          print(\"Vocabulary built\")\n",
                        "  1420                                                   \n",
                        "  1421         1          5.0      5.0      0.0          if save == True:\n",
                        "  1422                                                       self.save()\n",
                        "  1423                                                       print(f\"Vocabulary saved to {self.path}\")"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit"
        },
        "interpreter": {
            "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}