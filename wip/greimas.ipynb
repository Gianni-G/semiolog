{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "import semiolog as slg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter,defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import compress\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(30,4)})\n",
    "def plot(vector):\n",
    "    return sns.heatmap(vector, xticklabels=elements[:-1])#,yticklabels=elements[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"words\"\n",
    "\n",
    "if level == \"tags\":\n",
    "    ng1 = slg.util.load_file(\"wip/res_bnc/ng_t_1.json\")\n",
    "    ng2 = slg.util.load_file(\"wip/res_bnc/ng_t_2.json\")\n",
    "    ng3 = slg.util.load_file(\"wip/res_bnc/ng_t_3.json\")\n",
    "    ng4 = slg.util.load_file(\"wip/res_bnc/ng_t_4.json\")\n",
    "\n",
    "elif level == \"words\":\n",
    "    ng1 = slg.util.load_file(\"wip/res_bnc/ng_w_1.json\")\n",
    "    ng2 = slg.util.load_file(\"wip/res_bnc/ng_w_2.json\")\n",
    "    ng3 = slg.util.load_file(\"wip/res_bnc/ng_w_3_5.json\")\n",
    "    ng4 = slg.util.load_file(\"wip/res_bnc/ng_w_4_5.json\")\n",
    "\n",
    "elif level == \"chars\":\n",
    "    ng1 = slg.util.load_file(\"models/en_bnc/vocabulary/alpha.json\")\n",
    "    ng2 = slg.util.load_file(\"models/en_bnc/vocabulary/ngrams_rem/big_ng/2.json\")\n",
    "    ng3 = slg.util.load_file(\"models/en_bnc/vocabulary/ngrams_rem/big_ng/3.json\")\n",
    "    ng4 = slg.util.load_file(\"models/en_bnc/vocabulary/ngrams_rem/big_ng/4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 100\n",
    "\n",
    "if level == \"chars\":\n",
    "\n",
    "    ng1 = {k:v for k,v in ng1.items() if v>=thres}\n",
    "    ng2 = {tuple(list(k)):v for k,v in ng2.items() if v>=thres}\n",
    "    ng3 = {tuple(list(k)):v for k,v in ng3.items() if v>=thres}\n",
    "    ng4 = {tuple(list(k)):v for k,v in ng4.items() if v>=thres}\n",
    "    \n",
    "elif level == \"words\":\n",
    "    ng2 = {tuple(k.split()):v for k,v in ng2.items() if v>=thres}\n",
    "    ng3 = {tuple(k.split()):v for k,v in ng3.items() if v>=thres}\n",
    "    ng4 = {tuple(k.split()):v for k,v in ng4.items() if v>=thres}\n",
    "\n",
    "    ng2 = {k:v for k,v in ng2.items() if len(k)>1 and all([kn not in \",.'‘’?!\"+string.punctuation for kn in k])}\n",
    "    ng3 = {k:v for k,v in ng3.items() if len(k)>2 and all([kn not in \",.'‘’?!\"+string.punctuation for kn in k])}\n",
    "    ng4 = {k:v for k,v in ng4.items() if len(k)>3 and all([kn not in \",.'‘’?!\"+string.punctuation for kn in k])}\n",
    "\n",
    "else:    \n",
    "    ng2 = {tuple(k.split()):v for k,v in ng2.items() if v>=thres}\n",
    "    ng3 = {tuple(k.split()):v for k,v in ng3.items() if v>=thres}\n",
    "    ng4 = {tuple(k.split()):v for k,v in ng4.items() if v>=thres}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if level == \"chars\":\n",
    "    elements = string.ascii_lowercase+string.digits\n",
    "    \n",
    "else:\n",
    "    elements = ng1.keys()\n",
    "\n",
    "if level == \"words\":\n",
    "    elements = [e for e in elements if e not in \",.'‘’?!\"+string.punctuation]#[:100000]\n",
    "\n",
    "elements = list(elements) + [\" \"]\n",
    "elements_dict = {k:i for i,k in enumerate(elements)}\n",
    "elements_len = len(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term = \"that\"\n",
    "\n",
    "contexts = {(cl,cr):v for (cl,t,cr),v in ng3.items() if t == term and cl in elements and cr in elements}\n",
    "\n",
    "terms = defaultdict(int)\n",
    "for (cl,t,cr),v in ng3.items():\n",
    "    if (cl,cr) in contexts:\n",
    "        terms[t] += v\n",
    "\n",
    "len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building oR Matrix...\n",
      "Term-Context Matrix built in 177.26 secs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ortho = {(c,(l,r)):v for (l,c,r),v in ng3.items()}\n",
    "\n",
    "a_M = slg.util.build_term_context_matrix(elements, contexts, ortho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_kmeans = KMeans(n_clusters=4, random_state=0).fit(normalize(a_M.T,norm=\"l1\",axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11406209403790801\n",
      "[('at', 'time'), ('in', 'case'), ('at', 'moment'), ('in', 'way'), ('of', 'year'), ('by', 'time'), ('in', 'country'), ('all', 'time'), ('on', 'basis'), ('in', 'area'), ('in', 'context'), ('of', 'time'), ('in', 'sense'), ('is', 'most'), ('in', 'year'), ('on', 'one'), ('to', 'point'), ('to', 'end'), ('at', 'age'), ('is', 'the')]\n",
      "['the', 'this', 'that', 'a', 'any', 'one', 'some', 'only', 'which', 'an', 'no', 'his', 'either', 'each', 'just', 'all', 'another', 'last', 'its', 'what']\n",
      "\n",
      "0.9741279761258902\n",
      "[('is', 'the'), ('and', \"'s\"), ('so', 'the'), ('and', 'the'), ('fact', 'the'), ('is', 'it'), ('at', 'time'), ('said', 'the'), ('well', \"'s\"), ('and', 'was'), ('but', \"'s\"), ('and', 'is'), ('ensure', 'the'), ('so', 'they'), ('think', \"'s\"), ('so', 'it'), ('so', \"'s\"), ('was', 'the'), ('is', 'they'), ('to', 'of')]\n",
      "['that', 'in', 'of', 'what', 'as', 'by', 'when', 'to', 'on', 'like', 'only', 'why', 'about', 'which', 'and', 'this', 'for', 'out', 'how', 'not']\n",
      "\n",
      "0.18205529814488858\n",
      "[('that', 'was'), ('that', 'is'), ('and', 'was'), ('and', \"'s\"), ('and', 'is'), ('but', 'was'), ('but', 'is'), ('but', \"'s\"), ('think', \"'s\"), ('if', 'is'), ('that', 'would'), ('if', \"'s\"), ('so', \"'s\"), ('if', 'was'), ('that', \"'s\"), ('well', \"'s\"), ('is', 'the'), ('because', 'is'), ('because', 'was'), ('thought', 'was')]\n",
      "['it', 'that', 'he', 'there', 'i', 'this', 'she', 'what', 'they', 'you', 'so', 'them', 'we', 'which', 'one', 'certainly', 'him', 'anything', 'who', 'something']\n",
      "\n",
      "0.24351865189095737\n",
      "[('out', 'the'), ('and', 'the'), ('is', 'the'), ('and', \"'s\"), ('so', 'the'), ('was', 'the'), ('is', 'it'), ('at', 'time'), ('fact', 'the'), ('said', 'the'), ('and', 'was'), ('and', 'is'), ('well', \"'s\"), ('but', \"'s\"), ('it', 'the'), ('view', 'the'), ('time', 'the'), ('but', 'the'), ('is', 'a'), ('ensure', 'the')]\n",
      "['that', 'of', 'in', 'what', 'if', 'by', 'to', 'for', 'on', 'and', 'about', \"n't\", 'not', 'as', 'when', 'with', 'one', 'which', 'who', 'at']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c_n in range(len(contexts_kmeans.cluster_centers_)):\n",
    "    cluster_n = contexts_kmeans.cluster_centers_[c_n]\n",
    "\n",
    "    D = sparse.diags(cluster_n)\n",
    "    contexts_full = D.dot(a_M)\n",
    "    contexts_norm = normalize(np.asarray(contexts_full.sum(axis=0)),norm=\"l1\")\n",
    "    print(contexts_kmeans.cluster_centers_[:,elements_dict[term]][c_n])\n",
    "    print([list(contexts.keys())[i] for i in  np.flip(np.argsort(contexts_norm))[0][:20] if contexts_norm[0][i]>0])\n",
    "    print([elements[i] for i in  np.flip(np.argsort(cluster_n))[:20] if cluster_n[i]>0])\n",
    "    print()\n",
    "    # plot(contexts_kmeans.cluster_centers_[c_n:c_n+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_cluster = defaultdict(list)\n",
    "for i,c in enumerate(contexts_kmeans.labels_):\n",
    "    context_cluster[c] += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_clusters = [[elements[sort] for sort in np.argsort(centroid)] for centroid in contexts_kmeans.cluster_centers_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " 'z',\n",
       " 'v',\n",
       " '9',\n",
       " '5',\n",
       " 'q',\n",
       " '3',\n",
       " '6',\n",
       " '4',\n",
       " 'j',\n",
       " '8',\n",
       " '7',\n",
       " 'h',\n",
       " 'k',\n",
       " 'b',\n",
       " 'g',\n",
       " 'w']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_clusters[2][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n",
       "       [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "# This produces a feature matrix of token counts, similar to what\n",
    "# CountVectorizer would produce on text.\n",
    "X, _ = make_multilabel_classification(random_state=0)\n",
    "lda = LatentDirichletAllocation(n_components=5,\n",
    "    random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "# get topics for some given samples:\n",
    "lda.transform(X[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 1., 4., ..., 4., 1., 3.],\n",
       "       [5., 0., 6., ..., 0., 0., 3.],\n",
       "       [3., 4., 1., ..., 3., 2., 5.],\n",
       "       ...,\n",
       "       [2., 1., 2., ..., 1., 0., 3.],\n",
       "       [6., 4., 1., ..., 1., 3., 5.],\n",
       "       [2., 4., 2., ..., 5., 4., 2.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
