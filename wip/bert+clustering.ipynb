{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import BertForMaskedLM, BertConfig, BertTokenizer\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/Users/Gianni/semiolog/models/en_bnc_berttest/chains_BNC_101-150_1078011_voc30K.txt\"\n",
    "\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    lines = [l[:-1].split(\" \") for l in f.readlines()[:10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_counter = Counter()\n",
    "for l in lines:\n",
    "    freq_counter.update(l)\n",
    "id_to_token_and_freq = dict(enumerate(freq_counter.most_common()))\n",
    "id_to_token_and_freq[len(id_to_token_and_freq)] = (\"<mask>\", 0)\n",
    "id_to_token_and_freq[len(id_to_token_and_freq)] = (\"<pad>\", 0)\n",
    "id_to_token_and_freq[len(id_to_token_and_freq)] = (\"<unk>\", 0)\n",
    "id_to_token_and_freq[len(id_to_token_and_freq)] = (\"<s>\", 0)\n",
    "id_to_token_and_freq[len(id_to_token_and_freq)] = (\"</s>\", 0)\n",
    "\n",
    "token_to_id = {token[0]:id for id, token in id_to_token_and_freq.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn dataset into torch Tensor\n",
    "data = []\n",
    "max_len = max([len(line) for line in lines])\n",
    "for line in lines:\n",
    "    line_ids = [token_to_id[token] for token in line]  # have to do case for unknowns\n",
    "    data.append(line_ids)\n",
    "    #data.append(torch.tensor(line_ids + [token_to_id[\"<pad>\"]] * (max_len - len(line_ids)))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"gpu\"\n",
    "    print(\"Using gpu\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = BertForMaskedLM(BertConfig(vocab_size=len(token_to_id))).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_input(input_lines, mask=True):\n",
    "    input_ids = []\n",
    "    label_ids = []\n",
    "    att_masks = []\n",
    "    for line in input_lines:\n",
    "        pad_len = max_len - len(line)\n",
    "        masked_pos = random.randint(0, len(line) - 1)\n",
    "        inp_line = line[:]\n",
    "        if mask:\n",
    "            inp_line[masked_pos] = token_to_id[\"<mask>\"]\n",
    "        input_ids.append(inp_line + [token_to_id[\"<pad>\"] for _ in range(pad_len)])\n",
    "\n",
    "        # This line can't be simply uncommented but need to be adapted a bit\n",
    "        # labels = torch.tensor([input_line + [token_to_id[\"<pad>\"] for _ in range(pad_len)]])\n",
    "        # TODO: Decide whether to ignore other parts\n",
    "        if mask:\n",
    "            labels = [-100 for _ in range(max_len)]\n",
    "            labels[masked_pos] = line[masked_pos]\n",
    "        else:\n",
    "            pass\n",
    "        labels = line[:] + [token_to_id[\"<pad>\"] for _ in range(pad_len)]\n",
    "        label_ids.append(labels)\n",
    "        att_mask = [1 for _ in range(len(line))] + [0 for _ in range(pad_len)]\n",
    "        att_masks.append(att_mask)\n",
    "    input_tensor = torch.tensor(input_ids).to(device)\n",
    "    labels = torch.tensor(label_ids).to(device)\n",
    "    token_type_ids = torch.zeros_like(labels).to(device)\n",
    "    attention_mask = torch.tensor(att_masks).to(device)  # set the padding ids to 0\n",
    "    return input_tensor, labels, token_type_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedLMOutput(loss=tensor(5.2992, grad_fn=<NllLossBackward>), logits=tensor([[[ 0.0000,  0.0078,  0.0547,  ..., -0.1755, -0.4897,  0.4224],\n",
      "         [ 0.0000, -0.5832,  0.5144,  ..., -0.2148, -0.5365,  0.0297],\n",
      "         [ 0.0000,  0.2059,  0.4069,  ..., -0.3505, -1.2915, -0.4742],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0685, -0.1134,  ..., -0.5047, -0.8725,  0.6245],\n",
      "         [ 0.0000,  0.2854, -0.2177,  ...,  0.1600, -0.6393, -0.1853],\n",
      "         [ 0.0000, -0.0200,  0.6925,  ..., -1.4898, -0.1954,  0.1009]]],\n",
      "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "test_sentence = [data[0]]\n",
    "input_tensor, labels, token_type_ids, attention_mask = prep_input(test_sentence, mask=False)\n",
    "outputs = model(input_ids=input_tensor,\n",
    "                labels=labels,\n",
    "                token_type_ids=token_type_ids,\n",
    "                attention_mask=attention_mask)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 0\n",
      "tensor(10.2336, grad_fn=<AddBackward0>)\n",
      "Training epoch: 1\n",
      "tensor(9.5558, grad_fn=<AddBackward0>)\n",
      "Training epoch: 2\n",
      "tensor(8.8982, grad_fn=<AddBackward0>)\n",
      "Training epoch: 3\n",
      "tensor(8.5236, grad_fn=<AddBackward0>)\n",
      "Training epoch: 4\n",
      "tensor(7.0252, grad_fn=<AddBackward0>)\n",
      "Training epoch: 5\n",
      "tensor(8.6173, grad_fn=<AddBackward0>)\n",
      "Training epoch: 6\n",
      "tensor(8.7849, grad_fn=<AddBackward0>)\n",
      "Training epoch: 7\n",
      "tensor(8.5119, grad_fn=<AddBackward0>)\n",
      "Training epoch: 8\n",
      "tensor(9.7094, grad_fn=<AddBackward0>)\n",
      "Training epoch: 9\n",
      "tensor(10.5023, grad_fn=<AddBackward0>)\n",
      "Training epoch: 10\n",
      "tensor(8.4807, grad_fn=<AddBackward0>)\n",
      "Training epoch: 11\n",
      "tensor(10.0132, grad_fn=<AddBackward0>)\n",
      "Training epoch: 12\n",
      "tensor(9.5135, grad_fn=<AddBackward0>)\n",
      "Training epoch: 13\n",
      "tensor(9.0205, grad_fn=<AddBackward0>)\n",
      "Training epoch: 14\n",
      "tensor(8.9648, grad_fn=<AddBackward0>)\n",
      "Training epoch: 15\n",
      "tensor(9.2731, grad_fn=<AddBackward0>)\n",
      "Training epoch: 16\n",
      "tensor(9.3327, grad_fn=<AddBackward0>)\n",
      "Training epoch: 17\n",
      "tensor(9.2589, grad_fn=<AddBackward0>)\n",
      "Training epoch: 18\n",
      "tensor(9.1089, grad_fn=<AddBackward0>)\n",
      "Training epoch: 19\n",
      "tensor(8.9873, grad_fn=<AddBackward0>)\n",
      "MaskedLMOutput(loss=tensor(5.3943, grad_fn=<NllLossBackward>), logits=tensor([[[ 1.7010,  0.7304,  0.1402,  ..., -2.1160, -2.3273, -1.9049],\n",
      "         [ 1.7013,  0.6449,  0.2718,  ..., -2.0154, -2.4011, -1.7660],\n",
      "         [ 1.7004,  0.6598,  0.2145,  ..., -2.0255, -2.5445, -1.8387],\n",
      "         ...,\n",
      "         [ 1.7204,  0.6667,  0.2679,  ..., -2.0531, -2.4407, -1.9089],\n",
      "         [ 1.7086,  0.7032,  0.0762,  ..., -2.0693, -2.4654, -1.8400],\n",
      "         [ 1.7112,  0.7054,  0.2353,  ..., -1.8849, -2.3659, -1.9284]]],\n",
      "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(\"Training epoch:\", epoch)\n",
    "    epoch_loss = 0\n",
    "    for idx in range(0, len(data), BATCH_SIZE):\n",
    "        input_lines = data[idx:idx+BATCH_SIZE]\n",
    "        input_tensor, labels, token_type_ids, attention_mask = prep_input(input_lines)\n",
    "        outputs = model(input_ids=input_tensor,\n",
    "                        labels=labels,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        attention_mask=attention_mask)\n",
    "        epoch_loss += outputs.loss\n",
    "        outputs.loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_tensor, labels, token_type_ids, attention_mask = prep_input(test_sentence, mask=True)\n",
    "outputs = model(input_ids=input_tensor,\n",
    "                    labels=labels,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tensor = outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'askedto',\n",
       "  'to',\n",
       "  'an',\n",
       "  'acet',\n",
       "  'asked',\n",
       "  'adequately',\n",
       "  'theycan'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'theycan',\n",
       "  'an',\n",
       "  'askedto',\n",
       "  'acet',\n",
       "  'ensure',\n",
       "  'asked'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'an',\n",
       "  'ensure',\n",
       "  'askedto',\n",
       "  'asked',\n",
       "  'theycan',\n",
       "  'acet'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'acet',\n",
       "  'askedto',\n",
       "  'an',\n",
       "  'theycan',\n",
       "  'asked',\n",
       "  'one'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'askedto',\n",
       "  'theycan',\n",
       "  'an',\n",
       "  'to',\n",
       "  'acet',\n",
       "  'asked',\n",
       "  'ensure'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'an',\n",
       "  'theycan',\n",
       "  'asked',\n",
       "  'askedto',\n",
       "  'ensure',\n",
       "  'acet'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'acet',\n",
       "  'theycan',\n",
       "  'adequately',\n",
       "  'askedto',\n",
       "  'an',\n",
       "  'one'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'acet',\n",
       "  'askedto',\n",
       "  'theycan',\n",
       "  'an',\n",
       "  'asked',\n",
       "  'adequately'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'acet',\n",
       "  'to',\n",
       "  'askedto',\n",
       "  'an',\n",
       "  'ensure',\n",
       "  'theycan',\n",
       "  'your'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'an',\n",
       "  'askedto',\n",
       "  'acet',\n",
       "  'theycan',\n",
       "  'one',\n",
       "  'ensure'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'an',\n",
       "  'acet',\n",
       "  'ensure',\n",
       "  'askedto',\n",
       "  'theycan',\n",
       "  'asked'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'askedto',\n",
       "  'to',\n",
       "  'acet',\n",
       "  'ensure',\n",
       "  'asked',\n",
       "  'an',\n",
       "  'theycan'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'an',\n",
       "  'acet',\n",
       "  'to',\n",
       "  'askedto',\n",
       "  'your',\n",
       "  'asked',\n",
       "  'theycan'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'askedto',\n",
       "  'to',\n",
       "  'an',\n",
       "  'theycan',\n",
       "  'acet',\n",
       "  'one',\n",
       "  'your'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'askedto',\n",
       "  'an',\n",
       "  'theycan',\n",
       "  'ensure',\n",
       "  'asked',\n",
       "  'acet'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'acet',\n",
       "  'an',\n",
       "  'askedto',\n",
       "  'theycan',\n",
       "  'asked',\n",
       "  'one'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'an',\n",
       "  'askedto',\n",
       "  'acet',\n",
       "  'theycan',\n",
       "  'ensure',\n",
       "  'asked'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'ensure',\n",
       "  'asked',\n",
       "  'askedto',\n",
       "  'to',\n",
       "  'theycan',\n",
       "  'acet',\n",
       "  'an'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'askedto',\n",
       "  'acet',\n",
       "  'an',\n",
       "  'theycan',\n",
       "  'your',\n",
       "  'training'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'askedto',\n",
       "  'acet',\n",
       "  'asked',\n",
       "  'an',\n",
       "  'theycan',\n",
       "  'ensure'],\n",
       " ['<pad>',\n",
       "  'youare',\n",
       "  'and',\n",
       "  'to',\n",
       "  'asked',\n",
       "  'theycan',\n",
       "  'ensure',\n",
       "  'an',\n",
       "  'askedto',\n",
       "  'acet']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[id_to_token_and_freq[k][0] for v,k in sorted([(v,i) for i,v in enumerate(out_tensor[0,i])], reverse=True)[:10]] for i in range(len(out_tensor[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aids',\n",
       " 'acquired',\n",
       " 'immun',\n",
       " 'ede',\n",
       " 'ficiency',\n",
       " 's',\n",
       " 'yndrome',\n",
       " 'is',\n",
       " 'a',\n",
       " 'condition',\n",
       " 'causedby',\n",
       " 'a',\n",
       " 'virus',\n",
       " 'called',\n",
       " 'hiv',\n",
       " 'human',\n",
       " 'immuno',\n",
       " 'de',\n",
       " 'ficiency',\n",
       " 'virus']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id_to_token_and_freq[id][0] for id in test_sentence[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('and', 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_token_and_freq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 15, 16, 17, 3, 18, 19, 20, 4, 5, 21, 4, 6, 22, 23, 24, 25, 26, 3, 6]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"tf\")\n",
    "inputs[\"labels\"] = tokenizer(\"The capital of France is Paris.\", return_tensors=\"tf\")[\"input_ids\"]\n",
    "\n",
    "outputs = model(inputs)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
