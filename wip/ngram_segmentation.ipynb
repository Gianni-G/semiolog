{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "import semiolog as slg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG [I]: Checking config correctness... Config correct!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 23.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG [I]: Dataset loaded from disk (TXT files)\n",
      "SLG [I]: Vocabulary loaded from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG [I]: nGrams loaded from disk (['6.json', '7.json', '10.json', '2.json', '3.json', '8.json', '4.json', '5.json', '9.json'])\n",
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 11:26:38.101962: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-26 11:26:38.105585: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at models/en_bnc/paradigms/tf_model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG [I]: Paradigmatizer loaded from disk\n"
     ]
    }
   ],
   "source": [
    "semiotic = slg.Cenematic(\"en_bnc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c443849ef8a49bdadfc337b0b8a96ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5423648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "corpus = semiotic.corpus.train\n",
    "\n",
    "corpus_norm = []\n",
    "for sent in tqdm(corpus[\"text\"]):\n",
    "    corpus_norm.append(semiotic.syntagmatic.tokenizer.normalizer.normalize_str(sent))\n",
    "\n",
    "corpus_norm = Dataset.from_dict({\"text\":corpus_norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG [W]: This feature is not fully implemented/tested yet\n",
      "SLG: Counting 10-Grams...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ccaefdf7b849d08630720d89132b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/413051903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ng4 = slg.vocabulary.nGram.build(corpus=corpus,n=4, str_normalizer=semiotic.syntagmatic.tokenizer.normalizer.normalize_str, parallel=False, keep_in_memory=True)\n",
    "\n",
    "ng_n = 10\n",
    "ng = slg.vocabulary.nGram.build(\n",
    "    corpus=corpus_norm,\n",
    "    n=ng_n,\n",
    "    parallel=False,\n",
    "    keep_in_memory=True\n",
    "    )\n",
    "slg.util.save_file(ng,semiotic.paths.vocabulary / f\"ngrams/{ng_n}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import graphviz as gv\n",
    "from ipywidgets import interact\n",
    "from math import log\n",
    "from numpy.linalg import svd\n",
    "import numpy as np\n",
    "from itertools import chain, combinations\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng1 = semiotic.vocab.ng1\n",
    "ng2 = semiotic.vocab.ng2\n",
    "ng3 = semiotic.vocab.ng3\n",
    "ng4 = semiotic.vocab.ng4\n",
    "ng5 = semiotic.vocab.ng5\n",
    "ng6 = semiotic.vocab.ng6\n",
    "ng7 = semiotic.vocab.ng7\n",
    "ng8 = semiotic.vocab.ng8\n",
    "ng9 = semiotic.vocab.ng9\n",
    "ng10 = semiotic.vocab.ng10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_factor = 1/sum(ng1.freq.values())\n",
    "\n",
    "ng1.prob = {k:v*norm_factor for k,v in ng1.freq.items()}\n",
    "ng2.prob = {k:v*norm_factor for k,v in ng2.freq.items()}\n",
    "ng3.prob = {k:v*norm_factor for k,v in ng3.freq.items()}\n",
    "ng4.prob = {k:v*norm_factor for k,v in ng4.freq.items()}\n",
    "ng5.prob = {k:v*norm_factor for k,v in ng5.freq.items()}\n",
    "ng6.prob = {k:v*norm_factor for k,v in ng6.freq.items()}\n",
    "ng7.prob = {k:v*norm_factor for k,v in ng7.freq.items()}\n",
    "ng7.prob = {k:v*norm_factor for k,v in ng7.freq.items()}\n",
    "ng8.prob = {k:v*norm_factor for k,v in ng8.freq.items()}\n",
    "ng9.prob = {k:v*norm_factor for k,v in ng9.freq.items()}\n",
    "ng10.prob = {k:v*norm_factor for k,v in ng10.freq.items()}\n",
    "\n",
    "# ng_prob = {**ng1.prob,**ng2.prob,**ng3.prob, **ng4.prob, **ng5.prob, **ng6.prob, **ng7.prob}\n",
    "ng_prob = [ng1.prob, ng2.prob, ng3.prob, ng4.prob, ng5.prob, ng6.prob, ng7.prob, ng8.prob, ng9.prob, ng10.prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have made my plans\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "sent = semiotic.corpus.train[randint(0,100000)][\"text\"]\n",
    "sent = \"I have made my plans\"\n",
    "# sent = \"verygood\"\n",
    "print(sent)\n",
    "sent = semiotic.syntagmatic.tokenizer.normalizer.normalize_str(sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be81b372c3a445cb46d4f78c67d5bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32768 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7878bea9620048f597668008893001f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(1.0052526847183316e-13, ['i', 'havemade', 'myplans']),\n",
       " (2.3299475427595537e-14, ['i', 'havemade', 'm', 'yplans']),\n",
       " (2.1298318664759638e-14, ['i', 'havemade', 'myplan', 's']),\n",
       " (1.2581321579931439e-14, ['i', 'havemad', 'emyplans']),\n",
       " (1.236343015965356e-14, ['i', 'havemad', 'e', 'myplans']),\n",
       " (1.0995390151505617e-14, ['ihavemad', 'emyplans']),\n",
       " (1.0804964911883531e-14, ['ihavemad', 'e', 'myplans']),\n",
       " (9.35750897157668e-15, ['i', 'havemade', 'm', 'yplan', 's']),\n",
       " (7.840117907565132e-15, ['i', 'havemade', 'my', 'plans']),\n",
       " (7.005707550132993e-15, ['i', 'havemade', 'm', 'y', 'plans'])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_sent = len(sent)\n",
    "pws = list(powerset(range(len_sent+1)))\n",
    "possible_cuts = [i for i in pws if 0 in i and len_sent in i]\n",
    "possible_offsets = []\n",
    "for cut in tqdm(possible_cuts):\n",
    "    possible_offsets.append(list(zip(cut,cut[1:])))\n",
    "possible_offsets_n = []\n",
    "for offset in possible_offsets:\n",
    "    if max([r-l for l,r in offset])<9 and offset != [(0,len(sent))]:\n",
    "        possible_offsets_n.append(offset)\n",
    "seg_probs = []\n",
    "for offsets in tqdm(possible_offsets_n):\n",
    "    segs = [sent[l:r] for l,r in offsets]\n",
    "    seg_probs.append((np.prod([ng_prob[len(seg)-1][seg] for seg in segs]), segs))\n",
    "\n",
    "sorted(seg_probs, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "bla = Counter(ng_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e', 0.12283516959154912),\n",
       " ('t', 0.09144317330136345),\n",
       " ('a', 0.07986945016799842),\n",
       " ('o', 0.07554999657567639),\n",
       " ('i', 0.07229377970004593),\n",
       " ('n', 0.07001552324870404),\n",
       " ('s', 0.06436566038490976),\n",
       " ('r', 0.0612647297217823),\n",
       " ('h', 0.05187486568481037),\n",
       " ('l', 0.04127051180816118)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
