{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "import semiolog as slg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG [I]: Checking config correctness... Config correct!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG [I]: Dataset loaded from disk (TXT files)\n",
      "SLG [I]: Vocabulary loaded from disk\n",
      "SLG [I]: nGrams loaded from disk (['6.json', '7.json', '2.json', '3.json', '8.json', '4.json', '5.json'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at models/en_bnc/paradigms/tf_model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG [I]: Paradigmatizer loaded from disk\n"
     ]
    }
   ],
   "source": [
    "semiotic = slg.Cenematic(\"en_bnc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "corpus = semiotic.corpus.train\n",
    "\n",
    "corpus_norm = []\n",
    "for sent in tqdm(corpus[\"text\"]):\n",
    "    corpus_norm.append(semiotic.syntagmatic.tokenizer.normalizer.normalize_str(sent))\n",
    "\n",
    "corpus_norm = Dataset.from_dict({\"text\":corpus_norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLG [W]: This feature is not fully implemented/tested yet\n",
      "SLG: Counting 10-Grams...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ccaefdf7b849d08630720d89132b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/413051903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ng4 = slg.vocabulary.nGram.build(corpus=corpus,n=4, str_normalizer=semiotic.syntagmatic.tokenizer.normalizer.normalize_str, parallel=False, keep_in_memory=True)\n",
    "\n",
    "ng_n = 10\n",
    "ng = slg.vocabulary.nGram.build(\n",
    "    corpus=corpus_norm,\n",
    "    n=ng_n,\n",
    "    parallel=False,\n",
    "    keep_in_memory=True\n",
    "    )\n",
    "slg.util.save_file(ng,semiotic.paths.vocabulary / f\"ngrams/{ng_n}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from ipywidgets import interact\n",
    "from math import log, log2, exp\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng1 = semiotic.vocab.ng1\n",
    "ng2 = semiotic.vocab.ng2\n",
    "ng3 = semiotic.vocab.ng3\n",
    "ng4 = semiotic.vocab.ng4\n",
    "ng5 = semiotic.vocab.ng5\n",
    "ng6 = semiotic.vocab.ng6\n",
    "ng7 = semiotic.vocab.ng7\n",
    "ng8 = semiotic.vocab.ng8\n",
    "# ng9 = semiotic.vocab.ng9\n",
    "# ng10 = semiotic.vocab.ng10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_factor = 1/sum(ng1.freq.values())\n",
    "\n",
    "ng1.prob = {k:v*norm_factor for k,v in ng1.freq.items()}\n",
    "ng2.prob = {k:v*norm_factor for k,v in ng2.freq.items()}\n",
    "ng3.prob = {k:v*norm_factor for k,v in ng3.freq.items()}\n",
    "ng4.prob = {k:v*norm_factor for k,v in ng4.freq.items()}\n",
    "ng5.prob = {k:v*norm_factor for k,v in ng5.freq.items()}\n",
    "ng6.prob = {k:v*norm_factor for k,v in ng6.freq.items()}\n",
    "ng7.prob = {k:v*norm_factor for k,v in ng7.freq.items()}\n",
    "ng8.prob = {k:v*norm_factor for k,v in ng8.freq.items()}\n",
    "# ng9.prob = {k:v*norm_factor for k,v in ng9.freq.items()}\n",
    "# ng10.prob = {k:v*norm_factor for k,v in ng10.freq.items()}\n",
    "\n",
    "# ng_prob = {**ng1.prob,**ng2.prob,**ng3.prob, **ng4.prob, **ng5.prob, **ng6.prob, **ng7.prob}\n",
    "ng_prob = [ng1.prob, ng2.prob, ng3.prob, ng4.prob, ng5.prob, ng6.prob, ng7.prob, ng8.prob]#, ng9.prob, ng10.prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_data(string,ng_prob):\n",
    "\n",
    "    edge_data = []\n",
    "    for beginning in range(0, len(string)):\n",
    "        for end in range(beginning + 1, len(string) + 1):\n",
    "            subsequence_label = string[beginning:end]\n",
    "            if len(subsequence_label)>len(ng_prob) or subsequence_label not in ng_prob[len(subsequence_label)-1] or subsequence_label == string:\n",
    "                continue\n",
    "            edge_data.append(\n",
    "                (\n",
    "                    beginning,\n",
    "                    end,\n",
    "                    {\n",
    "                        \"label\": subsequence_label,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return edge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semiolog.util import subsequences\n",
    "from math import log2\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "def chain2seq(string, ng_prob):\n",
    "    \n",
    "    lSt = len(string)\n",
    "\n",
    "    # If sent of len <2r, then return the interval\n",
    "    if lSt<2:\n",
    "        return [(0,lSt)]\n",
    "    elif lSt == 2:\n",
    "        return [(0,1),(1,2)]\n",
    "\n",
    "    # If a character in the string not in vocab, add it\n",
    "    for c in string:\n",
    "        if c not in ng_prob[0]:\n",
    "            ng_prob[0][c]=0\n",
    "            # self.voc_rank[c]=(len(self.voc)+1)**self.zipf_factor\n",
    "\n",
    "    graph_data = build_graph_data(string, ng_prob)\n",
    "    seg_graph_full = nx.DiGraph()\n",
    "    seg_graph_full.add_edges_from(graph_data)\n",
    "\n",
    "    # Construct weights\n",
    "    for edge in seg_graph_full.edges:\n",
    "        label = seg_graph_full.edges[edge][\"label\"]\n",
    "        score = ng_prob[len(label)-1][label]\n",
    "        \n",
    "        if score == 0:\n",
    "            score = 0\n",
    "        else:\n",
    "            score = -log(score)\n",
    "\n",
    "\n",
    "        seg_graph_full.edges[edge][\"weight\"] = score\n",
    "\n",
    "    # Find best segmentation out of shortest path\n",
    "\n",
    "\n",
    "    # def k_shortest_paths(G, source, target, k, weight=None):\n",
    "    #     k_s = list(islice(nx.shortest_simple_paths(G, source, target, weight=weight), k))\n",
    "    #     k_l = [[string[l:r] for l,r in subsequences(sp, 2)] for sp in k_s]\n",
    "    #     k_sc = [[log(ng_prob[len(label)-1][label]) for label in k_] for k_ in k_l]\n",
    "    #     return (k_s,k_l,k_sc)\n",
    "\n",
    "    # k_short = k_shortest_paths(seg_graph_full, 0, lSt, 5)\n",
    "\n",
    "    # shortest_path = nx.shortest_path(seg_graph_full, 0, lSt, weight=\"weight\", method=\"dijkstra\")\n",
    "    shortest_path = nx.shortest_path(seg_graph_full, 0, lSt, weight=\"weight\", method=\"bellman-ford\")\n",
    "    shortest_path_length = nx.shortest_path_length(seg_graph_full, 0, lSt, weight=\"weight\",method=\"bellman-ford\")\n",
    "    # shortest_path_length = nx.shortest_path_length(seg_graph_full, 0, lSt, weight=\"weight\", method=\"dijkstra\")\n",
    "\n",
    "    seg_offsets = subsequences(shortest_path, 2)\n",
    "\n",
    "    splits = []\n",
    "    for start,end in seg_offsets:\n",
    "        splits.append(string[start:end])\n",
    "\n",
    "    return (splits, exp(-shortest_path_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And then there was his secrecy , his concealments , his clandestine behaviour .\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "sent = semiotic.corpus.train[randint(0,100000)][\"text\"]\n",
    "# sent = \"I have made my plans and I have to stick to them\"\n",
    "# sent = \"thisisverygood\"\n",
    "print(sent)\n",
    "sent = semiotic.syntagmatic.tokenizer.normalizer.normalize_str(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['andthen',\n",
       "  'therewas',\n",
       "  'his',\n",
       "  'secrecy',\n",
       "  'hisconce',\n",
       "  'al',\n",
       "  'ments',\n",
       "  'his',\n",
       "  'clandest',\n",
       "  'in',\n",
       "  'e',\n",
       "  'behaviou',\n",
       "  'r'],\n",
       " 9.753619874057792e-46)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2seq(sent,ng_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c4fbc5b2324801ae53508c69f5d373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a3aa41f8034f40897c7f39de92f2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(1.2334047306502716e-09, ['thisis', 'verygood']),\n",
       " (1.1336746360318178e-10, ['t', 'hisis', 'verygood']),\n",
       " (1.0396510461982027e-10, ['thisi', 's', 'verygood']),\n",
       " (1.0210446267371826e-10, ['this', 'is', 'verygood']),\n",
       " (5.607854089912726e-11, ['this', 'i', 's', 'verygood']),\n",
       " (4.722440266801482e-11, ['thisis', 'verygoo', 'd']),\n",
       " (4.288297282264796e-11, ['th', 'isis', 'verygood']),\n",
       " (2.205213091671237e-11, ['th', 'is', 'is', 'verygood']),\n",
       " (1.8042322396313062e-11, ['t', 'his', 'is', 'verygood']),\n",
       " (1.373847298376866e-11, ['th', 'isi', 's', 'verygood']),\n",
       " (1.3477759609945006e-11, ['thi', 'sis', 'verygood']),\n",
       " (1.3376587619711515e-11, ['thi', 's', 'is', 'verygood']),\n",
       " (1.2824876407454603e-11, ['thisis', 'v', 'erygood']),\n",
       " (1.2203233198586194e-11, ['th', 'i', 'sis', 'verygood']),\n",
       " (1.2111628553176633e-11, ['th', 'i', 's', 'is', 'verygood']),\n",
       " (1.211162855317663e-11, ['th', 'is', 'i', 's', 'verygood']),\n",
       " (1.1954058115308092e-11, ['thisis', 'very', 'good']),\n",
       " (1.1485026775147747e-11, ['thisi', 'sverygoo', 'd']),\n",
       " (1.1301720397105854e-11, ['thisisv', 'erygood']),\n",
       " (1.092785852454724e-11, ['t', 'hisi', 's', 'verygood']),\n",
       " (9.909332931412764e-12, ['t', 'his', 'i', 's', 'verygood']),\n",
       " (9.64243357286321e-12, ['thisisve', 'rygood']),\n",
       " (9.214127363594882e-12, ['thi', 'si', 's', 'verygood']),\n",
       " (8.68760876109533e-12, ['thisis', 've', 'rygood']),\n",
       " (8.342791991664063e-12, ['th', 'i', 'si', 's', 'verygood']),\n",
       " (7.346784815075844e-12, ['thi', 's', 'i', 's', 'verygood']),\n",
       " (7.205706057270796e-12, ['t', 'h', 'isis', 'verygood']),\n",
       " (6.6520349785767034e-12, ['th', 'i', 's', 'i', 's', 'verygood']),\n",
       " (6.194997312732066e-12, ['this', 'i', 'sverygoo', 'd']),\n",
       " (6.0668317188486864e-12, ['thisis', 'ver', 'ygood'])]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_sent = len(sent)\n",
    "pws = list(powerset(range(len_sent+1)))\n",
    "possible_cuts = [i for i in pws if 0 in i and len_sent in i]\n",
    "possible_offsets = []\n",
    "for cut in tqdm(possible_cuts):\n",
    "    possible_offsets.append(list(zip(cut,cut[1:])))\n",
    "possible_offsets_n = []\n",
    "for offset in possible_offsets:\n",
    "    if max([r-l for l,r in offset])<9 and offset != [(0,len(sent))]:\n",
    "        possible_offsets_n.append(offset)\n",
    "seg_probs = []\n",
    "for offsets in tqdm(possible_offsets_n):\n",
    "    segs = [sent[l:r] for l,r in offsets]\n",
    "    seg_probs.append((np.prod([ng_prob[len(seg)-1][seg] for seg in segs]), segs))\n",
    "\n",
    "sorted(seg_probs, reverse=True)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2809751852749289e-16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp(-36.59373983654784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "bla = Counter(ng_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e', 0.12283516959154912),\n",
       " ('t', 0.09144317330136345),\n",
       " ('a', 0.07986945016799842),\n",
       " ('o', 0.07554999657567639),\n",
       " ('i', 0.07229377970004593),\n",
       " ('n', 0.07001552324870404),\n",
       " ('s', 0.06436566038490976),\n",
       " ('r', 0.0612647297217823),\n",
       " ('h', 0.05187486568481037),\n",
       " ('l', 0.04127051180816118)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
