{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import os\n",
                "os.chdir(\"../\")\n",
                "\n",
                "import semiolog as slg\n",
                "\n",
                "from pyinstrument import Profiler\n",
                "import sys\n",
                "\n",
                "import regex as re\n",
                "import numpy as np\n",
                "from collections import Counter\n",
                "from scipy.sparse import csr_matrix, lil_matrix, coo_matrix\n",
                "from tqdm.notebook import tqdm, trange\n",
                "from functools import partial\n",
                "from functools import reduce\n",
                "import operator\n",
                "\n",
                "import time\n",
                "from pyinstrument import Profiler\n",
                "import sys\n",
                "\n",
                "from temp import findall_contexts, find_best_pair"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "semiotic = slg.Cenematic(\"fr_wiki\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Warning: models/fr_wiki/vocabulary/merges.txt does not exist.\n",
                        "Vocabulary will not be loaded from file.\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "chain_list, cl_chain, encode, decode = build_nb(semiotic.corpus.train,\n",
                "    voc_final_length = -10)"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "HBox(children=(FloatProgress(value=0.0, description='Normalize & Alphabet', max=4.0, style=ProgressStyle(descrâ€¦"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "d86be13b0f9447c091d0ccc5aed71f36"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "re_voc_l = \"(\"+\"|\".join([\" \"+k+\" \" for k in encode.keys()]+[\"\\[SEP\\] \",\"\\[SEP_i\\] \"])+\")\"\n",
                "re_voc_r = \"(\"+\"|\".join([\" \"+k+\" \" for k in encode.keys()]+[\" \\[SEP\\]\",\" \\[SEP_i\\]\"])+\")\"\n",
                "best_pair_string = \"e s\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "source": [
                "merge_context = re.findall(re_voc_l+best_pair_string+re_voc_r, cl_chain, overlapped=True)\n",
                "merge_context_count_l = Counter()\n",
                "merge_context_count_r = Counter()\n",
                "for l,r in merge_context:\n",
                "    if \"[SEP]\" not in l:\n",
                "        merge_context_count_l[encode[l.strip()]] += 1\n",
                "    if \"[SEP]\" not in r:\n",
                "        merge_context_count_r[encode[r.strip()]] += 1"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "source": [
                "merge_context[:10]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[(' c ', ' t '),\n",
                            " (' d ', ' t '),\n",
                            " (' l ', ' p '),\n",
                            " (' a ', ' t '),\n",
                            " (' l ', ' b '),\n",
                            " (' r ', ' a '),\n",
                            " (' e ', ' t '),\n",
                            " (' n ', ' i '),\n",
                            " (' d ', ' a '),\n",
                            " (' l ', ' t ')]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 29
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "source": [
                "merge_context_count_l_2 = Counter()\n",
                "merge_context_count_r_2 = Counter()\n",
                "\n",
                "for a,b,c,d in zip(*[chain_list[i:] for i in range(4)]):\n",
                "    if (b,c) == (\"e\",\"s\"):\n",
                "        merge_context_count_l_2[encode[a]] += 1\n",
                "        merge_context_count_r_2[encode[d]] += 1"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "for l1,l2 in zip(merge_context_count_r.most_common(),merge_context_count_r_2.most_common()):\n",
                "    if l1!=l2:\n",
                "        print(l1,l2)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "merge_context_count_r_2.most_common(10)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[(0, 471536)]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 46
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def build_nb_new(\n",
                "    corpus = None,\n",
                "    voc_final_length = -30,\n",
                "    # save = False,\n",
                "    # save_step = None,\n",
                "    # progress_bar = True,\n",
                "    # resume_merges = False,\n",
                "    parallel = False,\n",
                "    sparse = True,\n",
                "    sparse_mode = \"csr\",\n",
                "    cpu_count = 4,\n",
                "    corpus_length = None,\n",
                "    normalizer = None,\n",
                "):\n",
                "    \n",
                "    def agglutinate_chain(pair, chain_list):\n",
                "        chain_list = \" \".join(chain_list) \n",
                "        bigram = re.escape(\" \".join(pair))\n",
                "        p = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
                "        chain_list = p.sub(\"\".join(pair), chain_list)\n",
                "        chain_list = chain_list.split()\n",
                "        return chain_list\n",
                "\n",
                "    def extract_drc(pairs, encoder: dict):\n",
                "        data = []\n",
                "        rows = []\n",
                "        columns = []\n",
                "        for (r,c),d in pairs:\n",
                "            data.append(d)\n",
                "            rows.append(encoder[r])\n",
                "            columns.append(encoder[c])\n",
                "        return data, rows, columns\n",
                "\n",
                "    def parallel_chain(chain, n_of_parts, overlap = 0):\n",
                "        \"\"\"\n",
                "        Breaks the chain in n chunks to compute best pair of terms. Chunks are overlapping by one term, so as no pair of terms is lost due to the break.\n",
                "        \"\"\"\n",
                "        if not isinstance(chain,list):\n",
                "            chain = list(chain)\n",
                "        chunk_size = int(len(chain) / n_of_parts)+1\n",
                "        for i in range(0, len(chain), chunk_size):\n",
                "            yield chain[i : i + chunk_size + overlap]\n",
                "\n",
                "    def separate_chain(chain, n_of_parts, best_pair: list):\n",
                "        \"\"\"\n",
                "        Separate a chain (in list form) for parallel processing of regex findall of pair, taking care that the cuts of the chunks don't fall in the neiborhood of the pair, affecting the final counts\n",
                "        \"\"\"\n",
                "        chunk_size = int(len(chain) / n_of_parts)+1\n",
                "        b = 0\n",
                "        n = chunk_size\n",
                "        chain_len = len(chain)\n",
                "        for i in range(n_of_parts):\n",
                "            n = (i+1)*chunk_size\n",
                "            if chain_len > n:\n",
                "                while chain[n-2:n] == best_pair or chain[n-1:n+1] == best_pair:\n",
                "                    n = n+1\n",
                "            yield (\"[SEP_i] \" if i!=0 else \"\") + \" \".join(chain[b:n]) + (\" [SEP_i]\" if i!=n_of_parts-1 else \"\")\n",
                "            b = n-1\n",
                "        \n",
                "        \n",
                "    # normalizer = eval(f\"slg.syntagmatic.tokenizer.normalizers.Sequence({semiotic.config.vocabulary.normalizer})\")\n",
                "    \n",
                "    if parallel:\n",
                "        \n",
                "        par_corpus = parallel_chain(corpus[:corpus_length], cpu_count)\n",
                "\n",
                "        result = slg.util.multiprocessing_tqdm(partial(semiotic.vocab.chain_list_alpha, normalizer), par_corpus, cores=cpu_count, desc=\"Normalize & Alphabet\")\n",
                "        \n",
                "        chain_list = []\n",
                "        alphabet = Counter()\n",
                "        for chain_l, alpha in result:\n",
                "            chain_list += chain_l\n",
                "            alphabet += alpha\n",
                "            \n",
                "    else:\n",
                "        chain_list, alphabet = semiotic.vocab.chain_list_alpha(normalizer, semiotic.corpus.train[:corpus_length], progress_bar=True)\n",
                "\n",
                "    # cl, alphabet = semiotic.vocab.chain_list_alpha(normalizer, semiotic.corpus.train, progress_bar=True)\n",
                "    # cl_chain = \"[SEP] \"+\" \".join(chain_list)+\" [SEP]\"\n",
                "    encode = {k:i for i,(k,v) in enumerate(alphabet.most_common())}\n",
                "    decode = {i:k for k,i in encode.items()}\n",
                "    new_i = len(encode)\n",
                "    if parallel:\n",
                "        \n",
                "        par_chain = parallel_chain(chain_list, cpu_count, overlap=1)\n",
                "        \n",
                "        result = slg.util.multiprocessing(find_best_pair, par_chain, cores=cpu_count) \n",
                "                            \n",
                "        pairs = reduce(operator.add, result)\n",
                "        pairs = pairs.most_common()\n",
                "        \n",
                "    else:\n",
                "        pairs = find_best_pair(chain_list).most_common()\n",
                "    if voc_final_length<0:\n",
                "        voc_final_length = new_i + abs(voc_final_length)\n",
                "        \n",
                "    if sparse:\n",
                "        data, rows, columns = extract_drc(pairs,encode)\n",
                "        voc_matrix = coo_matrix((np.array(data), (np.array(rows),np.array(columns))), shape=(voc_final_length, voc_final_length), dtype=int)\n",
                "\n",
                "    else:\n",
                "        voc_matrix = np.zeros((voc_final_length, voc_final_length), dtype=int)\n",
                "        for (row,column),value in pairs:\n",
                "            voc_matrix[encode[row], encode[column]] = value\n",
                "    merges = []\n",
                "    delta_voc = voc_final_length - new_i\n",
                "    best_pair = \"init\"\n",
                "    pair_count = \"---\"\n",
                "\n",
                "    t = trange(delta_voc) #, disable = not progress_bar)\n",
                "\n",
                "    for _ in t:\n",
                "        t.set_description(f\"Pair: {best_pair}, {pair_count}\")\n",
                "        t.refresh()\n",
                "\n",
                "        if sparse:\n",
                "            max_i = voc_matrix.data.argmax()\n",
                "            pair_row = voc_matrix.row[max_i]\n",
                "            pair_col = voc_matrix.col[max_i]\n",
                "            pair_count = voc_matrix.data[max_i]\n",
                "        else:\n",
                "            pair_row,pair_col = np.unravel_index(np.argmax(voc_matrix, axis=None), voc_matrix.shape)\n",
                "            pair_count = voc_matrix[pair_row,pair_col]\n",
                "        \n",
                "        if pair_count == 0:\n",
                "            break\n",
                "\n",
                "        best_pair = (decode[pair_row], decode[pair_col])\n",
                "        best_pair_string = \" \".join(best_pair)\n",
                "        merges.append(best_pair_string)\n",
                "        best_pair_string_voc = \"\".join(best_pair)\n",
                "        re_voc_l = \"(\"+\"|\".join([\" \"+k+\" \" for k in encode.keys()]+[\"\\[SEP\\] \",\"\\[SEP_i\\] \"])+\")\"\n",
                "        re_voc_r = \"(\"+\"|\".join([\" \"+k+\" \" for k in encode.keys()]+[\" \\[SEP\\]\",\" \\[SEP_i\\]\"])+\")\"\n",
                "        if parallel:\n",
                "            result = slg.util.multiprocessing(\n",
                "                partial(findall_contexts,best_pair_string=best_pair_string,re_voc_l=re_voc_l,re_voc_r=re_voc_r),\n",
                "                separate_chain(cl_chain.split(), cpu_count, list(best_pair)),\n",
                "                cores = cpu_count\n",
                "                )\n",
                "            merge_context = reduce(operator.add, result)\n",
                "        else:\n",
                "            merge_context_count_l = Counter()\n",
                "            merge_context_count_r = Counter()\n",
                "            pair_pair_count = 0\n",
                "            pair_pair_overlap_control = [-4]\n",
                "\n",
                "            for j(a,b,c,d) in enumerate(zip(*[chain_list[i:] for i in range(4)])):\n",
                "                if (b,c) == best_pair:\n",
                "                    merge_context_count_l[encode[a]] += 1\n",
                "                    merge_context_count_r[encode[d]] += 1\n",
                "                    \n",
                "                # compute #(l,r)-(l,r)\n",
                "                if (a,b,c,d) == best_pair*2 and j-pair_pair_overlap_control[-1]>3:\n",
                "                    pair_pair_count += 1\n",
                "                    pair_pair_overlap_control.append(j)\n",
                "        \n",
                "        if sparse:\n",
                "            # Convert matrix to CSR or LIL, for item attribution and arithmetic \n",
                "            if sparse_mode == \"csr\":\n",
                "                voc_matrix = voc_matrix.tocsr()\n",
                "            else:\n",
                "                voc_matrix = voc_matrix.tolil()\n",
                "        \n",
                "        for row,key in merge_context_count_l.items():\n",
                "            voc_matrix[row,new_i] = key\n",
                "            \n",
                "        for column,key in merge_context_count_r.items():\n",
                "            voc_matrix[new_i,column] = key\n",
                "\n",
                "        # Correct previous counts\n",
                "        \n",
                "        # # compute #(l,r)-(l,r)\n",
                "        # pair_pair_count = len(re.findall(\" \"+best_pair_string+\" \"+best_pair_string+\" \", cl_chain, overlapped=False))\n",
                "        # remove #(l,r)-(l,r) from (l,r)-l\n",
                "        voc_matrix[new_i,pair_row] -= pair_pair_count\n",
                "        # remove #(l,r)-(l,r) from r-(l,r)\n",
                "        voc_matrix[pair_col,new_i] -= pair_pair_count\n",
                "        # remove #(l,r)-(l,r) from r-l\n",
                "        voc_matrix[pair_col,pair_row] -= pair_pair_count\n",
                "        # substract (l,r)- from r-\n",
                "        voc_matrix[pair_col,:new_i] -= voc_matrix[new_i,:new_i]\n",
                "        # substract -(l,r)- from -l\n",
                "        voc_matrix[:new_i,pair_row] -= voc_matrix[:new_i,new_i]\n",
                "        \n",
                "        # set l-r to 0\n",
                "        voc_matrix[pair_row,pair_col] = 0\n",
                "        # register #(l,r)-(l,r)\n",
                "        voc_matrix[new_i,new_i] = pair_pair_count\n",
                "        \n",
                "        if sparse:\n",
                "            # Convert matrix back to COO, to restart the loop\n",
                "            voc_matrix = voc_matrix.tocoo()\n",
                "        \n",
                "        best_pair_string_voc = \"\".join(best_pair)\n",
                "        encode[best_pair_string_voc] = new_i\n",
                "        decode[new_i] = best_pair_string_voc\n",
                "        new_i += 1\n",
                "        chain_list = agglutinate_chain(best_pair_string.split(),chain_list)\n",
                "\n",
                "\n",
                "    if sparse:\n",
                "        freq_values = voc_matrix.sum(axis=1).T.tolist()[0]\n",
                "    else:\n",
                "        freq_values = voc_matrix.sum(axis=1).T.tolist()\n",
                "    vocabulary = {decode[i]:v for i,v in enumerate(freq_values) if v>0} # Make sure dimension of matrix and size of voc coincide\n",
                "    vocabulary = sorted(vocabulary.items(), key=lambda x: x[1], reverse=True)\n",
                "    \n",
                "    return merges, vocabulary"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit"
        },
        "interpreter": {
            "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}