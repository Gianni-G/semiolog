{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from semiolog.util import subsequences\n",
    "\n",
    "import string\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from tokenizers import Tokenizer, Regex, NormalizedString, PreTokenizedString, normalizers\n",
    "# from tokenizers.models import WordLevel\n",
    "# from tokenizers.pre_tokenizers import PreTokenizer\n",
    "# from tokenizers.normalizers import NFKC, Lowercase, Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = \"...â€”â€¢â€¦â€“â€™\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at models/en_bnc_old_segments/paradigms/tf_model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "Using custom data configuration corpus-48c43d9b917d9e96\n",
      "Reusing dataset text (/Users/Gianni/.cache/huggingface/datasets/text/corpus-48c43d9b917d9e96/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 26.93it/s]\n",
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at models/en_bnc_old_segments/paradigms/tf_model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import semiolog as slg\n",
    "\n",
    "semiotic = slg.Cenematic(\"en_bnc_old_segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceSLG:\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, semiotic) -> None:\n",
    "        self.zipf_factor = .135\n",
    "        self.semiotic = semiotic\n",
    "        self.voc = self.semiotic.vocab.freq\n",
    "\n",
    "        # TODO: Zipf factor should (in principle) be computable following Mandelbrot (or not?)\n",
    "        self.voc_rank = {k:(v+1)**self.zipf_factor for v,k in enumerate(self.voc.keys())}\n",
    "\n",
    "    def build_graph_data(\n",
    "        self,\n",
    "        string: str,\n",
    "        voc: dict\n",
    "        )-> List[tuple]:\n",
    "\n",
    "        edge_data = []\n",
    "        for beginning in range(0, len(string)):\n",
    "            for end in range(beginning + 1, len(string) + 1):\n",
    "                subsequence_label = string[beginning:end]\n",
    "                if subsequence_label not in voc or subsequence_label == string:\n",
    "                    continue\n",
    "                edge_data.append(\n",
    "                    (\n",
    "                        beginning,\n",
    "                        end,\n",
    "                        {\n",
    "                            \"label\": subsequence_label,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return edge_data\n",
    "\n",
    "    def chain2seq(\n",
    "        self, string:str\n",
    "    ) -> List[tuple]:\n",
    "        lSt = len(string)\n",
    "        \n",
    "        # In case of a character in the string not in vocab, add it\n",
    "        for c in string:\n",
    "            if c not in self.voc:\n",
    "                self.voc[c]=1\n",
    "                self.voc_rank[c]=(len(self.voc)+1)**self.zipf_factor\n",
    "\n",
    "        graph_data = self.build_graph_data(string, self.voc)\n",
    "        seg_graph_full = nx.DiGraph()\n",
    "        seg_graph_full.add_edges_from(graph_data)\n",
    "\n",
    "        # Construct weights\n",
    "        for edge in seg_graph_full.edges:\n",
    "            rank = self.voc_rank[seg_graph_full.edges[edge][\"label\"]]\n",
    "            seg_graph_full.edges[edge][\"weight\"] = rank\n",
    "\n",
    "        # Find best segmentation out of shortest path\n",
    "        shortest_path = nx.shortest_path(seg_graph_full, 0, lSt, weight=\"weight\")\n",
    "\n",
    "        seg_offsets = subsequences(shortest_path, 2)\n",
    "\n",
    "        return seg_offsets\n",
    "\n",
    "    def SequenceSLG_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "\n",
    "        seg_offsets = self.chain2seq(str(normalized_string))\n",
    "\n",
    "        splits = []\n",
    "        for start,end in seg_offsets:\n",
    "            splits.append(normalized_string[start:end])\n",
    "\n",
    "        return splits\n",
    "\n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "\n",
    "        pretok.split(self.SequenceSLG_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDecoder:\n",
    "    def decode(self, tokens: List[str]) -> str:\n",
    "        return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section shows how to attach these custom components to the Tokenizer\n",
    "tok = Tokenizer(WordLevel(vocab=semiotic.vocab.encode,unk_token=\"[UNK]\"))\n",
    "tok.normalizer = normalizers.Sequence([NFKC(), Lowercase(), Replace(Regex(f\"{[i for i in string.whitespace]}\"),\"\"),Replace(Regex(f\"{[i for i in string.punctuation+punctuation]}\"),\"\")])\n",
    "tok.pre_tokenizer = PreTokenizer.custom(SequenceSLG(semiotic))\n",
    "# tok.decoder = Decoder.custom(CustomDecoder())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have made my plans: I must stick to them!\n",
      "ihavemademyplansimuststicktothem\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ihave', (0, 5)),\n",
       " ('made', (5, 9)),\n",
       " ('my', (9, 11)),\n",
       " ('plans', (11, 16)),\n",
       " ('imust', (16, 21)),\n",
       " ('stick', (21, 26)),\n",
       " ('tothem', (26, 32))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = semiotic.corpus.test[\"text\"][2]\n",
    "input = \"I have made my plans: I must stick to them!\"\n",
    "print(input)\n",
    "\n",
    "input_norm = tok.normalizer.normalize_str(input)\n",
    "print(input_norm)\n",
    "\n",
    "bla = tok.pre_tokenizer.pre_tokenize_str(input_norm)\n",
    "bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4484, 34, 29996, 22026, 920, 54, 2313]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'welcome tothe [UNK] token iz ers library'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tok.encode(\"Welcome to the ðŸ¤— Tokenizers library.\")\n",
    "print(output.ids)\n",
    "# [1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2]\n",
    "\n",
    "tok.decode(output.ids)\n",
    "# \"Hello , y ' all ! How are you ?\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
